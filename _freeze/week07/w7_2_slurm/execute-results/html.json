{
  "hash": "6ddded9e8a2a3fca0919a6263fd12653",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Managing batch jobs in multi-step analyses\"\nsubtitle: \"Week 7 - part II\"\npagetitle: \"PRACS25: Slurm batch jobs\"\nauthor: Jelmer Poelstra\ndate: 2025-10-08\nengine: knitr\n---\n\n\n\n----------------------------------------------------------------------------------------------------\n\n<br>\n\nWhile thinking about different ways to organize and run your analyses,\nlet's have an example in mind with a simple RNA-seq analysis that:\n\n- Trims reads in FASTQ files _(independently for each sample)_\n- Maps (aligns) reads to a reference genome _(independently for each sample)_\n- Creates a gene expression count table from the alignments _(for all samples together)_\n\n![A simple 3-step RNA-seq analysis, with arrows showing the connections<br>between the steps: the output of one step is the input of the next.<br>The example only has 3 samples, and the first two steps are executed separately<br>for each sample, while the last step is executed once for all samples.](img/schem-workflow.svg){fig-align=\"center\" width=\"75%\"}\n\n<br>\n\n## More on runner scripts\n\nIn the past two weeks of this course, we have created a few \"runner scripts\"\n(usually named `run/run.sh`): digital notebook-like scripts that we ran line-by-line in\norder to run (week 4) or submit as batch jobs (week 5) our \"primary scripts\"\n(e.g. `scripts/fastqc.sh`).\n\nThese runner scripts were useful to store that code instead of typing it directly\nin the terminal,\nespecially because we were passing arguments to our script and running loops ---\nnot only would this be tedious to type,\nbut it would also reduce reproducibility if we didn't store exactly _how_ we ran\nour scripts.\n\nHowever, our runner scripts so far have been very short.\nIn the context of a research project that would include running a series of steps\nwith different bioinformatics tools (and perhaps custom scripts),\nthe idea would be to **include all these steps in such a runner script**.\nFor example, for the above-mentioned RNA-seq analysis, such a runner script could\nlook like so:\n\n```sh\n# [hypothetical example - don't run this]\n# Define the inputs\nfastq_dir=data/fastq\nref_assembly=data/ref/assembly.fna\nref_annotation=data/ref/annotation.gtf\n\n# Trim the reads:\nfor R1 in \"$fastq_dir\"/*_R1.fastq.gz; do\n    # (The trim.sh script takes 2 arguments: R1 FASTQ and output dir)\n    sbatch scripts/trim.sh \"$R1\" results/trim\ndone\n\n# Align (map) the reads to a reference genome assembly:\nfor R1 in results/trim/*_R1.fastq.gz; do\n    # (The map.sh script takes 3 arguments: R1 FASTQ, ref. assembly, and output dir)\n    sbatch scripts/map.sh \"$R1\" \"$ref_assembly\" results/map\ndone\n\n# Count alignments per sample per gene using the annotation:\n# (The count.sh script takes 3 arguments: input dir, ref. annotation, and output dir)\nsbatch scripts/count.sh results/map \"$ref_annotation\" results/count_table.txt\n```\n\nThe code above runs a primary shell script for each of the three steps\n(`trim.sh`, `map.sh`, and` count.sh`): each of these _takes arguments_ and runs\na bioinformatics tool to perform that step (for example, TrimGalore for trimming).\n\nHere are some advantages of using such a script structure with flexible\n(i.e., argument-accepting) primary scripts, and an overarching runner script:\n\n- Rerunning everything, including with a modified sample set, or tool settings,\n  is relatively straightforward --- both for yourself and others, improving reproducibility.\n- Re-applying the same set of analyses in a different project is straightforward.\n- The runner script is a form of documentation of all steps taken.\n- It (more or less) ensures you are including all necessary steps.\n\n\n## Pipelines\n\nWhat exactly do we mean by a \"pipeline\"?\nWe may informally refer to any consecutive series of analysis steps as a pipeline.\nThe above runner script, in particular, can informally be called a pipeline.\nBut here, I am using pipeline in a stricter sense to mean a series of steps that\n**can be executed from start to finish with a single command**.\n\nAn advantage of a true pipeline is increased automation,\nas well as \"supercharging\" all the above-mentioned advantages of runner script.\nFor example, a pipeline _truly_ ensures that you are including all necessary steps.\n\n<hr style=\"height:1pt; visibility:hidden;\" />\n\n<details><summary>But wait, can we not just run our runner script with a single command: `bash run/run.sh`? _(Click to expand)_ </summary>\nThis doesn't work because we are submitting batch jobs in each step.\nBecause the script would continue to the next line/submission immediately after the\nprevious lines, all jobs would effectively be submitted at the same time,\nand e.g. the mapping script would fail because the trimmed reads it needs are not\nyet there. (Below, we'll briefly talk about ways around this problem.)\n</details>\n\n<hr style=\"height:1pt; visibility:hidden;\" />\n\nTo turn our runner script into a pipeline,\nwe would need to overcome the problem of simultaneous batch job submission.\nAdditionally, a pipeline worth its salt should also be able to\n**detect and stop upon failure**, and to **rerun parts of the pipeline** flexibly.\nThe latter may be necessary after, e.g.:\n  \n- Some scripts failed for all or some samples\n- You added or removed a sample\n- You had to modify a script or settings somewhere halfway the pipeline.\n\nSo how could we implement all of that?\n\n- **Push the limits of the Bash and Slurm tool set**\\\n  Use `if` statements, many script arguments, and Slurm \"job dependencies\"\n  (see the box below) --- but this is hard to manage for more complex workflows.\n  Alternatively, if you only want to solve the simultaneous batch job problem,\n  you can put all steps in a single script,\n  but this would make for a very inefficient pipeline.\n\n- **Use a formal workflow management system**.\\\n  We'll talk about these some more below.\n\n<hr style=\"height:1pt; visibility:hidden;\" />\n\n::: {.callout-warning collapse=\"true\"}\n#### Pushing the limits of the Bash and Slurm tool set _(Click to expand)_\n\n**First, here are some low-tech, ad-hoc solutions to rerunning parts of the workflow:**\n\n- Comment out part of the workflow --- e.g., to skip a step:\n\n  ```sh\n  # Trim:\n  #for R1 in data/fastq/*_R1.fastq.gz; do\n  #    sbatch scripts/trim.sh \"$R1\" results/trim\n  #done\n  \n  # Align (map):\n  for R1 in results/trim/*_R1.fastq.gz; do\n      sbatch scripts/map.sh \"$R1\" results/map\n  done\n  \n  # Count alignments per sample per gene:\n  sbatch scripts/count.sh results/map results/count_table.txt\n  ```\n\n- Make temporary changes --- e.g., to only run a single added sample:\n\n  ```sh\n  # Trim:\n  #for R1 in data/fastq/*_R1.fastq.gz; do\n      R1=data/fastq/newsample_R1.fastq.gz\n      sbatch scripts/trim.sh \"$R1\" results/trim\n  #done\n  \n  # Align (map):\n  #for R1 in results/trim/*_R1.fastq.gz; do\n      R1=results/trim/newsample_R1.fastq.gz\n      sbatch scripts/map.sh \"$R1\" results/map\n  #done\n  \n  # Count - NOTE, this steps should be rerun as a whole:\n  sbatch scripts/count.sh results/map results/count_table.txt\n  ```\n\n<hr style=\"height:1pt; visibility:hidden;\" />\n\n**Second, here are some more bespoke code-based solutions:**\n\n- Command-line options and `if`-statements to flexibly run part of the pipeline\n  (and perhaps change settings):\n\n  ```sh\n  trim=$1   # true or false\n  map=$2    # true or false\n  count=$3  # true or false\n  \n  if [[ \"$trim\" == true ]]; then\n      for R1 in data/fastq/*_R1.fastq.gz; do\n          bash scripts/trim.sh \"$R1\" results/trim\n      done\n  fi\n  \n  if [[ \"$map\" == true ]]; then\n      for R1 in results/trim/*_R1.fastq.gz; do\n          bash scripts/map.sh \"$R1\" results/map\n      done\n  fi\n  \n  if [[ \"$count\" == true ]]; then\n      bash scripts/count.sh results/map results/count_table.txt\n  fi\n  ```\n\n- Slurm job dependencies --- in the example below,\n  jobs will only start after their \"dependencies\" (jobs whose outputs they need)\n  have finished:\n  \n  ```sh\n  for R1 in data/fastq/*_R1.fastq.gz; do\n      # Submit the trimming job and store its job number:\n      submit_line=$(sbatch scripts/trim.sh \"$R1\" results/trim)\n      trim_id=$(echo \"$submit_line\" | sed 's/Submitted batch job //')\n      \n      # Submit the mapping job with the condition that it only starts when the\n      # trimming job is done, using '--dependency=afterok:':\n      R1_trimmed=results/trim/$(basename \"$R1\")\n      sbatch --dependency=afterok:$trim_id scripts/map.sh \"$R1_trimmed\" results/map\n  done\n  \n  # If you give the mapping and counting jobs the same name with `#SBATCH --job-name=`,\n  # then you can use '--dependency=singleton': the counting job will only start\n  # when ALL the mapping jobs are done:\n  sbatch --dependency=singleton scripts/count.sh results/map results/count_table.txt\n  ```\n:::\n\n<br>",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}