[
  {
    "objectID": "week16/w16_overview.html",
    "href": "week16/w16_overview.html",
    "title": "Week 16: Student presentations",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "week11/w11_exercises.html",
    "href": "week11/w11_exercises.html",
    "title": "Week 11 Exercises: R Basics",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "week10/w10_overview.html",
    "href": "week10/w10_overview.html",
    "title": "Week 10: Nextflow pipelines II",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "week04/w4_ga_git.html",
    "href": "week04/w4_ga_git.html",
    "title": "Graded Assignment II: Markdown and version control",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "week04/w4_ua_github-account.html",
    "href": "week04/w4_ua_github-account.html",
    "title": "Assignment: Create a GitHub account",
    "section": "",
    "text": "What?\nCreate a personal GitHub account.\n\n\nWhy?\nGitHub is a website that hosts Git repositories, i.e. version-controlled projects. In Week 3 of this course, you will be learning how to use Git together with GitHub. In addition, you will submit your final project assignments through GitHub.\n\n\nHow?\nIf you already have a GitHub account, log in and start at step 6.\n\nGo to https://github.com.\nClick “Sign Up” in the top right.\nFollow the prompts to create your account — some notes:\n\nWhen choosing your username, I would recommend to keep it professional and have it include or resemble your real name. (This is because you will hopefully continue to use GitHub to share your code, for instance when publishing a paper.)\nYou can choose whether you want to use your OSU email address or a non-institutional email address. (And note that you can always associate a second email address with your account.)\n\nCheck your email and click the link to verify your email address.\nBack on GitHub website, now logged in: in the far top-right of the page, click your randomly assigned avatar, and in the dropdown menu, click “Settings”.\nIn the “Emails” tab (left-hand menu), deselect the box “Keep my email addresses private”.\nIn the “Public profile” tab, enter your name.\nStill in the “Profile tab”, upload a Profile picture. This can be a picture of yourself but if you prefer, you can use something else.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "week04/w4_2_github.html#remote-repositories",
    "href": "week04/w4_2_github.html#remote-repositories",
    "title": "Git: Remotes on GitHub",
    "section": "1 Remote repositories",
    "text": "1 Remote repositories\nSo far, we have been locally version-controlling our originspecies repository. Now, we also want to put this repo online, so we can:\n\nShare our work (e.g. alongside a publication) and/or\nHave an online backup and/or\nCollaborate with others2.\n\nWe will use the GitHub website as the place to host our online repositories. Online counterparts of local repositories are usually referred to as “remote repositories” or simply “remotes”.\nUsing remote repositories will mean adding a couple of Git commands to our toolbox:\n\ngit remote to add and manage connections to remotes.\ngit push to upload (“push”) changes from local to remote.\ngit pull to download (“pull”) changes from remote to local.\n\nBut we will need to start with some one-time setup to enable GitHub authentication.\n\n\n1.1 One-time setup: GitHub authentication\nIn order to link your local Git repositories to their online counterparts on GitHub, you need to set up GitHub authentication. There are two options for this (see the box below) but we will use SSH access with an SSH key.\n\nUse the ssh-keygen command to generate a public-private SSH key pair — in the command below, replace your_email@example.com with the email address you used to sign up for a GitHub account:\nssh-keygen -t ed25519 -C \"your_email@example.com\"\nYou’ll be asked three questions, and for all three, you can accept the default simply by pressing Enter:\n# Enter file in which to save the key (&lt;default path&gt;):\n# Enter passphrase (empty for no passphrase):\n# Enter same passphrase again: \nGenerating public/private ed25519 key pair.\nEnter file in which to save the key (/users/PAS0471/jelmer/.ssh/id_ed25519): \nEnter passphrase (empty for no passphrase): \nEnter same passphrase again: \nYour identification has been saved in /users/PAS0471/jelmer/.ssh/id_ed25519.\nYour public key has been saved in /users/PAS0471/jelmer/.ssh/id_ed25519.pub.\nThe key fingerprint is:\nSHA256:KO73cE18HFC/aHKIXR7nf9Fk4++CiIw6GTk7ffG+p2c your_email@example.com\nThe key's randomart image is:\n+--[ED25519 256]--+\n|          ...    |\n|           . .   |\n|            + o.o|\n|       . + = *.+o|\n|    ... S * B oo.|\n|   .+.  .o =   .o|\n|    .*.o.+.. .  +|\n|   .= +o+ o E ...|\n|    o= o..+*   ..|\n+----[SHA256]-----+\nNow, you have a file ~/.ssh/id_ed25519.pub, which is your public key. To enable authentication, you will put this public key (which interacts with your private key) on GitHub. Print the public key to your screen using cat:\ncat ~/.ssh/id_ed25519.pub\nssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIBz4qiqjbNrKPodoGyoF/v7n0GKyvc/vKiW0apaRjba2 your_email@example.com\nCopy the line that was printed to screen to your clipboard.\nIn your browser, go to https://github.com and log in.\nGo to your personal Settings. (Click on your avatar in the far top-right of the page, then select “Settings”.)\nClick on SSH and GPG keys in the sidebar.\nClick the green New SSH key button.\nIn the form (see screenshot below):\n\nGive the key an arbitrary, informative “Title” (name), e.g. “OSC” to indicate that you will use this key at OSC.\nPaste the public key, which you copied to your clipboard earlier, into the “Key” box.\nClick the green Add SSH key button. Done!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAuthentication and GitHub URLs\n\n\n\nThe current two options to authenticate to GitHub when connecting with remotes are:\n\nSSH access with an SSH key (which we have just set up)\nHTTPS access with a Public Access Token (PAT). See this GitHub page to set this up instead of or in addition to SSH access.\n\nFor everything on GitHub, there are separate SSH and HTTPS URLs. When using SSH (as we are), we need to use URLs with the following format:\ngit@github.com:&lt;USERNAME&gt;/&lt;REPOSITORY&gt;.git\n(And when using HTTPS, you would use URLs like https://github.com/&lt;USERNAME&gt;/&lt;REPOSITORY&gt;.git)\n\n\n\n\n\n1.2 Create a remote repository\nWhile we can interact with online repos using Git commands, we can’t create a new online repo with the Git CLI. Therefore, we will to go to the GitHub website to create a new online repo:\n\nOn the GitHub website, click the + next to your avatar (top-right) and select “New repository”:\n\n\n\n\n\n\n\nIn the box “Repository name”, we’ll use the same name that we gave to our local directory: originspecies3.\n\n\n\n\n\n\n\nLeave other options as they are, as shown below, and click “Create repository”:\n\n\n\n\n\n\n\n\n\n1.3 Link the local and remote repositories\nAfter you clicked “Create repository”, a page similar to this screenshot should appear, which gives us some information about linking the remote and local repositories:\n\n\n\n\n\nWe go back to VS Code, where we’ll enter the commands that GitHub provided to us under the “…or push an existing repository from the command line” heading shown at the bottom of the screenshot above:4\n\nFirst, we tell Git to add a “remote” connection with git remote, providing three arguments to this command:\n\nadd — because we’re adding a remote connection.\norigin — the arbitrary nickname we’re giving the connection (usually called “origin” by convention).\n\nThe URL to the GitHub repo (in SSH format: click on the HTTPS/SSH button to toggle the URL type).\n\n# git remote add &lt;remote-nickname&gt; &lt;URL&gt;\ngit remote add origin git@github.com:&lt;user&gt;/originspecies.git\nSecond, we upload (“push”) our local repo to remote using git push. When we push a repository for the first time, we need to use the -u option to set up an “upstream” counterpart:\n# git push -u &lt;connection&gt; &lt;branch&gt;\ngit push -u origin main\nYou should then get a message like this: type yes and press Enter.\nThe authenticity of host 'github.com (140.82.114.4)' can't be established.\nECDSA key fingerprint is SHA256:p2QAMXNIC1TJYWeIOttrVc98/R1BUFWu3/LiyKgUfQM.\nECDSA key fingerprint is MD5:7b:99:81:1e:4c:91:a5:0d:5a:2e:2e:80:13:3f:24:ca.\nAre you sure you want to continue connecting (yes/no)?\nThen, the push/upload should go through, with a message along these lines printed to screen:\nCounting objects: 18, done.\nDelta compression using up to 40 threads.\nCompressing objects: 100% (12/12), done.\nWriting objects: 100% (18/18), 1.67 KiB | 0 bytes/s, done.\nTotal 18 (delta 1), reused 0 (delta 0)\nremote: Resolving deltas: 100% (1/1), done.\nremote: To git@github.com:jelmerp/originspecies2.git\n* [new branch]      main -&gt; main\n\n\n\n\n\n\n\n\nPushing will be easier from now on\n\n\n\nNote that when we don’t give git push any arguments, it will push:\n\nTo the default remote connection.\nTo & from the currently active repository “branch” (default: main)5.\n\nBecause we only have one remote connection and one branch, we can from now on simply use the following to push:\ngit push\n\nAlso, note that you can check your remote connection settings for a repo with git remote -v:\ngit remote -v\norigin  git@github.com:jelmerp/originspecies.git (fetch)\norigin  git@github.com:jelmerp/originspecies.git (push)\n\n\n\n\n\n\n1.4 Explore the repository on GitHub\nBack at GitHub on your repo page, click where it says &lt;&gt; Code in the lower of the top bars:\n\n\n\n\n\nThis is basically our repo’s “home page”, and we can see the files that we just uploaded from our local repo:\n\n\n\n\n\nNext, click where it says x commits (x should be 10 in this case):\n\n\n\n\n\nYou’ll get an overview of the commits that you made, somewhat similar to what you get when you run git log:\n\n\n\n\n\nYou can click on a commit to see the changes that were made by it:\n\n\n\nThe line that is highlighted in green was added by this commit.\n\n\nOn the right-hand side, a &lt; &gt; button will allow you to see the state of the repo at the time of that commit:\n\n\n\n\n\nOn the commit overview page, scroll down all the way to the first commit and click the &lt; &gt;: you’ll see the repo’s “home page” again, but now with only the origin.txt file, since that was the only file in your repo at the time:\n\n\n\n\n\n\n\nGitHub “Issues”\nEach GitHub repository has an “Issues” tab — issues are mainly used to track bugs and other (potential) problems with a repository. In an issue, you can reference specific commits and people, and use Markdown formatting.\n\n\n\n\n\n\nWhen you hand in your final project submissions, you will create an issue simply to notify me about your repository.\n\n\n\n\n\n\nTo go to the Issues tab for your repo, click on Issues towards the top of the page:\n\n\n\n\n\nAnd you should see the following page — in which you can open a new issue with the “New” button:"
  },
  {
    "objectID": "week04/w4_2_github.html#remote-repo-workflows-single-user",
    "href": "week04/w4_2_github.html#remote-repo-workflows-single-user",
    "title": "Git: Remotes on GitHub",
    "section": "2 Remote repo workflows: single-user",
    "text": "2 Remote repo workflows: single-user\nIn a single-user workflow, all changes are typically made in the local repository, and the remote repo is simply periodically updated (pushed to). So, the interaction between local and remote is unidirectional:\n\n\n\n\n\nAWhen pushing to remote for the first time, you first set up the connection with git remote and use the u option to git push.\n\n\n\n\n\n\n\n\nBNow, the local (“My repo”) and remote (“GitHub”) are in sync.\n\n\n\n\n\n\n\n\n\n\nCNext, you’ve made a new commit locally: the local and remote are out of sync.\n\n\n\n\n\n\n\n\nDYou simply use git push to update the remote, after which the local and remote will be back in sync (latter not shown).\n\n\n\n\n\nIn a single-user workflow with a remote, you commit just like you would without a remote in your day-to-day work, and in addition, push to remote occasionally — let’s run through an example.\n\nStart by creating a README.md file for your repo:\necho \"# Origin\" &gt; README.md\necho \"Repo for book draft on my new **theory**\" &gt;&gt; README.md\nAdd and commit the file:\ngit add README.md\ngit commit -m \"Added a README file\"\n[main 63ce484] Added a README file\n1 file changed, 2 insertions(+)\ncreate mode 100644 README.md\nIf you now run git status, you’ll see that Git knows that your local repo is now one commit “ahead” of its remote counterpart:\nOn branch main\nYour branch is ahead of 'origin/main' by 1 commit.\n(use \"git push\" to publish your local commits)\n\nnothing to commit, working tree clean\nBut Git will not automatically sync. So now, push to the remote repository:\ngit push\nCounting objects: 4, done.\nDelta compression using up to 40 threads.\nCompressing objects: 100% (3/3), done.\nWriting objects: 100% (3/3), 404 bytes | 0 bytes/s, done.\nTotal 3 (delta 0), reused 0 (delta 0)\nremote: To git@github.com:jelmerp/originspecies.git\n4968e62..b1e6dad  main -&gt; main\n\n\nLet’s go back to GitHub: note that the README.md Markdown has been automatically rendered!\n\n\n\n\n\n\n\n\n\n\n\n\nWant to collaborate on repositories?\n\n\n\nThe version control bonus page goes into collaboration with Git and Github, i.e. multi-user workflows."
  },
  {
    "objectID": "week04/w4_2_github.html#footnotes",
    "href": "week04/w4_2_github.html#footnotes",
    "title": "Git: Remotes on GitHub",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that Git is available at OSC even without loading this module, but that’s a much older version.↩︎\n We will not cover collaboration workflows in class, though – see the bonus material for this.↩︎\nThough note that these names don’t have to match up.↩︎\n Though we can skip the second one, git branch -M main, since our branch is already called “main”.↩︎\n To learn more about branches, see the Git bonus material.↩︎"
  },
  {
    "objectID": "week03/w3_3_shellfiles.html#overview-setting-up",
    "href": "week03/w3_3_shellfiles.html#overview-setting-up",
    "title": "Advanced file management in the shell",
    "section": "1 Overview & setting up",
    "text": "1 Overview & setting up\nIn this session, we will learn some more Unix shell skills, focusing on commands to manage files with an eye on organizing your research projects.\nSpecifically, we will learn about:\n\nWildcard expansion to select and operate on multiple files at once\nCommand substitution to save the output of commands\nFor loops to repeat operations, e.g. across files\nRenaming multiple files using for loops\n\n\n\n1.1 VS Code setup\n\n\n\n\n\n\nStarting VS Code at OSC - with a Terminal (Click to expand)\n\n\n\n\n\n\nLog in to OSC’s OnDemand portal at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then near the bottom of the dropdown menu, click Code Server.\nIn the form that appears on a new page:\n\nSelect OSC project PAS2700\nThe starting directory: /fs/ess/PAS2700/users/&lt;user&gt; (replace &lt;user&gt; with your OSC username)\nNumber of hours: 2\nClick Launch.\n\nOn the next page, once the top bar of the box has turned green and says Runnning, click Connect to VS Code.\nOpen a Terminal by clicking      =&gt; Terminal =&gt; New Terminal.\nType pwd to check where you are.\nIf you are not in /fs/ess/PAS2700/users/&lt;user&gt; click      =&gt;   File   =&gt;   Open Folder, then type/select /fs/ess/PAS2700/users/&lt;user&gt; and press OK.\n\n\n\n\n\n\n\n1.2 Create a dummy project – following Buffalo\nGo into the dir for this week that you created earlier:\n# You should be in /fs/ess/PAS2700/users/$USER/\ncd week03\nFirst, we’ll create a set of directories representing a dummy research project:\nmkdir zmays-snps\ncd zmays-snps\n\n# The -p option for mkdir will allow for 'recursive' (nested) dir creation\nmkdir -p data/fastq scripts results/figs\nWe will use the touch command to create empty files that are mock representations of FASTQ sequence files with forward (“R1”) and reverse (“R2”) DNA sequence reads for 3 samples:\ncd data/fastq\n\ntouch sample1_R1.fastq.gz sample1_R2.fastq.gz\ntouch sample2_R1.fastq.gz sample2_R2.fastq.gz\ntouch sample3_R1.fastq.gz sample3_R2.fastq.gz\nFor a nice recursive overview of your directory structure, use the tree command (with option -C to show colors):\n# \"../..\" tells tree to start two levels up\n# (Output colors are not shown on this webpage)\ntree -C ../..\n../..\n├── data\n│   └── fastq\n│       ├── sample1_R1.fastq.gz\n│       ├── sample1_R2.fastq.gz\n│       ├── sample2_R1.fastq.gz\n│       ├── sample2_R2.fastq.gz\n│       ├── sample3_R1.fastq.gz\n│       └── sample3_R2.fastq.gz\n├── results\n│   └── figs\n└── scripts\n\n5 directories, 6 files"
  },
  {
    "objectID": "week03/w3_3_shellfiles.html#wildcard-expansion",
    "href": "week03/w3_3_shellfiles.html#wildcard-expansion",
    "title": "Advanced file management in the shell",
    "section": "2 Wildcard expansion",
    "text": "2 Wildcard expansion\nShell wildcard expansion is a very useful technique to select files. Selecting files with wildcard expansion is called globbing. Wildcards are symbols that have a special meaning.\n\nThe * wildcard\nIn globbing, the * wildcard matches any number of any character (including “nothing” – the absence of any characters).\nWith the following files in our directory…\nls\nsample1_R1.fastq.gz  sample1_R2.fastq.gz  sample2_R1.fastq.gz\nsample2_R2.fastq.gz  sample3_R1.fastq.gz  sample3_R2.fastq.gz\n…we can match both “sample1” files as follows:\nls sample1*\nsample1_R1.fastq.gz  sample1_R2.fastq.gz\nls sample1*fastq.gz\nsample1_R1.fastq.gz  sample1_R2.fastq.gz\n\nTo match only files with forward reads (contain “_R1”):\nls *_R1*\nsample1_R1.fastq.gz  sample2_R1.fastq.gz  sample3_R1.fastq.gz\nls *R1.fastq.gz\nsample1_R1.fastq.gz  sample2_R1.fastq.gz  sample3_R1.fastq.gz\n\nWhen globbing, the pattern has to match the entire file name (i.e. no partial matches like with grep), so this doesn’t match anything:\n# There are no files that _end in_ R1: we'd need another asterisk at the end\nls *R1\nls: cannot access *R1: No such file or directory\n\nIn summary:\n\n\n\n\n\n\n\nPattern\nMatches files whose names…\n\n\n\n\n*\nContain anything (matches all files)1\n\n\n*fastq.gz\nEnd in “.fastq.gz”\n\n\nsample1*\nStart with “sample1”\n\n\n*_R1*\nContain “_R1”\n\n\n\n\n\n\n Exercise: File matching 1\n\nList only the FASTQ files for sample 3.\n\n\n\nClick for the solution\n\nls sample3*\nsample3_R1.fastq.gz  sample3_R2.fastq.gz\n\n\nWhich files would ls samp*le* match?\n\n\n\nClick for the solution\n\nAll of them, since all file names start with sample, and because * also matches “zero characters”, there is no requirement for there to be a character between the p and the l.\n\n\n\n\nOther shell wildcards\nThere are two more shell wildcards — here is a complete overview of shell wildcards:\n\n\n\n\n\n\n\nWildcard\nMatches\n\n\n\n\n*\nAny number of any character, including nothing\n\n\n?\nAny single character\n\n\n[] and [^]\nOne or none (^) of the “character set” within the brackets\n\n\n\n\nUsing the more restrictive wildcard ? can be useful to prevent accidental matches. Here is an example of using the ? wildcard to match both R1 and R2:\nls sample1_R?.fastq.gz\nsample1_R1.fastq.gz  sample1_R2.fastq.gz\n\nThe character class wildcard [] can be useful when wanting to include or exclude multiple files among files with rather similar names. For example, to match files for sample1 and sample2 using only a character class with []:\n\nMethod 1 — List all possible characters (1 and 2 in this case):\nls sample[12]*\nsample1_R1.fastq.gz  sample1_R2.fastq.gz  sample2_R1.fastq.gz  sample2_R2.fastq.gz\nMethod 2 – Use a range like [0-9], [A-Z], [a-z]:\nls sample[1-2]*\nsample1_R1.fastq.gz  sample1_R2.fastq.gz  sample2_R1.fastq.gz  sample2_R2.fastq.gz\nMethod 3 – Exclude the unwanted sample ID:\nls sample[^3]*\nsample1_R1.fastq.gz  sample1_R2.fastq.gz  sample2_R1.fastq.gz  sample2_R2.fastq.gz\n\n\n\n\n\n\n\nNote: [] works on single character ranges only: 0-9 works but 10-13 does not.\n\n\n\n\n\n\nThe examples so far may seem trivial, but you can use these techniques to easily operate on selections among 100s or 1000s of files.\n\n\n\nExpansion is done by the shell itself\nIt is important to realize that the “expansion” –of a wildcard to matching file names– is done by the shell and not by ls or another command you might be using wildcards with.\nThis means that a command like ls will “see”/“receive” the list of files after the expansion has already happened. Therefore, glob expansion will work with any command that accepts (multiple) paths, such as copy (cp command), move (mv) or delete (rm).\nAdditionally, we can first check which files those commands will “see” by first using echo (or ls) with the exact same globbing pattern:\n# 1. First check which files are selected \necho sample[12]*\nsample1_R1.fastq.gz sample1_R2.fastq.gz sample2_R1.fastq.gz sample2_R2.fastq.gz\n# 2. Now we feel comfortable to remove the files with rm\n# (The -v option will make rm report what it's removing)\nrm -v sample[12]*\nremoved ‘sample1_R1.fastq.gz’\nremoved ‘sample1_R2.fastq.gz’\nremoved ‘sample2_R1.fastq.gz’\nremoved ‘sample2_R2.fastq.gz’\n\n\n\n\n\n\nWildcards vs. regular expressions\n\n\n\nDon’t confuse wildcards with regular expressions! You may have used regular expressions before, for example with R or a text editor. They are similar to but not the same as shell wildcards. We’ll talk about regular expressions in ? (TBA)"
  },
  {
    "objectID": "week03/w3_3_shellfiles.html#variables-and-command-substitution",
    "href": "week03/w3_3_shellfiles.html#variables-and-command-substitution",
    "title": "Advanced file management in the shell",
    "section": "3 Variables and command substitution",
    "text": "3 Variables and command substitution\nNow, we’ll cover a few topics that do not directly relate to files, but that you need to know about before the application at the end of this session: renaming files with a loop. Additionally, you will keep using these constructs throughout the course.\n\n3.1 Variables\nVariables are truly ubiquitous in programming. We typically use them for items that:\n\nWe refer to repeatedly and/or\nAre subject to change.\n\nThese tend to be settings like the paths to input and output files, and parameter values for programs. Using variables makes it easier to change such settings and makes it possible to write scripts and programs that are flexible depending on user input. Earlier, we already saw a couple of handy applications of variables: for example, the environment variable $USER contains our user name.\n\nAssigning and referencing variables\nTo assign a value to a variable in the shell, use the syntax variable_name=value:\n# Assign the value \"beach\" to a variable with the name \"location\":\nlocation=beach\n\n# Assign the value \"200\" to a variable with the name \"n_lines\":\nn_lines=200\n\n\n\n\n\n\nRecall that there can’t be spaces around the equals sign (=)!\n\n\n\n\n\n\nTo reference a variable (i.e., to access its value):\n\nYou need to put a dollar sign $ in front of its name.\nIt is good practice to double-quote (\"...\") variable names2.\n\nAs before with the environment variables $USER and $HOME, we’ll use the echo command to see what values our variables contain:\necho \"$location\"\nbeach\necho \"$n_lines\"\n200\nConveniently, we can use variables in lots of contexts, as if we had directly typed their values:\ninput_file=results/figs/fig-1A.png\n\nls -lh \"$input_file\"\n-rw-rw----+ 1 jelmer PAS0471 0 Mar  7 13:17 results/figs/fig-1A.png\nYou will learn more about variables and their usage when we cover shell scripts.\n\n\n\n\n3.2 Command substitution\nCommand substitution allows you to store the output of a command in a variable.\nLet’s see an example. As you know, the date command will print the current date and time:\ndate\nThu Mar  7 14:52:22 EST 2024\nIf we try to store the date in a variable directly, it doesn’t work: the literal string “date” is stored instead:\ntoday=date\necho \"$today\"\ndate\nThat’s why we need command substitution, which we can use by wrapping the command inside $():\ntoday=$(date)\necho \"$today\"\nThu Mar  7 14:53:11 EST 2024\n\nOne practical example of using command substitution is when you want to automatically include the current date in a file name. First, note that we can use date +%F to print the date in YYYY-MM-DD format, and omit the time:\ndate +%F\n2024-03-07\nLet’s use that in a command substitution — but a bit differently than before: we use the command substitution $(date +%F) directly in our touch command, rather than first assigning it to a variable:\ntouch README_\"$(date +%F)\".txt\n\nls\nREADME_2024-03-07.txt\n\n\n Exercise: Command substitution\nSay we wanted to store and report the number of lines in a FASTQ file, which tells us how many sequence “reads” are in it (because FASTQ files contain 4 lines per read).\nHere is how we can get the number of lines of a compressed FASTQ file:\n\nUse zcat (instead of regular cat) to print the contents despite the file compression\nAs we’ve seen before, wc -l gets you the number of lines, but note here that if you pipe input into wc -l, it won’t include the file name in the output:\n\nzcat /fs/ess/PAS2700/share/garrigos/data/fastq/ERR10802863_R1.fastq.gz | wc -l\n2000000\nUse command substitution to store the output of the last command in a variable, and then use an echo command to print the following:\nThe file has 2000000 lines\n\n\nClick for the solution\n\nn_lines=$(zcat /fs/ess/PAS2700/share/garrigos/data/fastq/ERR10802863_R1.fastq.gz | wc -l)\n\necho \"The file has $n_lines lines\"\nThe file has 2000000 lines\nNote: You don’t have to quote variables inside a quoted echo call, since it’s, well, already quoted. If you also quote the variables, you will in fact unquote it, although that shouldn’t pose a problem inside echo statements.\n\n\n\n\n\n3.3 For loops\nLoops are another universal element of programming languages, and are used to repeat operations. Here, we’ll only cover the most common type of loop: the for loop.\nA for loop iterates over a collection, such as a list of files, and allows you to perform one or more actions for each element in the collection. In the example below, our “collection” is just a short list of numbers (1, 2, and 3):\nfor a_number in 1 2 3; do\n    echo \"In this iteration of the loop, the number is $a_number\"\n    echo \"--------\"\ndone\nIn this iteration of the loop, the number is 1\n--------\nIn this iteration of the loop, the number is 2\n--------\nIn this iteration of the loop, the number is 3\n--------\nThe indented lines between do and done contain the code that is being executed as many times as there are items in the collection: in this case 3 times, as you can tell from the output above.\nWhat was actually run under the hood is the following:\n# (Don't run this)\na_number=1\necho \"In this iteration of the loop, the number is $a_number\"\necho \"--------\"\n\na_number=2\necho \"In this iteration of the loop, the number is $a_number\"\necho \"--------\"\n\na_number=3\necho \"In this iteration of the loop, the number is $a_number\"\necho \"--------\"\nHere are two key things to understand about for loops:\n\nIn each iteration of the loop, one element in the collection is being assigned to the variable specified after for. In the example above, we used a_number as the variable name, so that variable contained 1 when the loop ran for the first time, 2 when it ran for the second time, and 3 when it ran for the third and last time.\nThe loop runs sequentially for each item in the collection, and will run exactly as many times as there are items in the collection.\n\n\n\n\n\n\n\nA further explanation of for loop syntax\n\n\n\nOn the first and last, unindented lines, for loops contain the following mandatory keywords:\n\n\n\n\n\n\n\nKeyword\nPurpose\n\n\n\n\nfor\nAfter for, we set the variable name (an arbitrary name; above we used a_number)\n\n\nin\nAfter in, we specify the collection (list of items) we are looping over\n\n\ndo\nAfter do, we have one ore more lines specifying what to do with each item\n\n\ndone\nTells the shell we are done with the loop\n\n\n\n\n\n\n\nCombining loops and globbing\nA very useful strategy is to loop over files with globbing, for example:\nfor fastq_file in data/fastq/*fastq; do\n    echo \"Running an analysis for file $fastq_file\"...\n    # [This is where you would put additional commands to process each FASTQ file]\ndone\nRunning an analysis for file data/fastq/sample001_R1.fastq...\nRunning an analysis for file data/fastq/sample001_R2.fastq...\nRunning an analysis for file data/fastq/sample002_R1.fastq...\nRunning an analysis for file data/fastq/sample002_R2.fastq...\nRunning an analysis for file data/fastq/sample003_R1.fastq...\nRunning an analysis for file data/fastq/sample003_R2.fastq...\n# [...output truncated...]\n\n\n\n Exercise: A simple loop\nCreate a loop that will print:\nmorel is an Ohio mushroom  \ndestroying_angel is an Ohio mushroom  \neyelash_cup is an Ohio mushroom\n\n\nClick for the solution\n\nfor mushroom in morel destroying_angel eyelash_cup; do\n    echo \"$mushroom is an Ohio mushroom\"\ndone\nmorel is an Ohio mushroom  \ndestroying_angel is an Ohio mushroom  \neyelash_cup is an Ohio mushroom"
  },
  {
    "objectID": "week03/w3_3_shellfiles.html#renaming-files-with-loops",
    "href": "week03/w3_3_shellfiles.html#renaming-files-with-loops",
    "title": "Advanced file management in the shell",
    "section": "4 Renaming files with loops",
    "text": "4 Renaming files with loops\nSometimes, you need to rename many files in a systematic/repetitive way. This is relatively common with omics data files, as you will often have separate files for each sample, and you may have many dozens of samples. Manually renaming these one-by-one is tedious as well as extremely error-prone, while doing this with code is a very nice example of a general task that you can apply your coding skills to. For example, maybe you have a few thousand photos that you want to rename in a systematic way?\nThere are many different ways to rename many files in a programmatic way in the shell – admittedly none as easy as one might have hoped. Here, we’ll use the basename command and a for loop. for loops are a verbose method for tasks like renaming, but are relatively intuitive and good to get practice with because you will use them in other contexts throughout the course.\nWe will start with creating some files…TBA\n# You should be in /fs/ess/PAS2700/users/$USER/week03\ntouch data/fastq/sample{001..100}_R{1,2}.fastq\n\nbasename\nWe will first have to learn about the basename command, which removes any dir name that may be present in a file path, and optionally, removes a suffix too:\n# Just remove the directory part of the path:\nbasename data/fastq/sample001_R1.fastq\nsample001_R1.fastq\n# Also remove a suffix by specifying it after the file name:\nbasename data/fastq/sample001_R1.fastq .fastq\nsample001_R1\n\n\n\nRenaming a single file\nLet’s say that we wanted to rename these files so that they have the suffix .fq instead of .fastq. Here’s how we could do that for one file in a way that we can use in a loop:\nThe original file name will be contained in a variable:\noldname=sample001_R1.fastq\nWe can also save the new name in a variable\nnewname=$(basename \"$oldname\" .fastq).fq\nBefore actually renaming, note this trick with echo to just print the command instead of executing it:\necho mv -v \"$oldname\" \"$newname\"\nmv -v sample001_R1.fastq sample001_R1.fq\nLooks good? Then we remove echo and rename the file (we’re using the -v to make mv report what it’s doing):\nmv -v \"$oldname\" \"$newname\"\nsample001_R1.fastq -&gt; sample001_R1.fq\n\n\n\nLooping over all files\nHere’s how we can loop over these files, saving each file name (one at a time) in the variable $oldname:\nfor oldname in *.fastq; do\n    # ...\ndone\nNext, we assign a new name for each file:\nfor oldname in *.fastq; do\n    newname=$(basename \"$oldname\" .fastq).fq\ndone\nWe build and check the renaming command:\nfor oldname in *.fastq; do\n    newname=$(basename \"$oldname\" .fastq).fq\n    echo mv -v \"$oldname\" \"$newname\"\ndone\nmv -v sample001_R1_001.fastq sample001_R1_001.fq\nmv -v sample001_R2_001.fastq sample001_R2_001.fq\nmv -v sample002_R1_001.fastq sample002_R1_001.fq\nmv -v sample002_R2_001.fastq sample002_R2_001.fq\n# [...output truncated...]\nWe do the renaming by removing echo:\nfor oldname in *.fastq; do\n    newname=$(basename \"$oldname\" .fastq).fq\n    mv -v \"$oldname\" \"$newname\"\ndone\n‘sample001_R1_001.fastq’ -&gt; ‘sample001_R1_001.fq’\n‘sample001_R2_001.fastq’ -&gt; ‘sample001_R2_001.fq’\n‘sample002_R1_001.fastq’ -&gt; ‘sample002_R1_001.fq’\n‘sample002_R2_001.fastq’ -&gt; ‘sample002_R2_001.fq’\n‘sample003_R1_001.fastq’ -&gt; ‘sample003_R1_001.fq’\n‘sample003_R2_001.fastq’ -&gt; ‘sample003_R2_001.fq’\n# [...output truncated...]"
  },
  {
    "objectID": "week03/w3_3_shellfiles.html#bonus-brace-expansion",
    "href": "week03/w3_3_shellfiles.html#bonus-brace-expansion",
    "title": "Advanced file management in the shell",
    "section": "5 Bonus: Brace expansion",
    "text": "5 Bonus: Brace expansion\nWhereas wildcard expansion looks for corresponding files and expands to whichever files are present, brace expansion with {}, is another type of shell expansion that expands to whatever you tell it to.\n# First move up to zmays-snps\ncd ../..\nUse .. within {} to indicate ranges of numbers or letters:\n# Here we'll create 31 _dirs_ for different dates\nmkdir -p data/obs/2024-03-{01..31}\n\nls data/obs\n2024-03-01  2024-03-04  2024-03-07  2024-03-10  2024-03-13  2024-03-16  2024-03-19  2024-03-22  2024-03-25  2024-03-28  2024-03-31\n2024-03-02  2024-03-05  2024-03-08  2024-03-11  2024-03-14  2024-03-17  2024-03-20  2024-03-23  2024-03-26  2024-03-29\n2024-03-03  2024-03-06  2024-03-09  2024-03-12  2024-03-15  2024-03-18  2024-03-21  2024-03-24  2024-03-27  2024-03-30\n# Here we'll create 6 empty _files_\ntouch results/figs/fig-1{A..F}.png\n\nls results/figs\nfig-1A.png  fig-1B.png  fig-1C.png  fig-1D.png  fig-1E.png  fig-1F.png\n\nFinally, you can also use a comma-separated list, and multiple brace expansions — with the latter, you will get all combinations among values in the expansions:\nmkdir -p data/obs2/treatment-{Kr,Df,Tr}_temp-{lo,med,hi}\n\nls data/obs2\ntreatment-Df_temp-hi   treatment-Kr_temp-hi   treatment-Tr_temp-hi\ntreatment-Df_temp-lo   treatment-Kr_temp-lo   treatment-Tr_temp-lo\ntreatment-Df_temp-med  treatment-Kr_temp-med  treatment-Tr_temp-med\n\n\n Bonus exercise: More file matching\n\nMove back into data/fastq/ and remove all (remaining) files in there in one go.\n\n\n\nClick for the solution\n\ncd data/fastq/   # Assuming you were still in /fs/ess/PAS2700/users/$USER/week03/zmays-snps\n\n# (You don't have to use the -v flag)\nrm -v *fastq.gz\nremoved ‘sample3_R1.fastq.gz’\nremoved ‘sample3_R2.fastq.gz’\n\n\nUsing brace expansion and the touch command, create empty R1 and R2 FASTQ files for 100 samples with IDs from 001 to 100: sample&lt;ID&gt;_R1.fastq and sample&lt;ID&gt;_R2.fastq.\n\n\n\nClick for the solution\n\ntouch sample{001..100}_R{1,2}.fastq\n\nls\nsample001_R1.fastq  sample026_R1.fastq  sample051_R1.fastq  sample076_R1.fastq\nsample001_R2.fastq  sample026_R2.fastq  sample051_R2.fastq  sample076_R2.fastq\nsample002_R1.fastq  sample027_R1.fastq  sample052_R1.fastq  sample077_R1.fastq\nsample002_R2.fastq  sample027_R2.fastq  sample052_R2.fastq  sample077_R2.fastq\nsample003_R1.fastq  sample028_R1.fastq  sample053_R1.fastq  sample078_R1.fastq\nsample003_R2.fastq  sample028_R2.fastq  sample053_R2.fastq  sample078_R2.fastq\nsample004_R1.fastq  sample029_R1.fastq  sample054_R1.fastq  sample079_R1.fastq\nsample004_R2.fastq  sample029_R2.fastq  sample054_R2.fastq  sample079_R2.fastq\nsample005_R1.fastq  sample030_R1.fastq  sample055_R1.fastq  sample080_R1.fastq\nsample005_R2.fastq  sample030_R2.fastq  sample055_R2.fastq  sample080_R2.fastq\nsample006_R1.fastq  sample031_R1.fastq  sample056_R1.fastq  sample081_R1.fastq\nsample006_R2.fastq  sample031_R2.fastq  sample056_R2.fastq  sample081_R2.fastq\nsample007_R1.fastq  sample032_R1.fastq  sample057_R1.fastq  sample082_R1.fastq\nsample007_R2.fastq  sample032_R2.fastq  sample057_R2.fastq  sample082_R2.fastq\nsample008_R1.fastq  sample033_R1.fastq  sample058_R1.fastq  sample083_R1.fastq\nsample008_R2.fastq  sample033_R2.fastq  sample058_R2.fastq  sample083_R2.fastq\nsample009_R1.fastq  sample034_R1.fastq  sample059_R1.fastq  sample084_R1.fastq\nsample009_R2.fastq  sample034_R2.fastq  sample059_R2.fastq  sample084_R2.fastq\nsample010_R1.fastq  sample035_R1.fastq  sample060_R1.fastq  sample085_R1.fastq\nsample010_R2.fastq  sample035_R2.fastq  sample060_R2.fastq  sample085_R2.fastq\nsample011_R1.fastq  sample036_R1.fastq  sample061_R1.fastq  sample086_R1.fastq\nsample011_R2.fastq  sample036_R2.fastq  sample061_R2.fastq  sample086_R2.fastq\nsample012_R1.fastq  sample037_R1.fastq  sample062_R1.fastq  sample087_R1.fastq\nsample012_R2.fastq  sample037_R2.fastq  sample062_R2.fastq  sample087_R2.fastq\nsample013_R1.fastq  sample038_R1.fastq  sample063_R1.fastq  sample088_R1.fastq\nsample013_R2.fastq  sample038_R2.fastq  sample063_R2.fastq  sample088_R2.fastq\nsample014_R1.fastq  sample039_R1.fastq  sample064_R1.fastq  sample089_R1.fastq\nsample014_R2.fastq  sample039_R2.fastq  sample064_R2.fastq  sample089_R2.fastq\nsample015_R1.fastq  sample040_R1.fastq  sample065_R1.fastq  sample090_R1.fastq\nsample015_R2.fastq  sample040_R2.fastq  sample065_R2.fastq  sample090_R2.fastq\nsample016_R1.fastq  sample041_R1.fastq  sample066_R1.fastq  sample091_R1.fastq\nsample016_R2.fastq  sample041_R2.fastq  sample066_R2.fastq  sample091_R2.fastq\nsample017_R1.fastq  sample042_R1.fastq  sample067_R1.fastq  sample092_R1.fastq\nsample017_R2.fastq  sample042_R2.fastq  sample067_R2.fastq  sample092_R2.fastq\nsample018_R1.fastq  sample043_R1.fastq  sample068_R1.fastq  sample093_R1.fastq\nsample018_R2.fastq  sample043_R2.fastq  sample068_R2.fastq  sample093_R2.fastq\nsample019_R1.fastq  sample044_R1.fastq  sample069_R1.fastq  sample094_R1.fastq\nsample019_R2.fastq  sample044_R2.fastq  sample069_R2.fastq  sample094_R2.fastq\nsample020_R1.fastq  sample045_R1.fastq  sample070_R1.fastq  sample095_R1.fastq\nsample020_R2.fastq  sample045_R2.fastq  sample070_R2.fastq  sample095_R2.fastq\nsample021_R1.fastq  sample046_R1.fastq  sample071_R1.fastq  sample096_R1.fastq\nsample021_R2.fastq  sample046_R2.fastq  sample071_R2.fastq  sample096_R2.fastq\nsample022_R1.fastq  sample047_R1.fastq  sample072_R1.fastq  sample097_R1.fastq\nsample022_R2.fastq  sample047_R2.fastq  sample072_R2.fastq  sample097_R2.fastq\nsample023_R1.fastq  sample048_R1.fastq  sample073_R1.fastq  sample098_R1.fastq\nsample023_R2.fastq  sample048_R2.fastq  sample073_R2.fastq  sample098_R2.fastq\nsample024_R1.fastq  sample049_R1.fastq  sample074_R1.fastq  sample099_R1.fastq\nsample024_R2.fastq  sample049_R2.fastq  sample074_R2.fastq  sample099_R2.fastq\nsample025_R1.fastq  sample050_R1.fastq  sample075_R1.fastq  sample100_R1.fastq\nsample025_R2.fastq  sample050_R2.fastq  sample075_R2.fastq  sample100_R2.fastq\n\n\n\nBonus: Count the number of “R1” files by first using ls with a globbing pattern that only selects R1 files, and then piping the ls output into wc -l.\n\n\n\nClick for the solution\n\n# wc -l will count the number of lines, i.e. the number of files\n# (Note that this works properly even though raw ls output may\n# put multiple files on 1 line.)\nls *R1*fastq | wc -l\n100\n\n\nBonus: Copy all files except the two for “sample100” into a new directory called selection — use a wildcard to do the move with a single command. (You will first need to create the new dir separately.)\n\n\n\nClick for the solution\n\nFirst create the selection dir:\nmkdir selection\nMethod 1 — Exclude sample numbers starting with a 1:\ncp sample[^1]* selection/\nMethod 2 — Other way around; include sample numbers starting with a 0:\ncp sample0* selection/"
  },
  {
    "objectID": "week03/w3_3_shellfiles.html#footnotes",
    "href": "week03/w3_3_shellfiles.html#footnotes",
    "title": "Advanced file management in the shell",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nExcept so-called hidden files.↩︎\nWe’ll talk more about quoting later.↩︎"
  },
  {
    "objectID": "week03/w3_2_vscode-markdown.html#vs-code",
    "href": "week03/w3_2_vscode-markdown.html#vs-code",
    "title": "VS Code and Markdown",
    "section": "1 VS Code",
    "text": "1 VS Code\n\n1.1 Why VS Code?\nVS Code is basically a fancy text editor. Its full name is Visual Studio Code, and it’s also called “Code Server” at OSC.\nTo emphasize the additional functionality relative to basic text editors like Notepad and TextEdit, editors like VS Code are also referred to as “IDEs”: Integrated Development Environments. The RStudio program is another good example of an IDE. Just like RStudio is an IDE for R, VS Code will be our IDE for shell code.\nSome advantages of VS Code:\n\nWorks with all operating systems, is free, and open source.\nHas an integrated terminal.\nVery popular nowadays – lots of development going on including by users (extensions).\nAvailable at OSC OnDemand (and also allows you to “SSH-tunnel” in with your local installation).\n\n\n\n\n1.2 Starting VS Code at OSC\n\nLog in to OSC’s OnDemand portal at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and near the bottom, click Code Server.\nInteractive Apps like VS Code and RStudio run on compute nodes (not login nodes). Because compute nodes always need to be “reserved”, we have to fill out a form and specify the following details:\n\nThe “Cluster” that we want to use: ascend\nThe “Account”, i.e. the OSC Project that we want to bill for the compute node usage: PAS2880.\nThe “Number of hours” we want to make a reservation for: 2\nThe “Working Directory” for the program: your personal folder in /fs/ess/PAS2880/users (e.g. /fs/ess/PAS2880/users/jelmer)\nThe “Codeserver Version”: 4.8 (should be the only one)\n\nClick Launch — you will be sent to the “My Interactive Sessions” page with a box for your job at the top.\nFirst, your job may be “Queued” for some seconds (i.e., waiting for computing resources to be assigned to it), but it should soon switch to “Starting” and then be ready for usage (“Running”) in another couple of seconds:\n\n\n\n\n\n\n\nOnce the blue Connect to VS Code button appears, click that to open VS Code in a new browser tab.\nWhen VS Code opens, you may get these two pop-ups (and possibly some others) — click “Yes” (and check the box) and “Don’t Show Again”, respectively:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou’ll also get a Get Started/Welcome page — you don’t have to go through any steps that may be suggested there.\n\n\n\n\n1.3 The VS Code User Interface\n\n\n\n\n\n\nSide bars\nThe Activity Bar (narrow side bar) on the far left has:\n\nA  (“hamburger menu”), which has menu items like File that you often find in a top bar.\nA  (cog wheel icon) in the bottom, through which you can mainly access settings.\nIcons to toggle (wide/Primary) Side Bar options:\n\nExplorer: File browser & outline for the active file.\nSearch: To search recursively across all files in the active folder.\nSource Control: To work with Git (next week).\nDebugger\nExtensions: To install extensions (up soon).\n\n\n\n\n\n\n\n\nToggle (hide/show) the side bars\n\n\n\nIf you want to save some screen space while coding along in class, you may want to occasionally hide the side bars:\n\nIn  &gt; View &gt; Appearance you can toggle both the Activity Bar and the Primary Side Bar.\nOr use keyboard shortcuts:\n\nCtrl/⌘+B for the primary/wide side bar\nCtrl+Shift+B for the activity/narrow side bar\n\n\n\n\n\n\n\n Exercise: Try a few color themes\n\nAccess the “Color Themes” option by clicking  =&gt; Color Theme.\nTry out a few themes and see pick one you like!\n\n\n\n\nTerminal (with a Unix shell)\n Open a terminal by clicking      =&gt; Terminal =&gt; New Terminal.\nCreate a directory for this week, e.g.:\n# You should be in your personal dir in /fs/ess/PAS2880\npwd\n/fs/ess/PAS2880/users/jelmer\nmkdir week03\n\n\n\nEditor pane and Get Started document\nThe main part of the VS Code is the editor pane. Here, we can open files like scripts and other types of text files, and images. (Whenever you open VS Code, an editor tab with a Get Started document is automatically opened. This provides some help and some shortcuts like to recently opened files and folders.)\n Let’s create and save a new file:\n\nOpen a new file: Click the hamburger menu , then File &gt; New File.\nSave the file (Ctrl/⌘+S), inside the dir you just created, as a Markdown file, e.g. markdown-intro.md. (Markdown files have the extension .md.)\n\n\n\n\n\n1.4 A folder as a starting point\nConveniently, VS Code takes a specific directory as a starting point in all parts of the program:\n\nIn the file explorer in the side bar\nIn the terminal\nWhen saving files in the editor pane.\n\nThis is why your terminal was “already” located in /fs/ess/PAS2880/users/$USER.\n\n\n\n\n\n\nIf you need to switch folders, click      &gt;   File   &gt;   Open Folder.\n\n\n\n\n\n\n\n\n\n\n\n\nTaking off where you were\n\n\n\nWhen you reopen a folder you’ve had open before, VS Code will resume where you were before in that it will:\n\nRe-open any files you had open in the editor pane\nRe-open a terminal if you had one active\n\nThis is quite convenient, especially when you start working on multiple projects and frequently switch between those.\n\n\n\n\nSome tips and tricks\n\nResizing panes\nYou can resize panes (terminal, editor, side bar) by hovering your cursor over the borders and then dragging.\nThe Command Palette\nTo access all the menu options that are available in VS Code, the so-called “Command Palette” can be handy, especially if you know what you are looking for. To access the Command Palette, click      and then Command Palette (or press F1 or Ctrl/⌘+Shift+P). To use it, start typing something to look for an option.\nKeyboard shortcuts\nFor a single-page PDF overview of keyboard shortcuts for your operating system:    =&gt;   Help   =&gt;   Keyboard Shortcut Reference. (Or for direct links to these PDFs: Windows / Mac / Linux.) A couple of useful keyboard shortcuts are highlighted below.\n\n\n\n\n\n\n\nSpecific useful VS Code keyboard shortcuts (Click to expand)\n\n\n\n\n\nWorking with keyboard shortcuts for common operations can be a lot faster than using your mouse. Below are some useful ones for VS Code (for Mac, in some case, you’ll have to replace Ctrl with ⌘):\n\nOpen a terminal: Ctrl+` (backtick) or Ctrl+Shift+C.\nToggle between the terminal and the editor pane: Ctrl+` and Ctrl+1.\nLine actions:\n\nCtrl/⌘+X / C will cut/copy the entire line where the cursor is, when nothing is selected (!)\nCtrl/⌘+Shift+K will delete a line\nAlt/Option+⬆/⬇ will move lines up or down.\n\n\n\n\n\n\n\n Exercise: Install two extensions\nClick the gear icon  and then Extensions, and search for and then install:\n\nshellcheck (by simonwong) — this will check our shell scripts later on!\nRainbow CSV (by mechatroner) — make CSV/TSV files easier to view with column-based colors"
  },
  {
    "objectID": "week03/w3_2_vscode-markdown.html#an-introduction-to-markdown",
    "href": "week03/w3_2_vscode-markdown.html#an-introduction-to-markdown",
    "title": "VS Code and Markdown",
    "section": "2 An introduction to Markdown",
    "text": "2 An introduction to Markdown\nMarkdown is a very lightweight text markup language that can be used in plain-text files. A source Markdown file can be “rendered” to produce an output file in a variety of formats like HTML and PDF.\nFor example, surrounding one or more characters by single or double asterisks (*) will make those characters italic or bold, respectively:\n\nWhen you write *italic example* this will be rendered as: italic example.\nWhen you write **bold example**this will be rendered as: bold example.\n\nMarkdown is:\n\nEasy to write — a dozen or so syntax constructs, like the two above, is nearly all you use.\nEasy to read in source (“non-rendered”) form, and editors like VS Code will apply some formatting even in these source files.\n\nI recommend that you use Markdown files (with a .md extension) instead of regular text (.txt) files to document your research projects as outlined in the previous session.\n\n\n\n\n\n\nMarkdown documentation\n\n\n\nLearn more about Markdown and its syntax in this excellent documentation: https://www.markdownguide.org.\n\n\n\n\nMarkdown in VS Code\nBelow, we’ll be trying some Markdown syntax in the markdown-intro.md file we created earlier.\nWhen you save a file in VS Code with an .md extension, as you have done:\n\nSome formatting will be automatically applied in the editor.\nYou can open a live rendered preview by pressing the icon to “Open Preview to the Side” (top-right corner):\n\n\n\n\nThe Markdown preview icon in VS Code\n\n\nThat will look something like this in VS Code:\n\n\n\nA source Markdown file on the left, and its preview on the right.\n\n\n\n\n\n2.1 Most common syntax\nHere is an overview of the most commonly used Markdown syntax:\n\n\n\nSyntax\nResult\n\n\n\n\n*italic*\nitalic (alternative: single _)\n\n\n**bold**\nbold (alternative: double _)\n\n\n[link text](website.com)\nlink text\n\n\n&lt;https://website.com&gt;\nClickable link: https://website.com\n\n\n# My Title\nHeader level 1 (largest)\n\n\n## My Section\nHeader level 2\n\n\n### My Subsection\nHeader level 3 – and so forth\n\n\n- List item\nUnordered (bulleted) list\n\n\n1. List item\nOrdered (numbered) list\n\n\n`inline code`\ninline code\n\n\n```\nStart/end of generic code block (on its own line)\n\n\n```bash\nStart of bash code block (end with ```)\n\n\n---\nHorizontal rule (line)\n\n\n&gt; Text\nBlockquote (like quoted text in emails)\n\n\n![](path/to/figure.png)\n[The figure will be inserted]\n\n\n\n\n\n\nLet’s try some of these things — type:\n# Introduction to Markdown\n\n## Part 1: Documentation\n\n- The Markdown _documentation_ can be found\n  [here](https://www.markdownguide.org/)\n- To be clear,\n  **the URL is &lt;https://www.markdownguide.org/&gt;**.\n\n## Part 2: The basics\n\n1. When you create a _numbered_ list...\n1. ...you don't need the numbers to increment.\n1. Markdown will take care of that for you.\n\n--------\n\n### Part 2b: Take it from the experts\n\n&gt; Markdown will take your science to\n&gt; the next level\n&gt; -- Wilson et al. 1843\n\n--------\n\n## Part 3: My favorite shell commands\n\nThe `date` command is terribly useful.\n\nHere is my shell code in a code block:\n\n```bash\n# Print the current date and time\ndate\n\n# List the files with file sizes\nls -lh\n```\n\n**The end.**\n\nThat should be previewed/rendered as:\n\n\n\n\n\n\n\n\n\n\n2.2 Whitespace\n\nIt’s recommended (in some cases necessary) to leave a blank line between different sections: lists, headers, etc.:\n## Section 2: List of ...\n\n- Item 1\n- Item 2\n\nFor example, ....\n\n\n\nA blank line between regular text will start a new paragraph, with some whitespace between the two:\n\n\n\n\n\nThis:\n\nParagraph 1.\n  \nParagraph 2.\n\nWill be rendered as:\n\nParagraph 1.\nParagraph 2.\n\n\n\n\nWhereas a single newline will be completely ignored!:\n\n\n\n\n\nThis:\n\nParagraph 1.\nParagraph 2.\n\nWill be rendered as:\n\nParagraph 1. Paragraph 2.\n\n\n\n\n\n\n\nThis:\n\nWriting  \none  \nword  \nper  \nline.\n\nWill be rendered as:\n\nWriting one word per line.\n\n\n\n\nMultiple consecutive spaces and blank line will be “collapsed” into a single space/blank line:\n\n\n\n\n\nThis:\n\nEmpty             space\n\nWill be rendered as:\n\nEmpty space\n\n\n\n\n\n\n\nThis:\n\nMany\n\n\n\n\nblank lines\n\nWill be rendered as:\n\nMany\nblank lines\n\n\n\n\nA single linebreak can be forced using two or more spaces (i.e., press the spacebar twice) or a backslash \\ after the last character on a line:\n\n\n\n\n\nThis:\n\nMy first sentence.\\\nMy second sentence.\n\nWill be rendered as:\n\nMy first sentence.\nMy second sentence.\n\n\n\n\nIf you want more vertical whitespace than what is provided between paragraphs, you’ll have to resort to HTML1: each &lt;br&gt; item forces a visible linebreak.\n\n\n\n\n\nThis:\n\nOne &lt;br&gt; word &lt;br&gt; per line\nand &lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;br&gt;\nseveral blank lines.\n\nWill be rendered as:\n\nOne  word  per line and      several blank lines.\n\n\n\n\n\n2.3 Tables\nTables are not all that convenient to create in Markdown, but you can do it as follows.\n\n\n\n\nThis:\n| city             | inhabitants |\n|——————|——————|\n| Columbus   | 906 K       |\n| Cleveland   | 368 K       |\n| Cincinnati   | 308 K       |\n\nWill be rendered as:\n\n\n\ncity            \ninhabitants\n\n\n\n\nColumbus  \n906 K      \n\n\nCleveland  \n368 K      \n\n\nCincinnati  \n308 K      \n\n\n\n\n\n\n\n\n\n\n\n\nSide note: HTML and CSS in Markdown\n\n\n\n\nIf you need “inline colored text”, you can also use HTML:\ninline &lt;span style=\"color:red\"&gt;colored&lt;/span&gt; text.\nFor systematic styling of existing or custom elements, you need to use CSS. For example, including the following anywhere in a Markdown document will turn all level 1 headers (#) red:\n&lt;style&gt;\nh1 {color: red}\n&lt;/style&gt;\n\n\n\n\n\n\n2.4 Markdown extensions – Markdown for everything!\nSeveral Markdown extensions allow Markdown documents to contain code that runs, and whose output can be included in rendered documents:\n\nR Markdown (.Rmd) and the follow-up Quarto — we will learn Quarto later in this course.\nJupyter Notebooks (for Python)\n\nThere are many possibilities with Markdown! For instance, consider that:\n\nThis website and last week’s slides are written using Quarto.\nR Markdown/Quarto also has support for citations, journal-specific formatting, etc., so you can even write manuscripts with it.\n\n\n\n\n\n\n\n\nPandoc to render Markdown files (Click to expand)\n\n\n\n\n\nIn practice, I rarely render “plain” Markdown files because:\n\nMarkdown source is so well readable\nGitHub will render Markdown files for you (as we’ll see next week)\n\nThat said, if you do need to render a Markdown file to, for example, HTML or PDF, use Pandoc:\npandoc README.md &gt; README.html\npandoc -o README.pdf README.md\nTo install Pandoc on your own computer, see https://pandoc.org/installing.html."
  },
  {
    "objectID": "week03/w3_2_vscode-markdown.html#footnotes",
    "href": "week03/w3_2_vscode-markdown.html#footnotes",
    "title": "VS Code and Markdown",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou can use any HTML markup in Markdown!↩︎"
  },
  {
    "objectID": "week03/w3_exercises.html#exercise-1-course-notes-in-markdown",
    "href": "week03/w3_exercises.html#exercise-1-course-notes-in-markdown",
    "title": "Week 3 exercises",
    "section": "Exercise 1: Course notes in Markdown",
    "text": "Exercise 1: Course notes in Markdown\nCreate a Markdown document with course notes. I recommend writing this document in VS Code.\nMake notes of this week’s material in some detail. If you have notes from last week in another format, include those too. (And try to keep using this document throughout the course!)\nSome pointers:\n\nUse several header levels and use them consistently: e.g. a level 1 header (#) for the document’s title, level 2 headers (##) for each week, and so on.\nThough this should foremost be a functional document for notes, try to incorporate any appropriate formatting option: e.g. bold text, italic text, inline code, code blocks, ordered/unordered lists, and hyperlinks.\nMake sure you understand and try out how Markdown deals with whitespace, e.g. starting a new paragraph and how to force a newline."
  },
  {
    "objectID": "week03/w3_exercises.html#exercise-2-organize-project-files",
    "href": "week03/w3_exercises.html#exercise-2-organize-project-files",
    "title": "Week 3 exercises",
    "section": "Exercise 2: Organize project files",
    "text": "Exercise 2: Organize project files\nWhile doing this exercise, save the commands you use in a text document – either write in a text document in VS Code and send the commands to the terminal, or copy them into a text document later.\n\nGetting set up\nCreate a directory for this exercise, and change your working dir to go there. Do this within your personal dir in the course’s project dir (e.g. /fs/ess/PAS2700/users/$USER/week02/ex2/).\nCreate a disorganized mock project\nUsing the touch command and brace expansions, create a mock project by creating 100s of empty files, either in a single directory or a disorganized directory structure.\nIf you want, you can create file types according to what you typically have in your project – otherwise, a suggestion is to create files with:\n\nRaw data (e.g. .fastq.gz)\nReference data (e.g. .fasta),\nMetadata (e.g. .txt or .csv)\nProcessed data and results (e.g. .bam, .out)\nScripts (e.g. .sh, .py or .R)\nFigures (e.g. .png or .eps)\nNotes (.txt and/or .md)\nPerhaps some other file type you usually have in your projects.\n\nOrganize the mock project\nOrganize the mock project according to some of the principles we discussed this week.\nEven while adhering to these principles, there is plenty of wiggle room and no single perfect dir structure: what is optimal will depend on what works for you and on the project size and structure. Therefore, think about what makes sense to you, and what makes sense given the files you find yourself with.\nTry to use as few commands as possible to move the files – use wildcards!\nCreate mock “alignment” files1\n\nCreate a directory alignment inside an appropriate dir in your project (e.g. analysis, results)\nInside the alignment dir, create files with names like sample01_A_08-14-2020.sam - sample50_H_09-16-2020.sam for all combinations of:\n\n30 samples (01-30)\n5 treatments (A-E)\n2 dates (08-14-2020 and 09-16-2020 – yes, use this date format for now)\n\n\nThese 300 files can be created with a single touch command2.\n\n\n\nHints\n\nUse brace expansion three times in the command: to expand (1) sample IDs, (2) treatments, and (3) dates.\nNote that {01..20} will successfully zero-pad single-digit numbers.\n\n\nRename files in a batch\nWoops! We stored the alignment files that we created in the previous step as SAM files (.sam), but this was a mistake – the files are actually the binary counterparts of SAM files: BAM files (.bam).\nMove into the dir with BAM files, and use a for loop to rename them, changing the extension from .sam to .bam.\n\n\n\nHints\n\n\nLoop over the files using globbing (wildcard expansion) directly; there is no need to call ls.\nUse the basename command, or alternatively, cut, to strip the extension.\nStore the output of the basename (or cut) call using command substitution ($(command) syntax).\nThe new extension can simply be pasted behind the file name, like newname=\"$filename_no_extension\"bam or newname=$(basename ...)bam.\n\n\n\nCopy files with wildcards\nStill in the dir with your SAM files, create a new dir called subset. Then, using a single cp command, copy files that satisfy the following conditions into the subset dir:\n\nThe sample ID/number should be 01-19, and\nThe treatment should be A, B, or C.\n\nCreate a README.md in the dir that explains what you did.\n\n\n\nHints\n\nJust like you used multiple consecutive brace expansions above, you can use two consecutive wildcard character sets ([]) here.\n\n\nCreate a README\nInclude a project-wide README.md that described what you did. Again, try to get familiar with Markdown syntax by using formatting liberally.\nBonus: a trickier renaming loop\nYou now realize that your date format is suboptimal (MM-DD-YYYY; which gave 08-14-2020 and 09-16-2020) and that you should use the YYYY-MM-DD format. Use a for loop to rename the files.\n\n\n\nHints\n\n\nUse cut to extract the three elements of the date (day, month, and year) on three separate lines.\nStore the output of these lines in variables using commands substitution, like: day=$(commands).\nFinally, paste your new file name together like: newname=\"$part1\"_\"$year\" etc.\nWhen first writing your commands, it’s helpful to be able to experiment easily: start by echo-ing a single example file name, as in: echo sample23_C_09-16-2020.sam | cut ....\n\n\n\nBonus: Change file permissions\nMake sure no-one has write permissions for the raw data files, not even yourself. You can also change other permissions to what you think is reasonable or necessary precaution for your fictional project.\n\n\n\nHints\n\nUse the chmod command to change file permissions and recall that you can use wildcard expansion to operate on many files at once.\nSee this Bonus section of the Managing files in the shell page for an overview of file permissions and the chmod command.\nAlternatively, chmod also has an -R argument to act recursively: that is, to act on dirs and all of their contents (including other dirs and their contents)."
  },
  {
    "objectID": "week03/w3_exercises.html#bonus-exercises",
    "href": "week03/w3_exercises.html#bonus-exercises",
    "title": "Week 3 exercises",
    "section": "Bonus exercises",
    "text": "Bonus exercises\n\nExercise 3\nIf you feel like it would be good to reorganize one of your own, real projects, you can do so using what you’ve learned this week. Make sure you create a backup copy of the entire project first!\n\n\n\nBuffalo Chapter 3 code-along\nMove back to /fs/ess/PAS1855/users/$USER and download the repository accompanying the Buffalo book using git clone https://github.com/vsbuffalo/bds-files.git. Then, move into the new dir bds-files, and code along with Buffalo Chapter 3."
  },
  {
    "objectID": "week03/w3_exercises.html#solutions",
    "href": "week03/w3_exercises.html#solutions",
    "title": "Week 3 exercises",
    "section": "Solutions",
    "text": "Solutions\n\nExercise 2\n\n\n1. Getting set up\n\n# For example:\nmkdir /fs/ess/PAS2700/users/$USER/week02/ex2\n\ncd /fs/ess/PAS2700/users/$USER/week02/ex2\n\n\n\n2. Create a disorganized mock project\n\nAn example:\ntouch sample{001..150}_{F,R}.fastq.gz\ntouch ref.fasta ref.fai\ntouch sample_info.csv sequence_barcodes.txt\ntouch sample{001..150}{.bam,.bam.bai,_fastqc.zip,_fastqc.html} gene-counts.tsv DE-results.txt GO-out.txt\ntouch fastqc.sh multiqc.sh align.sh sort_bam.sh count1.py count2.py DE.R GO.R KEGG.R\ntouch Fig{01..05}.png all_qc_plots.eps weird-sample.png\ntouch dontforget.txt README.md README_DE.md tmp5.txt\ntouch slurm-84789570.out slurm-84789571.out slurm-84789572.out\n\n\n\n3. Organize the mock project\n\nAn example:\n\nCreate directories:\nmkdir -p data/{fastq,meta,ref}\nmkdir -p results/{bam,counts,DE,enrichment,logfiles,qc/figures}\nmkdir -p scripts\nmkdir -p figures/{ms,sandbox}\nmkdir -p doc/misc\nMove files:\nmv *fastq.gz data/fastq/\nmv ref.fa* data/ref/\nmv sample_info.csv sequence_barcodes.txt data/meta/\nmv *.bam *.bam.bai results/bam/\nmv *fastqc* results/qc/\nmv gene-counts.tsv results/counts/\nmv DE-results.txt results/DE/\nmv GO-out.txt results/enrichment/\nmv *.sh *.R *.py scripts/\nmv README_DE.md results/DE/\nmv Fig[0-9][0-9]* figures/ms\nmv weird-sample.png figures/sandbox\nmv all_qc_plots.eps results/qc/figures/\nmv dontforget.txt tmp5.txt doc/misc/\nmv slurm* results/logfiles/\n\n\n\n\n4. Create mock alignment files\n\nmkdir -p results/alignment\ncd results/alignment \n\n# Create the files:\ntouch sample{01..30}_{A..E}_{08-14-2020,09-16-2020}.sam\n\n# Check if we have 300 files:\nls | wc -l\n300\n\n\n\n5. Rename files in a batch\n\nfor oldname in *.sam; do\n   newname=$(basename \"$oldname\" sam)bam\n   mv -v \"$oldname\" \"$newname\"\ndone\nIn the code above:\n\n$oldname will contain the old file name in each iteration of the loop.\nWe remove the sam suffix using basename \"$oldname\" sam.\nWe use command substitution ($() syntax) to catch the output of the basename command, and paste bam at the end.\n\nAlso, note that:\n\nWe don’t need a special construction to paste strings together: we simply type bam after what will be the extension-less file name from the basename command.\nI used informative variable names (oldname and newname), not cryptic ones like i and o.\n\n\n\n\n6. Copy files with wildcards\n\n\nCreate the new dir:\nmkdir subset\nCopy the files using four consecutive wildcard selections:\n\nThe first digit should be a 0 or a 1 [01] (or [0-1]),\nThe second can be any number [0-9] (? would work, too),\nThe third, after an underscore, should be A, B, or C [A-C],\nWe don’t care about what comes after that, but do need to account for the additional characters, so will use a * to match any character:\n\ncp -v sample[01][0-9]_[A-C]* subset/\n‘sample01_A_08-14-2020.bam’ -&gt; ‘subset/sample01_A_08-14-2020.bam’\n‘sample01_A_09-16-2020.bam’ -&gt; ‘subset/sample01_A_09-16-2020.bam’\n‘sample01_B_08-14-2020.bam’ -&gt; ‘subset/sample01_B_08-14-2020.bam’\n‘sample01_B_09-16-2020.bam’ -&gt; ‘subset/sample01_B_09-16-2020.bam’\n‘sample01_C_08-14-2020.bam’ -&gt; ‘subset/sample01_C_08-14-2020.bam’\n‘sample01_C_09-16-2020.bam’ -&gt; ‘subset/sample01_C_09-16-2020.bam’\n‘sample02_A_08-14-2020.bam’ -&gt; ‘subset/sample02_A_08-14-2020.bam’\n# [...output truncated...]\nReport what we did, including a command substitution to insert the current date:\necho \"On $(date), created a dir 'subset' and copied only files for samples 1-29\nand treatments A-C into this dir.\" &gt; subset/README.md\nCheck the resulting README files:\ncat subset/README.md\nOn Mon Mar 18 10:07:17 EDT 2024, created a dir 'subset' and copied only files for samples 1-29\nand treatments A-C into this dir.\n\n\n\n\n8. Bonus: a trickier renaming loop\n\n\nIn the loop, first use cut to extract the month, day, and year:\n\nStart by extracting the entire date: cut by an _ and take the third item (cut -d \"_\" -f 3).\nThen extract the different components of the date separately for month, date, and year, with cut -d \"-\": the first item is the month, the second is the day, and the third is the year.\nSave these compoinents of the date in variables using command substitution ($()).\n\nSecond, use cut to extract what we may call the “sample prefix”, which contains the sample number and the treatment.\nThird, build the new file name simply by putting the variables in the right order with _ and - delimiters.\nUse mv to rename the files — below, I’ve added -v for verbose so it will report what it does.\n\nfor oldname in *.bam; do\n     # Extract and store the month, day, and year:\n     # (First cut by '_' taking the 3rd item, then by '-')\n     month=$(echo \"$oldname\" | cut -d \"_\" -f 3 | cut -d \"-\" -f 1)\n     day=$(echo \"$oldname\" | cut -d \"_\" -f 3 | cut -d \"-\" -f 2)\n     year=$(basename \"$oldname\" .bam | cut -d \"_\" -f 3 | cut -d \"-\" -f 3)\n     \n     # Extract and store the sample prefix:\n     sample_prefix=$(echo \"$oldname\" | cut -d \"_\" -f 1-2)\n     \n     # Paste together the new name:\n     newname=\"$sample_prefix\"_\"$year\"-\"$month\"-\"$day\".bam\n     \n     # Execute the move:\n     mv -v \"$oldname\" \"$newname\"\ndone\n‘sample01_A_08-14-2020.bam’ -&gt; ‘sample01_A_2020-08-14.bam’\n‘sample01_A_09-16-2020.bam’ -&gt; ‘sample01_A_2020-09-16.bam’\n‘sample01_B_08-14-2020.bam’ -&gt; ‘sample01_B_2020-08-14.bam’\n‘sample01_B_09-16-2020.bam’ -&gt; ‘sample01_B_2020-09-16.bam’\n‘sample01_C_08-14-2020.bam’ -&gt; ‘sample01_C_2020-08-14.bam’\n‘sample01_C_09-16-2020.bam’ -&gt; ‘sample01_C_2020-09-16.bam’\n‘sample01_D_08-14-2020.bam’ -&gt; ‘sample01_D_2020-08-14.bam’\n‘sample01_D_09-16-2020.bam’ -&gt; ‘sample01_D_2020-09-16.bam’\n‘sample01_E_08-14-2020.bam’ -&gt; ‘sample01_E_2020-08-14.bam’\n# [...output truncated...]\n\n\n\n9. Change file permissions\n\nBefore we start, let’s check the current file permissions:\nls -lh data/fastq\nls -lh data/fastq/ | head\ntotal 0\n-rw-rw----+ 1 jelmer PAS0471 0 Mar 18 10:19 sample001_F.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 0 Mar 18 10:19 sample001_R.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 0 Mar 18 10:19 sample002_F.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 0 Mar 18 10:19 sample002_R.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 0 Mar 18 10:19 sample003_F.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 0 Mar 18 10:19 sample003_R.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 0 Mar 18 10:19 sample004_F.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 0 Mar 18 10:19 sample004_R.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 0 Mar 18 10:19 sample005_F.fastq.gz\nThe file “owner”/“user” (you) and the “group” (in this case, PAS0471, likely a different group for you) have read and write permissions, and “others” have no permissions at all.\nThere are several different ways to change permissions with the chmod command. Here are some examples which would ensure that no-one has write permission for the raw data:\n\nSet read(-only) permissions for all:\n# a=r =&gt; all=read\nchmod a=r data/fastq/*\nTake away write permissions for all:\n# a-w =&gt; all minus write\nchmod a-w data/fastq/*\nYou can also use the “numeric” syntax:\nchmod 444 data/fastq/*\n\nWhereas after running the second option, others won’t have read-access, the first and third option should give this result:\nls -lh data/fastq\nls -lh data/fastq/ | head\ntotal 0\n-r--r--r--+ 1 jelmer PAS0471 0 Mar 18 10:19 sample001_F.fastq.gz\n-r--r--r--+ 1 jelmer PAS0471 0 Mar 18 10:19 sample001_R.fastq.gz\n-r--r--r--+ 1 jelmer PAS0471 0 Mar 18 10:19 sample002_F.fastq.gz\n-r--r--r--+ 1 jelmer PAS0471 0 Mar 18 10:19 sample002_R.fastq.gz\n-r--r--r--+ 1 jelmer PAS0471 0 Mar 18 10:19 sample003_F.fastq.gz\n-r--r--r--+ 1 jelmer PAS0471 0 Mar 18 10:19 sample003_R.fastq.gz\n-r--r--r--+ 1 jelmer PAS0471 0 Mar 18 10:19 sample004_F.fastq.gz\n-r--r--r--+ 1 jelmer PAS0471 0 Mar 18 10:19 sample004_R.fastq.gz\n-r--r--r--+ 1 jelmer PAS0471 0 Mar 18 10:19 sample005_F.fastq.gz"
  },
  {
    "objectID": "week03/w3_exercises.html#footnotes",
    "href": "week03/w3_exercises.html#footnotes",
    "title": "Week 3 exercises",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nReal alignment files like SAM/BAM are generated by aligning FASTQ sequence reads to a reference genome.↩︎\n If you already happened to have an alignment dir among your mock project dirs, first delete its contents or rename it.↩︎"
  },
  {
    "objectID": "week02/removed.html",
    "href": "week02/removed.html",
    "title": "Removed",
    "section": "",
    "text": "Because spaces are special characters used to separate commands from options and arguments, etc., using them in file names is inconvenient at best:\n# You should be in /fs/ess/PAS2700/users/$USER/CSB/unix/sandbox\nls\n\ncd Papers and reviews     # NOPE!\n\ncd Papers\\ and\\ reviews   # \\ to escape each individual space\ncd \"Papers and reviews\"   # Quotes to escape special characters\nWe’ll talk more about spaces and file names in week 2.\n\n\n\nUse uniq -c to count occurrences of each unique value (more on this in week 4):\ncut -d \";\" -f 2 Pacifici2013_data.csv | tail -n +2 | sort | uniq -c\n   54 Afrosoricida\n  280 Carnivora\n  325 Cetartiodactyla\n 1144 Chiroptera\n   21 Cingulata\ncut -d \";\" -f 2 Pacifici2013_data.csv | tail -n +2 | sort | uniq -c | sort -nr\n 2220 Rodentia\n 1144 Chiroptera\n  442 Eulipotyphla\n  418 Primates\n\n\n\n\nLet’s say we want a list of animals sorted by body weight…\ncd ../sandbox/\ntail -n +2 ../data/Pacifici2013_data.csv\nIn the following commands, we will build up our “pipeline”.\nFirst, we print the file with the exception of the first line (tail -n +2) and then pipe that into cut to select the columns of interest — and to check our partial pipeline, end with a head command to only print the first lines:\n# Using head just to check the output\ntail -n +2 ../data/Pacifici2013_data.csv |\n    cut -d \";\" -f 5-6 | head\nSecond, we’ll add a tr command to change the column delimiter:\ntail -n +2 ../data/Pacifici2013_data.csv |\n    cut -d \";\" -f 5-6 | tr \";\" \" \" | head\nFinally, we’ll sort in reverse numerical order with sort, and redirect the output to a file:\ntail -n +2 ../data/Pacifici2013_data.csv |\n    cut -d \";\" -f 5-6 | tr \";\" \" \" | sort -r -n -k 3 &gt; BodyM.csv\nLet’s take a look at the output:\nhead BodyM.csv"
  },
  {
    "objectID": "week02/removed.html#bonus-material",
    "href": "week02/removed.html#bonus-material",
    "title": "Removed",
    "section": "",
    "text": "Because spaces are special characters used to separate commands from options and arguments, etc., using them in file names is inconvenient at best:\n# You should be in /fs/ess/PAS2700/users/$USER/CSB/unix/sandbox\nls\n\ncd Papers and reviews     # NOPE!\n\ncd Papers\\ and\\ reviews   # \\ to escape each individual space\ncd \"Papers and reviews\"   # Quotes to escape special characters\nWe’ll talk more about spaces and file names in week 2.\n\n\n\nUse uniq -c to count occurrences of each unique value (more on this in week 4):\ncut -d \";\" -f 2 Pacifici2013_data.csv | tail -n +2 | sort | uniq -c\n   54 Afrosoricida\n  280 Carnivora\n  325 Cetartiodactyla\n 1144 Chiroptera\n   21 Cingulata\ncut -d \";\" -f 2 Pacifici2013_data.csv | tail -n +2 | sort | uniq -c | sort -nr\n 2220 Rodentia\n 1144 Chiroptera\n  442 Eulipotyphla\n  418 Primates\n\n\n\n\nLet’s say we want a list of animals sorted by body weight…\ncd ../sandbox/\ntail -n +2 ../data/Pacifici2013_data.csv\nIn the following commands, we will build up our “pipeline”.\nFirst, we print the file with the exception of the first line (tail -n +2) and then pipe that into cut to select the columns of interest — and to check our partial pipeline, end with a head command to only print the first lines:\n# Using head just to check the output\ntail -n +2 ../data/Pacifici2013_data.csv |\n    cut -d \";\" -f 5-6 | head\nSecond, we’ll add a tr command to change the column delimiter:\ntail -n +2 ../data/Pacifici2013_data.csv |\n    cut -d \";\" -f 5-6 | tr \";\" \" \" | head\nFinally, we’ll sort in reverse numerical order with sort, and redirect the output to a file:\ntail -n +2 ../data/Pacifici2013_data.csv |\n    cut -d \";\" -f 5-6 | tr \";\" \" \" | sort -r -n -k 3 &gt; BodyM.csv\nLet’s take a look at the output:\nhead BodyM.csv"
  },
  {
    "objectID": "week02/w2_ga_shell.html",
    "href": "week02/w2_ga_shell.html",
    "title": "Graded Assignment I: Shell basics",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "week02/w2_overview.html#content-overview",
    "href": "week02/w2_overview.html#content-overview",
    "title": "Week 2: Unix shell basics",
    "section": "1 Content overview",
    "text": "1 Content overview\nThis week, we will focus on the basics of the Unix shell, and you will learn:\n\nUnix shell basics (Tuesday)\n\nWhat the Unix shell is and what you can do with it\nWhy it can be beneficial to use a command-line interface in general and the shell specifically\nWhat the Unix file structure looks like and what paths are\nHow Unix commands are stuctured\nHow to navigate around your computer with Unix commands\n\n\n\n\nUnix shell: working with files (Thursday)\nUsing Unix commands, how to:\n\nCreate, copy, move, rename and delete directories and files\nViewing the contents of text files in various ways\nSearch within, manipulate, and extract information from text files"
  },
  {
    "objectID": "week02/w2_overview.html#assignments-and-exercises",
    "href": "week02/w2_overview.html#assignments-and-exercises",
    "title": "Week 2: Unix shell basics",
    "section": "2 Assignments and exercises",
    "text": "2 Assignments and exercises\n\nExercises: Shell basics\nGraded assignment: Shell basics (due Mon 09/08)"
  },
  {
    "objectID": "week02/w2_overview.html#readings",
    "href": "week02/w2_overview.html#readings",
    "title": "Week 2: Unix shell basics",
    "section": "3 Readings",
    "text": "3 Readings\n\nOptional readings\n\nCSB Chapter 1: Unix"
  },
  {
    "objectID": "week05/w5_3_bonus.html#while-loops",
    "href": "week05/w5_3_bonus.html#while-loops",
    "title": "Advanced Unix Shell: while loops, arrays and more",
    "section": "1 While loops",
    "text": "1 While loops\nIn bash, while loops are mostly useful in combination with the read command, to loop over each line in a file. If you use while loops, you’ll very rarely need Bash arrays (next section), and conversely, if you like to use arrays, you may not need while loops much.\nwhile loops will run as long as a condition is true and this condition can include constructs such as read -r which will read input line-by-line, and be true as long as there is a line left to be read from the file. In the example below, while read -r will be true as long as lines are being read from a file fastq_files.txt — and in each iteration of the loop, the variable $fastq_file contains one line from the file:\n# [ Don't run this - hypothetical example]\ncat fastq_files.txt\nseq/zmaysA_R1.fastq\nseq/zmaysA_R2.fastq\nseq/zmaysB_R1.fastq\n# [ Don't run this - hypothetical example]\ncat fastq_files.txt | while read -r fastq_file; do\n    echo \"Processing file: $fastq_file\"\n    # More processing...\ndone\nProcessing file: seq/zmaysA_R1.fastq\nProcessing file: seq/zmaysA_R2.fastq\nProcessing file: seq/zmaysB_R1.fastq\nA more elegant but perhaps confusing syntax variant used input redirection instead of cat-ing the file:\n# [ Don't run this - hypothetical example]\nwhile read -r fastq_file; do\n    echo \"Processing file: $fastq_file\"\n    # More processing...\ndone &lt; fastq_files.txt\nProcessing file: seq/zmaysA_R1.fastq\nProcessing file: seq/zmaysA_R2.fastq\nProcessing file: seq/zmaysB_R1.fastq\nWe can also process each line of the file inside the while loop, like when we need to select a specific column:\n# [ Don't run this - hypothetical example]\nhead -n 2 samples.txt\nzmaysA  R1      seq/zmaysA_R1.fastq\nzmaysA  R2      seq/zmaysA_R2.fastq\n# [ Don't run this - hypothetical example]\nwhile read -r my_line; do\n    echo \"Have read line: $my_line\"\n    fastq_file=$(echo \"$my_line\" | cut -f 3)\n    echo \"Processing file: $fastq_file\"\n    # More processing...\ndone &lt; samples.txt\nHave read line: zmaysA  R1      seq/zmaysA_R1.fastq\nProcessing file: seq/zmaysA_R1.fastq\nHave read line: zmaysA  R2      seq/zmaysA_R2.fastq\nProcessing file: seq/zmaysA_R2.fastq\nAlternatively, you can operate on file contents before inputting it into the loop:\n# [ Don't run this - hypothetical example]\nwhile read -r fastq_file; do\n    echo \"Processing file: $fastq_file\"\n    # More processing...\ndone &lt; &lt;(cut -f 3 samples.txt)\nFinally, you can extract columns directly as follows:\n# [ Don't run this - hypothetical example]\nwhile read -r sample_name readpair_member fastq_file; do\n    echo \"Processing file: $fastq_file\"\n    # More processing...\ndone &lt; samples.txt\nProcessing file: seq/zmaysA_R1.fastq\nProcessing file: seq/zmaysA_R2.fastq\nProcessing file: seq/zmaysB_R1.fastq"
  },
  {
    "objectID": "week05/w5_3_bonus.html#arrays",
    "href": "week05/w5_3_bonus.html#arrays",
    "title": "Advanced Unix Shell: while loops, arrays and more",
    "section": "2 Arrays",
    "text": "2 Arrays\nBash “arrays” are basically lists of items, such as a list of file names or samples IDs. If you’re familiar with R, they are like R vectors1.\nArrays are mainly used with for loops: you create an array and then loop over the individual items in the array. This usage represents an alternative to looping over files with a glob. Looping over files with a glob is generally easier and preferable, but sometimes this is not the case; or you are looping e.g. over samples and not files.\n\nCreating arrays\nYou can create an array “manually” by typing a space-delimited list of items between parentheses:\n# The array will contain 3 items: 'zmaysA', 'zmaysB', and 'zmaysC'\nsample_names=(zmaysA zmaysB zmaysC)\nMore commonly, you would populate an array from a file, in which case you also need command substitution:\n\nSimply reading in an array from a file with cat will only work if the file simply contains a list of items:\nsample_files=($(cat fastq_files.txt))\nFor tabular files, you can include e.g. a cut command to extract the focal column:\nsample_files=($(cut -f 3 samples.txt))\n\n\n\n\n\n\n\n\nAlternatively, use the mapfile command\n\n\n\nTODO\n\n\n\n\n\nAccessing arrays\nFirst off, it is useful to realize that arrays are closely related to regular variables, and to recall that the “full” notation to refer to a variable includes curly braces: ${myvar}. When referencing arrays, the curly braces are always needed.\n\nUsing [@], we can access all elements in the array (and arrays are best quoted, like regular variables):\necho \"${sample_names[@]}\"\nzmaysA zmaysB zmaysC\nWe can also use the [@] notation to loop over the elements in an array:\nfor sample_name in \"${sample_names[@]}\"; do\n    echo \"Processing sample: $sample_name\"\ndone\nProcessing sample: zmaysA\nProcessing sample: zmaysB\nProcessing sample: zmaysC\n\n\n\n\n\n\n\n\nOther array operations (Click to expand)\n\n\n\n\n\n\nExtract specific elements (note: Bash arrays are 0-indexed!):\n# Extract the first item\necho ${sample_names[0]}\nzmaysA\n# Extract the third item\necho ${sample_names[2]}\nzmaysC\nCount the number of elements in the array:\necho ${#sample_names[@]}\n3\n\n\n\n\n\n\n\nArrays and filenames with spaces\nThe file files.txt contains a short list of file names, the last of which has a space in it:\ncat files.txt\nfile_A\nfile_B\nfile_C\nfile D\nWhat will happen if we read this list into an array, and then loop over the array?\n# Populate an array with the list of files from 'files.txt'\nall_files=($(cat files.txt))\n\n# Loop over the array:\nfor file in \"${all_files[@]}\"; do\n    echo \"Current file: $file\"\ndone\nCurrent file: file_A\nCurrent file: file_B\nCurrent file: file_C\nCurrent file: file\nCurrent file: D\nUh-oh! The file name with the space in it was split into two items! And note that we did quote the array in \"${all_files[@]}\", so clearly, this doesn’t solve that problem.\nFor this reason, it’s best not to use arrays to loop over filenames with spaces (though there are workarounds). Direct globbing and while loops with the read function (while read ..., see below) are easier choices for problematic file names.\nAlso, this example once again demonstrates you should not have spaces in your file names!\n\n\n\n Exercise: Bash arrays\n\nCreate an array with the first three file names (lines) listed in samples.txt.\nLoop over the contents of the array with a for loop.\nInside the loop, create (touch) the file listed in the current array element.\nCheck whether you created your files.\n\n\n\nSolutions\n\n\nCreate an array with the first three file names (lines) listed in samples.txt.\n\ngood_files=($(head -n 3 files.txt))\n\nLoop over the contents of the array with a for loop.\nInside the loop, create (touch) the file listed in the current array element.\nfor good_file in \"${good_files[@]}\"; do\n    touch \"$good_file\"\ndone\nCheck whether you created your files.\nls\nfile_A  file_B  file_C"
  },
  {
    "objectID": "week05/w5_3_bonus.html#miscellaneous",
    "href": "week05/w5_3_bonus.html#miscellaneous",
    "title": "Advanced Unix Shell: while loops, arrays and more",
    "section": "3 Miscellaneous",
    "text": "3 Miscellaneous\n\n3.1 More on the && and || operators\nAbove, we saw that we can combine tests in if statements with && and ||. But these shell operators can be used to chain commands together in a more general way, as shown below.\n\nOnly if the first command succeeds, also run the second:\n# Move into the data dir and if that succeeds, then list the files there:\ncd data && ls data\n# Stage all changes =&gt; commit them =&gt; push the commit to remote:\ngit add --all && git commit -m \"Add README\" && git push\nOnly if the first command fails, also run the second:\n# Exit the script if you can't change into the output dir:\ncd \"$outdir\" || exit 1\n# Only create the directory if it doesn't already exist:\n[[ -d \"$outdir\" ]] || mkdir \"$outdir\"\n\n\n\n\n3.2 Parameter expansion to provide default values\nIn scripts, it may be useful to have optional arguments that have a default value if they are not specified on the command line. You can use the following “parameter expansion” syntax for this.\n\nAssign the value of $1 to number_of_lines unless $1 doesn’t exist: in that case, set it to a default value of 10:\nnumber_of_lines=${1:-10}\nSet true as the default value for $3:\nremove_unpaired=${3:-true}\n\nAs a more worked out example, say that your script takes an input dir and an output dir as arguments. But if the output dir is not specified, you want it to be the same as the input dir. You can do that like so:\ninput_dir=$1\noutput_dir=${2:-$input_dir}\nNow you can call the script with or without the second argument, the output dir:\n# Call the script with 2 args: input and output dir\nsort_bam.sh results/bam results/bam\n# Call the script with 1 arg: input dir (which will then also be the output dir)\nsort_bam.sh results/bam\n\n\n\n3.3 Standard output and standard error\nAs you’ve seen, when commands run into errors, they will print error messages. Error messages are not part of “standard out”, but represent a separate output stream: “standard error”.\nWe can see this when we try to list a non-existing directory and try to redirect the output of the ls command to a file:\nls -lhr solutions/ &gt; solution_files.txt \nls: cannot access solutions.txt: No such file or directory\nEvidently, the error was printed to screen rather than redirected to the output file. This is because &gt; only redirects standard out, and not standard error. Was anything at all printed to the file?\ncat solution_files.txt\n# We just get our prompt back - the file is empty\nNo, because there were no files to list, only an error to report.\nThe figure below draws the in- and output streams without redirection (a) versus with &gt; redirection (b):\n\n\n\nFigure from Buffalo.\n\n\nTo redirect the standard error, use 2&gt; 2:\nls -lhr solutions/ &gt; solution_files.txt 2&gt; errors.txt\nTo combine standard out and standard error, use &&gt;:\n# (&&gt; is a bash shortcut for 2&gt;&1)\nls -lhr solutions/ &&gt; out.txt\ncat out.txt\nls: cannot access solutions.txt: No such file or directory\nFinally, if you want to “manually” designate an echo statement to represent standard error instead of standard out in a script, use &gt;&2:\necho \"Error: Invalid line number\" &gt;&2\necho \"Number should be &gt;0 and &lt;= the file's nr. of lines\" &gt;&2\necho \"File contains $(wc -l &lt; $2) lines; you provided $1.\" &gt;&2\nexit 1"
  },
  {
    "objectID": "week05/w5_3_bonus.html#footnotes",
    "href": "week05/w5_3_bonus.html#footnotes",
    "title": "Advanced Unix Shell: while loops, arrays and more",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n Or if you’re familiar with Python, they are like Python lists.↩︎\n Note that 1&gt; is the full notation to redirect standard out, and the &gt; we’ve been using is merely a shortcut for that.↩︎"
  },
  {
    "objectID": "week05/w5_2_cli-tools.html#overview-and-setting-up",
    "href": "week05/w5_2_cli-tools.html#overview-and-setting-up",
    "title": "Running command-line tools with shell scripts",
    "section": "Overview and setting up",
    "text": "Overview and setting up\nThis session focuses on using programs, like various bioinformatics tools, with command-line interfaces (CLIs), and on running these inside shell scripts.\n\nThe strategy that you’ll learn is to write scripts that run a single program a single time, even if you need to run the program many times: in that case, you’ll loop over files outside of that script.\nThis means you’ll also need a higher-level “runner” script in which you save the looping code and more. We will talk about why this strategy makes sense, especially when you have a supercomputer at your disposal.\n\nWe’ll start with a quick intro to FASTQ files and the FastQC program, which will be our first example of a CLI tool.\n\nOur practice data set\nOur practice data set is from the paper “Two avian Plasmodium species trigger different transcriptional responses on their vector Culex pipiens”, published last year in Molecular Ecology (link):\n\n\n\n\n\nThis paper uses RNA-seq data to study gene expression in Culex pipiens mosquitos infected with malaria-causing Plasmodium protozoans — specifically, it compares mosquitos according to:\n\nInfection status: Plasmodium cathemerium vs. P. relictum vs. control\nTime after infection: 24 h vs. 10 days vs. 21 days\n\nHowever, to keep things manageable for our practice, I have subset the data to omit the 21-day samples and only keep 500,000 reads per FASTQ file. All in all, our set of files consists of:\n\n44 paired-end Illumina FASTQ files for 22 samples.\nCulex pipiens reference genome file from NCBI: assembly in FASTA format and annotation in GTF format.\nA metadata file in TSV format with sample IDs and treatment & time point info.\nA README file describing the data set.\n\n\n\nGet your own copy of the data\n# (Assuming you are in /fs/ess/PAS2700/users/$USER)\ncd week04\n\ncp -rv /fs/ess/PAS2700/share/garrigos_data .\n‘/fs/ess/PAS2700/share/garrigos_data’ -&gt; ‘./garrigos_data’\n‘/fs/ess/PAS2700/share/garrigos_data/meta’ -&gt; ‘./garrigos_data/meta’\n‘/fs/ess/PAS2700/share/garrigos_data/meta/metadata.tsv’ -&gt; ‘./garrigos_data/meta/metadata.tsv’\n‘/fs/ess/PAS2700/share/garrigos_data/ref’ -&gt; ‘./garrigos_data/ref’\n‘/fs/ess/PAS2700/share/garrigos_data/ref/GCF_016801865.2.gtf’ -&gt; ‘./garrigos_data/ref/GCF_016801865.2.gtf’\n‘/fs/ess/PAS2700/share/garrigos_data/ref/GCF_016801865.2.fna’ -&gt; ‘./garrigos_data/ref/GCF_016801865.2.fna’\n‘/fs/ess/PAS2700/share/garrigos_data/fastq’ -&gt; ‘./garrigos_data/fastq’\n‘/fs/ess/PAS2700/share/garrigos_data/fastq/ERR10802868_R2.fastq.gz’ -&gt; ‘./garrigos_data/fastq/ERR10802868_R2.fastq.gz’\n‘/fs/ess/PAS2700/share/garrigos_data/fastq/ERR10802863_R1.fastq.gz’ -&gt; ‘./garrigos_data/fastq/ERR10802863_R1.fastq.gz’\n‘/fs/ess/PAS2700/share/garrigos_data/fastq/ERR10802880_R2.fastq.gz’ -&gt; ‘./garrigos_data/fastq/ERR10802880_R2.fastq.gz’\n‘/fs/ess/PAS2700/share/garrigos_data/fastq/ERR10802880_R1.fastq.gz’ -&gt; ‘./garrigos_data/fastq/ERR10802880_R1.fastq.gz’\n‘/fs/ess/PAS2700/share/garrigos_data/fastq/ERR10802870_R1.fastq.gz’ -&gt; ‘./garrigos_data/fastq/ERR10802870_R1.fastq.gz’\n# [...output truncated...]\nTake a look at the files you just copied with the tree command, a sort of recursive ls with simple tree-like output:\n# -C will add colors (not shown in the output below)\ntree -C garrigos_data\ngarrigos_data\n├── fastq\n│   ├── ERR10802863_R1.fastq.gz\n│   ├── ERR10802863_R2.fastq.gz\n│   ├── ERR10802864_R1.fastq.gz\n│   ├── ERR10802864_R2.fastq.gz\n│   ├── [...other FASTQ files not shown...]\n├── meta\n│   └── metadata.tsv\n├── README.md\n└── ref\n    ├── GCF_016801865.2.fna\n    └── GCF_016801865.2.gtf\n\n3 directories, 48 files"
  },
  {
    "objectID": "week05/w5_2_cli-tools.html#fastq-files-and-fastqc",
    "href": "week05/w5_2_cli-tools.html#fastq-files-and-fastqc",
    "title": "Running command-line tools with shell scripts",
    "section": "1 FASTQ files and FastQC",
    "text": "1 FASTQ files and FastQC\n\n1.1 The FASTQ format\nFASTQ is a very common output format of high-throughput sequencing machines. Like most genomic data files, these are plain text files. Each sequence that is read by the sequencer (i.e., each “read”) forms one FASTQ entry represented by four lines. The lines contain, respectively:\n\nA header that starts with @ and e.g. uniquely identifies the read\nThe nucleotide sequence itself\nA + (plus sign)\nOne-character quality scores for each base in the sequence\n\n\n\n\nOne entry (read) in a FASTQ file covers 4 lines. The header line is annotated, with some of the more useful components highlighted in red. For viewing purposes, this read (at only 56 bp) is shorter than what is typical.\n\n\nThe “Q” in FASTQ stands for “quality”, to contrast this format with FASTA, a more basic and generic sequence data format that does not include base quality scores. FASTQ files have the extension .fastq or .fq, but they are very commonly gzip-compressed, in which case their name ends in .fastq.gz or .fq.gz.\n\n\n\n\n\n\nFASTQ quality scores (Click to expand)\n\n\n\n\n\nThe quality scores we saw in the read above represent an estimate of the error probability of the base call. Specifically, they correspond to a numeric “Phred” quality score (Q), which is a function of the estimated probability that a base call is erroneous (P):\n\nQ = -10 * log10(P)\n\nFor some specific probabilities and their rough qualitative interpretations for Illumina data:\n\n\n\n\n\n\n\n\n\nPhred quality score\nError probability\nRough interpretation\nASCII character\n\n\n\n\n10\n1 in 10\nterrible\n+\n\n\n20\n1 in 100\nbad\n5\n\n\n30\n1 in 1,000\ngood\n?\n\n\n40\n1 in 10,000\nexcellent\n?\n\n\n\nThis numeric quality score is represented in FASTQ files not by the number itself, but by a corresponding “ASCII character” (last column in the table). This allows for a single-character representation of each possible score — as a consequence, each quality score character can conveniently correspond to (& line up with) a base character in the read. (For your reference, here is a complete lookup table — look at the top table, “BASE=33”).\n\n\n\n\n\n\n1.2 Our FASTQ files\nTake a look at a file listing of your FASTQ files:\nls -lh garrigos_data/fastq\ntotal 941M\n-rw-r-----+ 1 jelmer PAS0471 21M Mar 21 09:38 ERR10802863_R1.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 22M Mar 21 09:38 ERR10802863_R2.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 21M Mar 21 09:38 ERR10802864_R1.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 22M Mar 21 09:38 ERR10802864_R2.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 22M Mar 21 09:38 ERR10802865_R1.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 22M Mar 21 09:38 ERR10802865_R2.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 21M Mar 21 09:38 ERR10802866_R1.fastq.gz\n# [...output truncated...]\nNote that:\n\nThere are two files per sample: _R1 (forward reads) and _R2 (reverse reads).\nThe files all have a .gz extension, indicating they have been compressed with the gzip utility.\nThe files are ~21-22 Mb in size — considerably smaller than the original file sizes (around 1-2 Gb, which is typical) because they were subsampled.\n\n\n\n\n1.3 Viewing the contents of FASTQ files\nNext, try to take a peak inside one of these FASTQ files. Use -n 8 with head to print the first 8 lines (2 reads):\nhead -n 8 garrigos_data/fastq/ERR10802863_R1.fastq.gz\n�\nԽے�8�E��_1f�\"�QD�J��D�fs{����Yk����d��*��\n|��x���l޴�j�N������?������ٔ�bUs�Ng�Ǭ���i;_��������������|&lt;�v����3��������|���ۧ��3ĐHyƕ�bIΟD�%����Sr#~��7��ν��1y�Ai,4\nw\\]\"b�#Q����8��+[e�3d�4H���̒�l�9LVMX��U*�M����_?���\\[\"��7�s\\&lt;_���:�$���N��v�}^����sw�|�n;&lt;�&lt;�oP����\ni��k��q�ְ(G�ϫ��L�^��=��&lt;���K��j�_/�[ۭV�ns:��U��G�z�ݎ�j����&��~�F��٤ZN�'��r2z}�f\\#��:�9$�����H�݂�\"�@M����H�C�\n�0�pp���1�O��I�H�P됄�.Ȣe��Q�&gt;���\n�'�;@D8���#��St�7k�g��|�A䉻���_���d�_c������a\\�|�_�mn�]�9N������l�٢ZN�c�9u�����n��n�`��\n\"gͺ�\n    ���H�?2@�FC�S$n���Ԓh�       nԙj��望��f      �?N@�CzUlT�&�h�Pt!�r|��9~)���e�A�77�h{��~��     ��\n# [...output truncated...]\n\n\nOuch! 😳 What went wrong here? (Click for the solution)\n\nWe were directly presented with the contents of the compressed file, which isn’t human-readable.\n\n\n\n\n\n\n\n\nNo need to decompress\n\n\n\nTo get around the problem we just encountered with head, we might be inclined to uncompress these files, which we could do with the gunzip command. However, uncompressed files take up several times as much disk storage space as compressed ones. Fortunately, we don’t need to decompress them:\n\nAlmost any bioinformatics tool will accept compressed FASTQ files.\nWe can still view these files in compressed form, as shown below.\n\n\n\nInstead, we’ll use the less command, which automatically displays gzip-compressed files in human-readable form:\nless -S garrigos_data/fastq/ERR10802863_R1.fastq.gz\n@ERR10802863.8435456 8435456 length=74\nCAACGAATACATCATGTTTGCGAAACTACTCCTCCTCGCCTTGGTGGGGATCAGTACTGCGTACCAGTATGAGT\n+\nAAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n@ERR10802863.27637245 27637245 length=74\nGCCACACTTTTGAAGAACAGCGTCATTGTTCTTAATTTTGTCGGCAACGCCTGCACGAGCCTTCCACGTAAGTT\n+\nAAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEE&lt;EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n@ERR10802863.10009244 10009244 length=73\nCTCGGCGTTAACTTCATCACGCAGATCATTCCGTTCCAGCAGCTGAAGCAAGACTACCGTCAGTACGAGATGA\n+\nAAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n@ERR10802863.6604176 6604176 length=74\nAACTACAAATCTTCCTGTGCCGTTTCCAGCAAGTACGTCGATACCTTCGATGGACGCAACTACGAGTACAACAT\n+\nAAAAAEAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n@ERR10802863.11918285 11918285 length=35\nNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN\n+\n###################################\n\n\n\n\n\n\nThe -S option to less suppresses line-wrapping: lines in the file will not be “wrapped” across multiple lines.\n\n\n\n\n\n\n\n\n Exercise: Explore the file with less\nAfter running the command above, you should be viewing the file inside the less pager. Do you notice anything that strikes you as potentially unusual, some reads that look different from the others?\nRecall that you can move around in the file in several ways: by scrolling with your mouse, with up and down arrows, or, if you have them, PgUp and PgDn keys (also, u will move up half a page and d down half a page). And that you won’t get your shell prompt back until you press q to quit less.\n\n\nClick for the solution\n\nThere are a number of reads that are much shorter than the others and only consist of N, i.e. uncalled bases. For example:\n@ERR10802863.11918285 11918285 length=35\nNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN\n+\n###################################\n\n\n\n\n\n1.4 FastQC\nFastQC is a ubiquitous tool for quality control of FASTQ files. Running FastQC or a similar program is the first step in nearly any high-throughput sequencing project. FastQC is also a good introductory example of a tool with a command-line interface.\nFor each FASTQ file, FastQC outputs an HTML file that you can open in your browser with about a dozen graphs showing different QC metrics. The most important one is the per-base quality score graph:\n\n\n\nA FastQC per-base quality score graph for files with reasonably good quality reads. The y-axis shows Phred quality scores (higher is better, see also the color-coding) and the x-axis shows the position along the read."
  },
  {
    "objectID": "week05/w5_2_cli-tools.html#running-fastqc-interactively",
    "href": "week05/w5_2_cli-tools.html#running-fastqc-interactively",
    "title": "Running command-line tools with shell scripts",
    "section": "2 Running FastQC interactively",
    "text": "2 Running FastQC interactively\nThe command fastqc will run the FastQC program. If you want to analyze a FASTQ file with default FastQC settings, a complete FastQC command would simply be fastqc followed by the file name:\n# (Don't run this)\nfastqc garrigos_data/fastq/ERR10802863_R1.fastq.gz\nHowever, an annoying FastQC default behavior is that it writes its output files in the same dir that contains the input FASTQ files — this means mixing your raw data with your results, which we don’t want!\nTo figure out how we can change that behavior, first consider that many commands and bioinformatics tools alike have an option -h and/or --help to print usage information to the screen. Let’s try that:\nfastqc -h\nbash: fastqc: command not found...\nHowever, there is a wrinkle — while FastQC is installed at OSC1, we have to first “load it”2:\nmodule load fastqc/0.11.8\n\n\n Exercise: FastQC help and output dir\nPrint FastQC’s help info, and figure out which option you can use to specify a custom output directory.\n\n\nClick for the solution\n\nRunning fastqc -h or fastqc --help will work to show the help info. You’ll get quite a bit of output printed to screen, including the snippet about output directories that is reproduced below:\nfastqc -h\n  -o --outdir     Create all output files in the specified output directory.\n                    Please note that this directory must exist as the program\n                    will not create it.  If this option is not set then the \n                    output file for each sequence file is created in the same\n                    directory as the sequence file which was processed.\nSo, you can use -o or equivalently, --outdir to specify an output dir.\n\n\n\nWith the added --outdir (or -o) option, let’s try to run the following FastQC command:\n# We'll have to first create the outdir ourselves, in this case\nmkdir -p results/fastqc\n\n# Now we run FastQC\nfastqc --outdir results/fastqc garrigos_data/fastq/ERR10802863_R1.fastq.gz\nStarted analysis of ERR10802863_R1.fastq.gz\nApprox 5% complete for ERR10802863_R1.fastq.gz\nApprox 10% complete for ERR10802863_R1.fastq.gz\nApprox 15% complete for ERR10802863_R1.fastq.gz\n[...truncated...]\nAnalysis complete for ERR10802863_R1.fastq.gz\nIn the output dir we specified, we have a .zip file, which contains tables with FastQC’s data summaries, and an .html (HTML) file, which contains the graphs:\nls -lh results/fastqc\ntotal 512K\n-rw-rw----+ 1 jelmer PAS0471 241K Mar 21 09:53 ERR10802863_R1_fastqc.html\n-rw-rw----+ 1 jelmer PAS0471 256K Mar 21 09:53 ERR10802863_R1_fastqc.zip\n\n\n\n\n\n\nSpecifying an output dir vs. output file(s)\n\n\n\nFastQC allows us to specify the output directory, but not the output file names: these will be automatically determined based on the input file name(s). This kind of behavior is fairly common for bioinformatics programs, since they will often produce multiple output files.\n\n\n\n\n Exercise: Another FastQC run\nRun FastQC for the corresponding R2 FASTQ file. Would you use the same output dir or a separate one?\n\n\nClick for the solution\n\nYes, it makes sense to use the same output dir, since as you could see above, the output file names have the input file identifiers in them. As such, we don’t need to worry about overwriting files, and it will be more convenient to have all results in a single dir.\nTo run FastQC for the R2 (reverse-read) file:\nfastqc --outdir results/fastqc garrigos_data/fastq/ERR10802863_R2.fastq.gz\nStarted analysis of ERR10802863_R2.fastq.gz\nApprox 5% complete for ERR10802863_R2.fastq.gz\nApprox 10% complete for ERR10802863_R2.fastq.gz\nApprox 15% complete for ERR10802863_R2.fastq.gz\n[...truncated...]\nAnalysis complete for ERR10802863_R2.fastq.gz\nls -lh results/fastqc\ntotal 1008K\n-rw-rw----+ 1 jelmer PAS0471 241K Mar 21 09:53 ERR10802863_R1_fastqc.html\n-rw-rw----+ 1 jelmer PAS0471 256K Mar 21 09:53 ERR10802863_R1_fastqc.zip\n-rw-rw----+ 1 jelmer PAS0471 234K Mar 21 09:55 ERR10802863_R2_fastqc.html\n-rw-rw----+ 1 jelmer PAS0471 244K Mar 21 09:55 ERR10802863_R2_fastqc.zip\nNow, we have four files: two for each of our preceding successful FastQC runs."
  },
  {
    "objectID": "week05/w5_2_cli-tools.html#running-fastqc-with-a-shell-script",
    "href": "week05/w5_2_cli-tools.html#running-fastqc-with-a-shell-script",
    "title": "Running command-line tools with shell scripts",
    "section": "3 Running FastQC with a shell script",
    "text": "3 Running FastQC with a shell script\nInstead of running FastQC interactively, we’ll want to write a script that runs it. Specifically, our script will deliberately run FastQC on only one FASTQ file.\nIn bioinformatics, you commonly need to run a CLI tool many times, because most tools can or have to be run separately run for each file or sample. Instead of writing a script that runs one file or sample, a perhaps more intuitive approach would be writing a script that processes all files/samples in a single run. That can be accomplished by:\n\nLooping over files/samples inside the script; or\nPassing many file names or a glob with * to a single run of the tool (this can be done with some tools).\n\nHowever, given that we have access to OSC’s clusters, it will save running time -potentially a lot of it- when we submit a separate batch job for each FASTQ file. This is why we will write a script such that runs only one file, and then we’ll run that script many times using a loop outside of te script.\nFor now, we’ll practice with writing scripts this way, and running them interactively. Next week, we will take the next step and submit each run of the script as a batch job.\n\n\n3.1 Arguments to the script\nOur favored approach of running the script for one FASTQ file at a time means that our script needs to accept a FASTQ file name as an argument. So instead of using a line like the one we ran above…\n# [Don't copy or run this]\nfastqc --outdir results/fastqc garrigos_data/fastq/ERR10802863_R2.fastq.gz\n…we would use a variable for the file name — for example:\n# [Don't copy or run this]\nfastqc --outdir results/fastqc \"$fastq_file\"\nAnd while we’re at it, we may also want to use a variable for the output dir:\n# [Don't copy or run this]\nfastqc --outdir \"$outdir\" \"$fastq_file\"\nOf course, these variables don’t appear out of thin air completely — we need to pass arguments to the script, and copy the placeholder variables inside the script:\n# [Don't copy or run this]\n# Copy the placeholder variables\nfastq_file=$1\noutdir=$2\n\n# Run FastQC\nfastqc --outdir \"$outdir\" \"$fastq_file\"\n\n\n\n\n\n\n\nRunning the script\n\n\n\nAnd such a script would be run for a single file as follows:\n# [Don't copy or run this]\n# Syntax: 'bash &lt;script-path&gt; &lt;argument1&gt; &lt;argument2&gt;'\nbash scripts/fastqc.sh garrigos_data/fastq/ERR10802863_R2.fastq.gz results/fastqc\nAnd it would be run for all files by looping over all them as follows:\n# [Don't copy or run this]\n# Run the script separately for each FASTQ file\nfor fastq_file in data/fastq/*fastq.gz; do\n    bash scripts/fastqc.sh \"$fastq_file\" results/fastqc\ndone\n\n\n\n\n\n3.2 Creating an initial script\nWe saw some code to run FastQC inside a script, to which we should add a number of “boilerplate” bits of code:\n\nThe shebang line and strict Bash settings:\n#!/bin/bash\nset -euo pipefail\nA line to load the relevant OSC software module:\nmodule load fastqc/0.11.8\nA line to create the output directory if it doesn’t yet exist:\nmkdir -p \"$outdir\"\n\n\n\n\n\n\n\nRefresher: the -p option to mkdir (Click to expand)\n\n\n\n\n\nUsing the -p option does two things at once, and both are necessary for a foolproof inclusion of this command in a script:\n\nIt will enable mkdir to create multiple levels of directories at once (i.e., to act recursively): by default, mkdir errors out if the parent directory/ies of the specified directory don’t yet exist.\nmkdir newdir1/newdir2\nmkdir: cannot create directory ‘newdir1/newdir2’: No such file or directory\n# This successfully creates both directories\nmkdir -p newdir1/newdir2\nIf the directory already exists, it won’t do anything and won’t return an error. Without this option, mkdir would return an error in this case, which would in turn lead the script to abort at that point with our set settings:\nmkdir newdir1/newdir2\nmkdir: cannot create directory ‘newdir1/newdir2’: File exists\n# This does nothing since the dirs already exist\nmkdir -p newdir1/newdir2\n\n\n\n\nWith those additions, our partial script would look like this:\n# [Don't copy or run this - we'll add to it later]\n\n#!/bin/bash\nset -euo pipefail\n\n# Load the OSC module for FastQC\nmodule load fastqc\n\n# Copy the placeholder variables\nfastq_file=$1\noutdir=$2\n\n# Create the output dir if needed\nmkdir -p \"$outdir\"\n\n# Run FastQC\nfastqc --outdir \"$outdir\" \"$fastq_file\"\nNotice that this script to run a CLI tool is very similar to our “toy scripts” from the previous sessions: mostly standard (“boilerplate”) code with just a single command to run our program of interest. Therefore, you can adopt this script as a template for scripts that run other command-line programs, and will generally only need minor modifications!\n\n\n\n3.3 Add some “logging” statements\nIt is often useful to have your scripts “report” or “log” what is going on. For instance:\n\nAt what date and time did we run this script.\nWhich arguments were passed to the script.\nWhat are the designated output dirs/files.\nPerhaps even summaries of the output (we won’t do this here).\n\nAll of this can help with troubleshooting and record-keeping3. Let’s try this with our FastQC script:\n#!/bin/bash\nset -euo pipefail\n\n# Load the OSC module for FastQC\nmodule load fastqc\n\n# Copy the placeholder variables\nfastq_file=$1\noutdir=$2\n\n# Initial reporting\necho \"# Starting script fastqc.sh\"\ndate\necho \"# Input FASTQ file:   $fastq_file\"\necho \"# Output dir:         $outdir\"\necho\n\n# Create the output dir if needed\nmkdir -p \"$outdir\"\n\n# Run FastQC\nfastqc --outdir \"$outdir\" \"$fastq_file\"\n\n# Final reporting\necho\necho \"# Done with script fastqc.sh\"\ndate\nA couple of notes about the lines that were added to the script above:\n\nWe printed a “marker line” Done with script that indicates the end of the script was reached. This is handy due to our set settings: seeing this line printed means that no errors were encountered.\nRunning date at the beginning and end of the script is one way to check the running time of the script.\nPrinting the input files can be particularly useful for troubleshooting.\nThe lines that only have echo will simply print a blank line, basically as a separator between sections.\n\n Create a script to run FastQC:\ntouch scripts/fastqc.sh\nOpen it and paste in the code in the box above.\n\n\n3.4 Add the program version\nHere is one small best-practice addition to shell scripts that was used in last week’s exercises, which I also wanted to point out here (you may want to use this in your final project!). For scripts that run bioinformatics tools, it’s a good idea include a line to print the program version. For example, the FastQC script contained the following lines at the end:\necho \"# Used FastQC version:\"\nfastqc --version\nThis way you’ll always have a clear record of the version that you used when you ran the script, at least if you keep the Slurm log file (and in last week’s exercises, we additionally moved Slurm log files to a logs dir within the output dir)."
  },
  {
    "objectID": "week05/w5_2_cli-tools.html#a-runner-script",
    "href": "week05/w5_2_cli-tools.html#a-runner-script",
    "title": "Running command-line tools with shell scripts",
    "section": "4 A “runner” script",
    "text": "4 A “runner” script\n\n4.1 Running our FastQC script for 1 file\nLet’s run our FastQC script for one FASTQ file:\nbash scripts/fastqc.sh garrigos_data/fastq/ERR10802863_R1.fastq.gz results/fastqc\n# Starting script fastqc.sh\nWed Mar 27 21:53:13 EDT 2024\n# Input FASTQ file:   garrigos_data/fastq/ERR10802863_R1.fastq.gz\n# Output dir:         results/fastqc\n\nStarted analysis of ERR10802863_R1.fastq.gz\nApprox 5% complete for ERR10802863_R1.fastq.gz\nApprox 10% complete for ERR10802863_R1.fastq.gz\nApprox 15% complete for ERR10802863_R1.fastq.gz\nApprox 20% complete for ERR10802863_R1.fastq.gz\nApprox 25% complete for ERR10802863_R1.fastq.gz\nApprox 30% complete for ERR10802863_R1.fastq.gz\nApprox 35% complete for ERR10802863_R1.fastq.gz\nApprox 40% complete for ERR10802863_R1.fastq.gz\nApprox 45% complete for ERR10802863_R1.fastq.gz\nApprox 50% complete for ERR10802863_R1.fastq.gz\nApprox 55% complete for ERR10802863_R1.fastq.gz\nApprox 60% complete for ERR10802863_R1.fastq.gz\nApprox 65% complete for ERR10802863_R1.fastq.gz\nApprox 70% complete for ERR10802863_R1.fastq.gz\nApprox 75% complete for ERR10802863_R1.fastq.gz\nApprox 80% complete for ERR10802863_R1.fastq.gz\nApprox 85% complete for ERR10802863_R1.fastq.gz\nApprox 90% complete for ERR10802863_R1.fastq.gz\nApprox 95% complete for ERR10802863_R1.fastq.gz\nApprox 100% complete for ERR10802863_R1.fastq.gz\nAnalysis complete for ERR10802863_R1.fastq.gz\n\n# Done with script fastqc.sh\nWed Mar 27 21:53:19 EDT 2024\nHowever, as discussed above, we’ll want to run the script for each FASTQ file. We’ll have to write a loop to do so, and that loop will go in a “runner script”.\n\n\n\n4.2 What are runner scripts and why do we need them\nThe loop code could be directly typed in the terminal, but it’s better to save this in a file/script as well.\nWe will now create such a file, which has the overall purpose of documenting the steps we took. You can think of this file as akin to an analysis lab notebook4. Because it will contain shell code, we will save it as a shell script (.sh) just like the script to run fastqc.sh and other individual analysis steps.\nHowever, it is important to realize that this script is conceptually different from the scripts that run individual steps of your analysis. The latter are meant to be run/submitted in their entirety by the runner script, whereas commands in the former typically have to be run one-by-one, i.e. interactively. This kind of script is sometimes called a “runner” or “master” script.\nTo summarize, we’ll separate our code into two hierarchical levels of scripts:\n\nScripts that run individual steps of your analysis, like fastqc.sh. These will eventually be submitted a batch jobs.\nAn overarching “runner” script with code that we run interactively, to orchestrates batch job submission of the individual steps.\n\nTo make this division clearer, we’ll also save these scripts in separate directories:\n\nscripts for the analysis scripts.\nrun for the runner script(s).\n\n\n\n\n\n\n\n\nKeep the scripts for individual steps simple (Click to expand)\n\n\n\n\n\nIt is a good idea to keep the shell scripts you will submit (e.g., fastqc.sh) simple in the sense that they should generally just run one program, and not a sequence of programs.\nOnce you get the hang of writing these scripts, it may seem appealing to string a series of programs/steps together in a single script, so that it’s easier to rerun everything at once — but in practice, that will often end up leading to more difficulties than convenience. If you do want to develop a workflow that can be easily run and rerun from start to finish, you should learn a workflow management system like Snakemake or Nextflow — we will talk about Nextflow in week 6.\n\n\n\n\n\n\n\n\n\nWhy the runner script generally can’t itself be run at once in its entirety (Click to expand)\n\n\n\n\n\nFirst off, not that this applies only once we start submitting our scripts as batch jobs.\nOnce we’ve added multiple batch job steps, and the input of a later step uses the output of an earlier step, we won’t be able to just run the script as is. This is because the runner script would then submit jobs from different steps all at once, and that later step would start running before the earlier step has finished.\nFor example, consider the following series of two steps, in which the second step uses the output of the first step:\n# This script would create a genome \"index\" for STAR, that will be used in the next step\n# ('my_genome.fa' = input genome FASTA, 'results/star_index' = output index dir)\nsbatch scripts/star_index.sh my_genome.fa results/star_index\n\n# This script would align a FASTQ file to the genome index created in the previous step\n# ('results/star_index' = input index dir, 'sampleA.fastq.gz' = input FASTQ file,\n# 'results/star_align' = output dir)\nsbatch scripts/star_align.sh results/star_index sampleA.fastq.gz results/star_align \nIf these two lines were included in your runner script, and you would run that script in its entirety all at once, the script in the second step would be submitted just a split-second after the first one (when using sbatch, you get your prompt back immediately – there is no waiting). As such, it would fail because of the missing output from the first step.\nIt is possible to make sbatch batch jobs wait for earlier steps to finish (e.g. with the --dependency option), but this quickly gets tricky. If you want to create a workflow/pipeline that can run from start to finish in an automated way, you should consider using a workflow management system like Snakemake or NextFlow — we will talk about Nextflow in week 6!\n\n\n\n\n\n\n4.3 Creating our runner script\nCreate a new file, and open it after running these commands:\nmkdir run\ntouch run/run.sh\nIn this script, add the code that runs our fastqc.sh script for each FASTQ file, and then run that code:\n# Run FastQC for each FASTQ file\nfor fastq_file in garrigos_data/fastq/*fastq.gz; do\n    bash scripts/fastqc.sh \"$fastq_file\" results/fastqc\ndone\n# Starting script fastqc.sh\nThu Mar 21 10:06:46 EDT 2024\n# Input FASTQ file:   garrigos_data/fastq/ERR10802863_R1.fastq.gz\n# Output dir:         results/fastqc\n\nStarted analysis of ERR10802863_R1.fastq.gz\nApprox 5% complete for ERR10802863_R1.fastq.gz\nApprox 10% complete for ERR10802863_R1.fastq.gz\nApprox 15% complete for ERR10802863_R1.fastq.gz\nApprox 20% complete for ERR10802863_R1.fastq.gz\nApprox 25% complete for ERR10802863_R1.fastq.gz\nApprox 30% complete for ERR10802863_R1.fastq.gz\nApprox 35% complete for ERR10802863_R1.fastq.gz\nApprox 40% complete for ERR10802863_R1.fastq.gz\nApprox 45% complete for ERR10802863_R1.fastq.gz\nApprox 50% complete for ERR10802863_R1.fastq.gz\nApprox 55% complete for ERR10802863_R1.fastq.gz\nApprox 60% complete for ERR10802863_R1.fastq.gz\nApprox 65% complete for ERR10802863_R1.fastq.gz\nApprox 70% complete for ERR10802863_R1.fastq.gz\nApprox 75% complete for ERR10802863_R1.fastq.gz\nApprox 80% complete for ERR10802863_R1.fastq.gz\nApprox 85% complete for ERR10802863_R1.fastq.gz\nApprox 90% complete for ERR10802863_R1.fastq.gz\nApprox 95% complete for ERR10802863_R1.fastq.gz\nApprox 100% complete for ERR10802863_R1.fastq.gz\nAnalysis complete for ERR10802863_R1.fastq.gz\n\n# Done with script fastqc.sh\nThu Mar 21 10:06:51 EDT 2024\n\n# Starting script fastqc.sh\nThu Mar 21 10:06:51 EDT 2024\n# Input FASTQ file:   garrigos_data/fastq/ERR10802863_R2.fastq.gz\n# Output dir:         results/fastqc\n\nStarted analysis of ERR10802863_R2.fastq.gz\nApprox 5% complete for ERR10802863_R2.fastq.gz\nApprox 10% complete for ERR10802863_R2.fastq.gz\n# [...output truncated...]"
  },
  {
    "objectID": "week05/w5_2_cli-tools.html#looping-over-samples-rather-than-files",
    "href": "week05/w5_2_cli-tools.html#looping-over-samples-rather-than-files",
    "title": "Running command-line tools with shell scripts",
    "section": "5 Looping over samples rather than files",
    "text": "5 Looping over samples rather than files\nIn some cases, we can’t simply loop over all files like we have done so far. For example, in many tools that process paired-end FASTQ files, the corresponding R1 and R2 files for each sample must be processed together. That is, we don’t run the tool separately for each FASTQ file, but separately for each sample i.e. each pair of FASTQ files.\nHow can we loop over pairs of FASTQ files instead? There are two main ways:\n\nCreate a list of sample IDs, loop over these IDs, and find the pair of FASTQ files with matching names.\nLoop over the R1 files only and then infer the name of the corresponding R2 file within the loop. This is generally straightforward because the file names should be identical other than the read-direction identifier (R1/R2).\n\nBelow, we will use the second method — but first, we’ll recap/learn a few prerequisites.\n\n\n5.1 Recap of basename, and dirname\nRunning the basename command on a filename will strip any directories in its name:\nbasename garrigos_data/fastq/ERR10802863_R1.fastq.gz\nERR10802863_R1.fastq.gz\nYou can also provide any arbitrary suffix to also strip from the file name:\nbasename garrigos_data/fastq/ERR10802863_R1.fastq.gz .fastq.gz\nERR10802863_R1\nIf you instead want the directory part of the path, use the dirname command:\ndirname garrigos_data/fastq/ERR10802863_R1.fastq.gz\ngarrigos_data/fastq\n\n\n\n5.2 Parameter expansion\nYou can use so-called “parameter expansion”, with parameter basically being another word for variable, to search-and-replace text in your variable’s values. For example:\n\nAssign a short DNA sequence to a variable:\ndna_seq=\"AAGTTCAT\"\necho \"$dna_seq\"\nAAGTTCAT\nUse parameter expansion to replace all Ts with U to convert the DNA to RNA:\necho \"${dna_seq//T/U}\"\nAAGUUCAT\nYou can also assign the result of the parameter expansion back to a variable:\nrna_seq=\"${dna_seq//T/U}\"\necho \"$rna_seq\"\nAAGUUCAT\n\nSo, the syntax for this type of parameter expansion is {var_name//&lt;search&gt;/replace} — let’s deconstruct that:\n\nReference the variable, using the full notation, with braces (${var_name})\nAfter the first two forward slashes, enter the search pattern: (T)\nAfter another forward slash, enter the replacement: (U).\n\nIf you needed to replace at most one of the search patterns, a single backslash after the variable name would suffice: {var_name/&lt;search&gt;/replace}.\n\n\n Exercise: Get the R2 file name with parameter expansion\nFile names of corresponding R1 and R2 FASTQ files should be identical other than the marker indicating the read direction, which is typically R1/R2 (and in some cases just 1 and 2).\nAssign the file name garrigos_data/fastq/ERR10802863_R1.fastq.gz to a variable and use parameter expansion to get the name of the corresponding R2 file name. Also save the R2 file name in a variable.\n\n\nSolutions\n\nfastq_R1=garrigos_data/fastq/ERR10802863_R1.fastq.gz\nfastq_R2=${fastq_R1/_R1/_R2}\nAbove, e.g. ${fastq_R1/R1/R2}, that is without underscores, would have also worked. But note that it’s generally good to avoid unwanted search-pattern matches by making the search string as specific as possible. So perhaps ${fastq_R1/_R1.fastq.gz/_R2.fastq.gz} would have been even better.\nTest that it worked:\necho \"$fastq_R2\"\ngarrigos_data/fastq/ERR10802863_R2.fastq.gz\n\n\n\n\n\n5.3 A per-sample loop\nWe will now create a loop that:\n\nLoops over R1 FASTQ files only, and then infers the corresponding R2 file name.\nDefines output file names that are the same as the input file names but in a different dir.\n\nTo stay focused just on the shell syntax here, we won’t include code to run an actual bioinformatics tool (you’ll do that in this week’s exercises), but will use a fictional tool trimmer:\n# [Don't run or copy this]\n\n# Loop over the R1 files - our glob is `*_R1.fastq.gz` to only select R1 files\nfor R1_in in garrigos_data/fastq/*_R1.fastq.gz; do\n    # Get the R2 file name with parameter expansion\n    R2_in=${R1_in/_R1/_R2}\n    \n    # Define the output files (assume that a variable $outdir exists)\n    R1_out=\"$outdir\"/$(basename \"$R1_in\")\n    R2_out=\"$outdir\"/$(basename \"$R2_in\")\n    \n    # Report\n    echo \"Input files: $R1_in $R2_in\"\n    echo \"Output files: $R1_out $R2_out\"\n    \n    # Use the imaginary program 'trimmer' with options --in1/--in2 for the R1/R2 input files,\n    # and --out1/--out2 for the R1/R2 output files:\n    trimmer --in1 \"$R1_in\" --in2 \"$R2_in\" --out1 \"$R1_out\" --out2 \"$R2_out\"\ndone\n\n\n\n5.4 Converting to the single-sample script format\nBut wait! Aren’t we supposed to write a script that only processes one sample at a time, and then run/submit that script with a loop? That’s right, so now that we know what to do, let’s switch to that setup.\nCreate a new script scripts/trim_mock.sh and paste the following code into it:\n#!/bin/bash\nset -euo pipefail\n\n# Copy the placeholder variables\nR1_in=$1\noutdir=$2\n\n# Create the output dir if needed\nmkdir -p \"$outdir\"\n\n# Infer the R2_in file name\nR2_in=${R1_in/_R1/_R2}\n    \n# Define the output file names\nR1_out=\"$outdir\"/$(basename \"$R1_in\")\nR2_out=\"$outdir\"/$(basename \"$R2_in\")\n\n# Initial reporting\necho \"# Starting script trim_mock.sh\"\ndate\necho \"# Input R1 file:       $R1_in\"\necho \"# Input R2 file:       $R2_in\"\necho \"# Output R1 file:      $R1_out\"\necho \"# Output R2 file:      $R2_out\"\necho\n\n# Mock-run the tool: I preface the command with 'echo' so this will just report\n# and not try to run a program that doesn't exist\necho trimmer --in1 \"$R1_in\" --in2 \"$R2_in\" --out1 \"$R1_out\" --out2 \"$R2_out\"\n\n# Final reporting\necho\necho \"# Done with script trim_mock.sh\"\ndate\nNow, add the following code to our run.sh script, and run that:\n# Run the trim_mock.sh script for each sample\nfor R1_in in garrigos_data/fastq/*_R1.fastq.gz; do\n    bash scripts/trim_mock.sh \"$R1_in\" results/trim_mock\ndone\n# Starting script trim_mock.sh\nThu Mar 21 10:12:17 EDT 2024\n# Input R1 file:       garrigos_data/fastq/ERR10802863_R1.fastq.gz\n# Input R2 file:       garrigos_data/fastq/ERR10802863_R2.fastq.gz\n# Output R1 file:      results/trim_mock/ERR10802863_R1.fastq.gz\n# Output R2 file:      results/trim_mock/ERR10802863_R2.fastq.gz\n\ntrimmer --in1 garrigos_data/fastq/ERR10802863_R1.fastq.gz --in2 garrigos_data/fastq/ERR10802863_R2.fastq.gz --out1 results/trim_mock/ERR10802863_R1.fastq.gz --out2 results/trim_mock/ERR10802863_R2.fastq.gz\n\n# Done with script trim_mock.sh\nThu Mar 21 10:12:17 EDT 2024\n\n# Starting script trim_mock.sh\nThu Mar 21 10:12:17 EDT 2024\n# Input R1 file:       garrigos_data/fastq/ERR10802864_R1.fastq.gz\n# Input R2 file:       garrigos_data/fastq/ERR10802864_R2.fastq.gz\n# Output R1 file:      results/trim_mock/ERR10802864_R1.fastq.gz\n# Output R2 file:      results/trim_mock/ERR10802864_R2.fastq.gz\n# [...output truncated...]\nCheck the files in the output dir:\nls -lh results/trim_mock\ntotal 0\n-rw-rw----+ 1 jelmer PAS0471 0 Mar 21 10:12 ERR10802863_R1.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 0 Mar 21 10:12 ERR10802863_R2.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 0 Mar 21 10:12 ERR10802864_R1.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 0 Mar 21 10:12 ERR10802864_R2.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 0 Mar 21 10:12 ERR10802865_R1.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 0 Mar 21 10:12 ERR10802865_R2.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 0 Mar 21 10:12 ERR10802866_R1.fastq.gz\n# [...output truncated...]\n\n\n\n5.5 At-home reading: Script arguments and hard-coding\nWe’ve been writing shell scripts that accept arguments, instead of including variable things like inputs and outputs in the scripts themselves. The latter method can be referred to as “hard-coding” these items.\nYou can also use arguments for settings like bioinformatics program options you may want to vary among different runs of the script, or relatedly, for information about your data like read length or RNA-seq library strandedness (as you did in last week’s exercises). If you don’t hard-code these options in your script but have arguments for them, it will be easier to re-use your script in different contexts.\nBut there is balance to be struck here, and you can also “hobble” your script with a confusing array of arguably too many arguments. Along those lines, here are two things to consider:\n\nWhen you do hard-code potentially variable things in your script, it can be a good idea to define them clearly at the top of your script. We actually did this in our primary script to run the nf-core rnaseq pipeline, where we had the following lines to define a variable with the location of the workflow dir:\n# Settings and constants\nWORKFLOW_DIR=software/nfc-rnaseq/3_14_0\nSuch variables can be referred to as “constants”, since they are hard-coded in the script, and in shell code, ALL-CAPS is regularly used for them5.\nIt is also possible to make your shell script accept options. The example below uses both long and short options (e.g. -o | --outdir). It also uses “flag” options that turn functionality on/off (e.g. --no_gcbias) and options that take arguments (e.g. --infile).\n# Process command-line options\nwhile [ \"$1\" != \"\" ]; do\n    case \"$1\" in\n        -o | --outdir )     shift && outdir=$1 ;;\n        -i | --infile )     shift && infile=$1 ;;\n        --transcripts )     shift && transcripts=$1 ;;\n        --no_gcbias )       gcbias=false ;;\n        --dl_container )    dl_container=true ;;\n        -h | --help )       script_help; exit 0 ;;\n        -v | --version )    echo \"Version 2024-04-13\" && exit 0 ;;\n        * )                 echo \"Invalid option $1\" && exit 1 ;;\n    esac\n    shift\ndone\nSuch a script could for example be run like so:\nsbatch scripts/salmon.sh -i my.bam -o results/salmon --no_gcbias\nThat’s more readable less error-prone than “anonymous” positional arguments. It also makes it possible to add several/many settings that have defaults — when a script only takes arguments, all of them always need to be provided."
  },
  {
    "objectID": "week05/w5_2_cli-tools.html#footnotes",
    "href": "week05/w5_2_cli-tools.html#footnotes",
    "title": "Running command-line tools with shell scripts",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor a full list of installed software at OSC: https://www.osc.edu/resources/available_software/software_list.↩︎\n We’ll talk more about loading (and installing) software at OSC next week.↩︎\nWe’ll see in the upcoming Slurm module that we when submit scripts to the OSC queue (rather than running them directly), the output of scripts that is normally printed to screen, will instead go to a sort of “log” file. So, your script’s reporting will end up in this file.↩︎\n Or depending on how you use this exactly, as your notebook entry that contains the final protocol you followed.↩︎\n But this is far from universal, it is also fairly common to use all-caps for all shell variables, but this is not what we’ve been doing.↩︎"
  },
  {
    "objectID": "week05/w5_ga_shell-scripts.html",
    "href": "week05/w5_ga_shell-scripts.html",
    "title": "Graded Assignment III: Shell scripts",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "week12/w12_exercises.html",
    "href": "week12/w12_exercises.html",
    "title": "Week 12 Exercises: R Data visualization",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "week15/w15_exercises.html",
    "href": "week15/w15_exercises.html",
    "title": "Week 15 Exercises: Running nf-core pipelines",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "week15/w15_1_nfcore.html#introduction",
    "href": "week15/w15_1_nfcore.html#introduction",
    "title": "Running an nf-core Nextflow pipeline",
    "section": "1 Introduction",
    "text": "1 Introduction\nTo learn how to run a Nextflow/nf-core pipeline, we’ll use nf-core rnaseq, which performs a reference-based RNA-seq analysis.\n\nGeneral mechanics of running a pipeline\nRunning a pipeline like this is a bit different, and more involved, than running a typical piece of bioinformatics software:\n\nThe pipeline submits Slurm batch jobs for us, and tries to parallelize as much as possible, spawning many jobs. The main pipeline process functions only to orchestrate these jobs, and will keep running until the pipeline has finished or failed. We will need a small “config file” to tell the pipeline how to submit jobs.\nWe only need an installation of Nextflow, and a download of the pipeline’s code and associated files. The pipeline then runs all its constituent tools via separate Singularity containers that it will download1.\nNextflow distinguishes between a final output dir and a “work dir”. All processes/jobs will run and produce outputs in various sub-dirs of the work dir. After each process, only certain output files are copied to the final output dir2. This distinction is very useful at OSC, where a Scratch dir is most suitable as the work dir (due to its fast I/O and ample storage space), while a Project dir is most suitable as the final output dir.\nWhen you run a pipeline, there is a distinction between:\n\nPipeline-specific options to e.g. set inputs and outputs and customize which steps will be run and how (there can be 100+ of these for larger pipelines…). These by convention use a double dash --, e.g. --input.\nGeneral Nextflow options to e.g. pass a configuration file for Slurm batch job submissions and determine resuming/rerunning behavior (see below). These always use a single dash, e.g. -resume.\n\n\n\n\nResuming a pipeline\nFinally, we talked about the need for flexible rerunning of parts of a pipeline earlier. Nextflow can do this when you use the -resume option, which will make a pipeline, for example:\n\nStart where it left off if the previous run failed before finishing, or timed out.\nCheck for changes in input files or pipeline settings, such that it will only rerun what is needed:\n\nAfter adding or removing samples.\nAfter changing/replacing the reference genome files, which would be all steps that make use of the affected file(s) and anything downstream of these steps.\nGiven the settings that have changed; a setting only affecting the very last step will mean that only that step has to be rerun."
  },
  {
    "objectID": "week15/w15_1_nfcore.html#the-nf-core-rnaseq-pipeline",
    "href": "week15/w15_1_nfcore.html#the-nf-core-rnaseq-pipeline",
    "title": "Running an nf-core Nextflow pipeline",
    "section": "2 The nf-core rnaseq pipeline",
    "text": "2 The nf-core rnaseq pipeline\nThe nf-core rnaseq pipeline is meant for RNA-seq projects that:\n\nAttempt to sequence only mRNA while avoiding non-coding RNAs (“mRNA-seq”).\nDo not distinguish between RNA from different cell types (“bulk RNA-seq”).\nUse short reads (≤150 bp) that do not cover full transcripts but do uniquely ID genes.\nUse reference genomes (are reference-based) to associate reads with genes.\nDownstream of this pipeline, such projects typically aim to statistically compare expression between groups of samples, and have multiple biological replicates per group.\n\nThat might seem quite specific, but this is by far the most common use RNA-seq use case. The inputs of this pipeline are FASTQ files with raw reads, and reference genome files (assembly & annotation), while the outputs include a gene count table and many “QC outputs”.\n\n\n\n\n\n\nWhat the pipeline does and does not do (Click to expand)\n\n\n\n\n\nThere are typically two main parts to the kind of RNA-seq data analysis I just described, but this pipeline only does the first:\n\nFrom reads to counts: yes\nGenerating a count table using the reads & the reference genome.\nCount table analysis: no\nDifferential expression analysis, function enrichment analysis, etc.\n\nThat makes sense because the latter part is not nearly as “standardized” as the first. It also does not need much computing power or parallelization, and is best done interactively using a language like R.\n\n\n\n\n\nThe main steps\nLet’s take a closer look at the steps in the pipeline:\n\n\n\n\n\nRead QC and pre-processing\n\nRead QC (FastQC)\nAdapter and quality trimming (TrimGalore)\nOptional removal of rRNA (SortMeRNA) — off by default, but we will include this\n\nAlignment & quantification\n\nAlignment to the reference genome/transcriptome (STAR)\nGene expression quantification (Salmon)\n\nPost-processing, QC, and reporting\n\nPost-processing alignments: sort, index, mark duplicates (samtools, Picard)\nAlignment/count QC (RSeQC, Qualimap, dupRadar, Preseq, DESeq2)\nCreate a QC/metrics report (MultiQC)\n\n\n\n\n\n\n\n\nCustomizing what the pipeline runs (Click to expand)\n\n\n\n\n\nThis pipeline is quite flexible and you can turn several steps off, add optional steps, and change individual options for most tools that the pipeline runs.\n\nOptional removal of contaminants (BBSplit)\nMap to 1 or more additional genomes whose sequences may be present as contamination, and remove reads that map better to contaminant genomes.\nAlternative quantification routes\n\nUse RSEM instead of Salmon to quantify.\nSkip STAR and perform direct pseudo-alignment & quantification with Salmon.\n\nTranscript assembly and quantification (StringTie)\nWhile the pipeline is focused on gene-level quantification, it does produce transcript-level counts as well (this is run by default)."
  },
  {
    "objectID": "week15/w15_1_nfcore.html#getting-set-up",
    "href": "week15/w15_1_nfcore.html#getting-set-up",
    "title": "Running an nf-core Nextflow pipeline",
    "section": "3 Getting set up",
    "text": "3 Getting set up\n\n3.1 How we’ll run the pipeline\nThe entire pipeline can be run with a single command. But we do need to do some prep before we can do so, such as:\n\nActivating the software environment and downloading the pipeline files.\nDefining the pipeline’s inputs and outputs, which includes creating a “sample sheet”.\nCreating a small “config file” so Nextflow knows how to submit Slurm batch jobs at OSC.\n\nThe main Nextflow process does not need much computing power (a single core with the default 4 GB of RAM will be sufficient). But even though our VS Code shell already runs on a compute node, we are better off submitting the main process as a batch job as well, because this process can run for many hours, and we want to be able to log off in the mean time.\nLike we’ve done before, we will use both a primary script that will be submitted as a batch job, and a runner script with commands to run interactively including the submission of said batch job:\nmkdir -p week06/nfc-rnaseq\ncd week06/nfc-rnaseq\nmkdir scripts run software\ntouch scripts/nfc-rnaseq.sh run/run.sh\nOpen the run/run.sh in the VS Code editor pane.\n\n\n\n3.2 Activating the Conda environment\nIf you did last week’s exercises (TBA), you created a Conda environment with Nextflow and nf-core tools. If not, you can use my Conda environment. We will activate this environment in our runner script because we’ll first interactively download the pipeline files.\n\n\nCode to create this Conda environment\n\nmodule load miniconda3/23.3.1-py310\nconda create -y -n nextflow -c bioconda nextflow=23.10.1 nf-core=2.13.1\n\n# [Paste this code into the run/run.sh script, then run it in the terminal]\n# Activate the Conda environment (or use mine: /fs/ess/PAS0471/jelmer/conda/nextflow)\nmodule load miniconda3/23.3.1-py310\nconda activate nextflow-23.10\nCheck that Nextflow and nf-core tools can be run by printing the versions:\n# [Run this code directly in the terminal]\nnextflow -v\nnextflow version 23.10.1.5891\n# [Run this code directly in the terminal]\nnf-core --version\n                                          ,--./,-.\n          ___     __   __   __   ___     /,-._.--~\\\n    |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n    | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                          `._,._,'\n\n    nf-core/tools version 2.13.1 - https://nf-co.re\n\nnf-core, version 2.13.1\n\n\n\n3.3 Downloading the pipeline\nWe’ll use the nf-core download command to download the rnaseq pipeline’s files, including the Singularity containers for all individual tools that the pipeline runs.\nFirst, we need to set the environment variable NXF_SINGULARITY_CACHEDIR to tell Nextflow where to store these containers3. Here, we will use a shared PAS2700 dir that already has all the containers, to save some downloading time and storage space — this took around 15 minutes to download. But when you run a pipeline for your own research, you’ll want to use a dir that you have permissions to write to.\n# [Paste this code into the run/run.sh script, then run it in the terminal]\n# Create an environment variable for the container dir\nexport NXF_SINGULARITY_CACHEDIR=/fs/ess/PAS2700/containers\nNext, we’ll run the nf-core download command to download the latest version (3.14.0) of the rnaseq pipeline to software/nfc-rnaseq, and the associated container files to the previously specified dir:\n# [Paste this code into the run/run.sh script, then run it in the terminal]\n# Download the nf-core rnaseq pipeline files\nnf-core download rnaseq \\\n    --revision 3.14.0 \\\n    --outdir software/nfc-rnaseq \\\n    --compress none \\\n    --container-system singularity \\\n    --container-cache-utilisation amend \\\n    --download-configuration\n\n\n\n\n\n\n\n\n\n\n\nExplanation of all options given to nf-core download (Click to expand)\n\n\n\n\n\n\n--revision: The version of the rnaseq pipeline.\n--outdir: The dir to save the pipeline definition files.\n--compress: Whether to compress the pipeline files — we chose not to.\n--container-system: The type of containers to download. This should always be singularity at OSC, because that’s the only supported type.\n--container-cache-utilisation: This is a little technical and not terribly interesting, but we used amend, which will make it check our $NXF_SINGULARITY_CACHEDIR dir for existing containers, and simply download any that aren’t already found there.\n--download-configuration: This will download some configuration files that we will actually not use, but if you don’t provide this option, it will ask you about it when you run the command.\n\nAlso, don’t worry about the following warning, this doesn’t impact the downloading:\n\nWARNING Could not find GitHub authentication token. Some API requests may fail.\n\n\n\n\n\nLet’s take a quick peek at the dirs and files we just downloaded:\n# [Run this code directly in the terminal]\nls software/nfc-rnaseq\n3_14_0  configs\n# [Run this code directly in the terminal]\nls software/nfc-rnaseq/3_14_0\nassets        CODE_OF_CONDUCT.md  LICENSE       nextflow.config       subworkflows\nbin           conf                main.nf       nextflow_schema.json  tower.yml\nCHANGELOG.md  docs                modules       pyproject.toml        workflows\nCITATIONS.md  lib                 modules.json  README.md"
  },
  {
    "objectID": "week15/w15_1_nfcore.html#defining-inputs-and-outputs",
    "href": "week15/w15_1_nfcore.html#defining-inputs-and-outputs",
    "title": "Running an nf-core Nextflow pipeline",
    "section": "4 Defining inputs and outputs",
    "text": "4 Defining inputs and outputs\nNext, we will define the pipeline’s inputs and outputs in the runner script. We’ll save these in variables, which we’ll later pass as arguments to the primary script. We will need the following inputs:\n\nInput 1: A sample sheet: a text file pointing to the FASTQ files (we’ll create this in a bit)\nInput 2: A FASTA reference genome assembly file (we already have this)\nInput 3: A GTF reference genome annotation file (we already have this)\n\n(If you’re not familiar with FASTA or GTF files, take a look a this self-study section at the bottom of this page.)\nAnd we will need these outputs:\n\nOutput 1: The desired output dir for the final pipeline output\nOutput 2: The desired Nextflow “workdir” for the initial pipeline output\n\n# [Paste this into run/run.sh and then run it in the terminal]\n# Defining the pipeline outputs\noutdir=results/nfc-rnaseq\nworkdir=/fs/scratch/PAS2700/week6/nfc-rnaseq/$USER\n\n# Defining the pipeline inputs\nsamplesheet=\"$outdir\"/nfc_samplesheet.csv\nfasta=../../garrigos_data/ref/GCF_016801865.2.fna\ngtf=../../garrigos_data/ref/GCF_016801865.2.gtf\nJust to give you an idea, here is how we will eventually run the primary script (that we have not yet written), passing those inputs and outputs as arguments:\n# [Don't copy or run this yet]\nsbatch scripts/nfc-rnaseq.sh \"$samplesheet\" \"$fasta\" \"$gtf\" \"$outdir\" \"$workdir\"\n\n\n4.1 Preparing the sample sheet\nThis pipeline requires a “sample sheet” as one of its inputs. In the sample sheet, you provide the paths to your FASTQ files and the so-called “strandedness” of your RNA-Seq library.\n\n\n\n\n\n\nRNA-Seq library strandedness (Click to expand)\n\n\n\n\n\nDuring RNA-Seq library prep, information about the directionality of the original RNA transcripts can be retained (resulting in a “stranded” library) or lost (resulting in an “unstranded” library: specify unstranded in the sample sheet).\nIn turn, stranded libraries can prepared either in reverse-stranded (reverse, by far the most common) or forward-stranded (forward) fashion. For more information about library strandedness, see this page.\nThe pipeline also allows for a fourth option: auto, in which case the strandedness is automatically determined at the start of the pipeline by pseudo-mapping a small proportion of the data with Salmon.\n\n\n\nThe sample sheet should be a plain-text comma-separated values (CSV) file. Here is the example file from the pipeline’s documentation:\nsample,fastq_1,fastq_2,strandedness\nCONTROL_REP1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz,auto\nCONTROL_REP1,AEG588A1_S1_L003_R1_001.fastq.gz,AEG588A1_S1_L003_R2_001.fastq.gz,auto\nCONTROL_REP1,AEG588A1_S1_L004_R1_001.fastq.gz,AEG588A1_S1_L004_R2_001.fastq.gz,auto\nSo, we need a header row with column names, then one row per sample, and the following columns:\n\nSample ID (we will simply use the file name part that is shared by R1 and R2).\nR1 FASTQ file path (including the dir, unless these files are in your working dir).\nR2 FASTQ file path (idem).\nStrandedness: unstranded, reverse, forward, or auto. This data is forward-stranded, so we’ll use forward.\n\nWe will create this file with a helper script that comes with the pipeline, and tell the script about the strandedness of the reads, the R1 and R2 FASTQ file suffices, the input FASTQ dir (data/fastq), and the output file ($samplesheet):\n# [Paste this into run/run.sh and then run it in the terminal]\n# Create the dir that will contain the sample sheet\nmkdir -p \"$outdir\"\n\n# Create the sample sheet for the nf-core pipeline\npython3 software/nfc-rnaseq/3_14_0/bin/fastq_dir_to_samplesheet.py \\\n    --strandedness forward \\\n    --read1_extension \"_R1.fastq.gz\" \\\n    --read2_extension \"_R2.fastq.gz\" \\\n    ../../garrigos_data/fastq \\\n    \"$samplesheet\"\nCheck the contents of your newly created sample sheet file:\n# [Run this directly in the terminal]\nhead \"$samplesheet\"\nsample,fastq_1,fastq_2,strandedness\nERR10802863,../../garrigos_data/fastq/ERR10802863_R1.fastq.gz,../../garrigos_data/fastq/ERR10802863_R2.fastq.gz,forward\nERR10802864,../../garrigos_data/fastq/ERR10802864_R1.fastq.gz,../../garrigos_data/fastq/ERR10802864_R2.fastq.gz,forward\nERR10802865,../../garrigos_data/fastq/ERR10802865_R1.fastq.gz,../../garrigos_data/fastq/ERR10802865_R2.fastq.gz,forward\nERR10802866,../../garrigos_data/fastq/ERR10802866_R1.fastq.gz,../../garrigos_data/fastq/ERR10802866_R2.fastq.gz,forward\nERR10802867,../../garrigos_data/fastq/ERR10802867_R1.fastq.gz,../../garrigos_data/fastq/ERR10802867_R2.fastq.gz,forward\nERR10802868,../../garrigos_data/fastq/ERR10802868_R1.fastq.gz,../../garrigos_data/fastq/ERR10802868_R2.fastq.gz,forward\nERR10802869,../../garrigos_data/fastq/ERR10802869_R1.fastq.gz,../../garrigos_data/fastq/ERR10802869_R2.fastq.gz,forward\nERR10802870,../../garrigos_data/fastq/ERR10802870_R1.fastq.gz,../../garrigos_data/fastq/ERR10802870_R2.fastq.gz,forward\nERR10802871,../../garrigos_data/fastq/ERR10802871_R1.fastq.gz,../../garrigos_data/fastq/ERR10802871_R2.fastq.gz,forward\n\n\n\n4.2 Creating an OSC configuration file\nTo tell the pipeline how to submit Slurm batch jobs for us, we have to use a configuration (config) file. There are multiple ways of storing this file and telling Nextflow about it — we’ll simply create a file nextflow.config in the dir from which we submit the nextflow run command: Nextflow will automatically detect and parse such a file.\nWe’ll keep this file as simple as possible, specifying only our “executor” program (Slurm) and OSC project:\n# [Paste this into run/run.sh and then run it in the terminal]\n# Create a config file for batch job submissions\necho \"\nprocess.executor='slurm'\nprocess.clusterOptions='--account=PAS2700'\n\" &gt; nextflow.config\n\n\n\n\n\n\nMulti-line echo strings\n\n\n\nIt might look odd, but you can include newlines in echo strings as shown above. Note also that we are using single quotes ('...') inside the string, because a double quote would mark the end the string."
  },
  {
    "objectID": "week15/w15_1_nfcore.html#writing-a-shell-script-to-run-the-pipeline",
    "href": "week15/w15_1_nfcore.html#writing-a-shell-script-to-run-the-pipeline",
    "title": "Running an nf-core Nextflow pipeline",
    "section": "5 Writing a shell script to run the pipeline",
    "text": "5 Writing a shell script to run the pipeline\nNow, we’ll go through the key parts of the script scripts/nfc-rnaseq.sh that you will submit as a Slurm batch job.\n\n5.1 Processing arguments passed to the script\nThe inputs and outputs we discussed above, and the Nextflow “workdir”, will be the arguments passed to the script:\n# [Partial shell script code, don't copy or run]\n# Process command-line arguments\nsamplesheet=$1\nfasta=$2\ngtf=$3\noutdir=$4\nworkdir=$5\n\n\n\n5.2 Building the nextflow run command\nTo run the pipeline, use the command nextflow run, followed by the path to the dir with pipeline files:\n# [Partial shell script code, don't copy or run]\nnextflow run software/nfc-rnaseq/3_14_0\nAfter that, there are several required options (see the pipeline’s documentation), which represent the inputs and outputs that we talked about above:\n\n--input: The path to the “sample sheet”.\n--fasta: The path to the reference genome assembly FASTA file.\n--gtf: The path to the reference genome annotation GTF file.\n--outdir: The path to the output dir.\n\nThis pipeline has different options for e.g. alignment and quantification. We will stick close to the defaults, which includes alignment with STAR and quantification with Salmon, with one exception: we want to remove reads from ribosomal RNA (this step is skipped by default).\n\n\n Exercise: Find the option to remove rRNA\nTake a look at the “Parameters” tab on the pipeline’s documentation website:\n\nBrowse through the options for a bit to get a feel for the extent to which you can customize the pipeline.\nTry to find the option to turn on removal of rRNA with SortMeRNA.\n\n\n\nClick for the solution\n\nThe option we want is --remove_ribo_rna.\n\n\n\nWe’ll also use several general Nextflow options (note the single dash - option notation):\n\n-profile: A so-called “profile” to indicate how the pipeline should run software — this should be singularity when running the pipeline with Singularity containers.\n-work-dir: The dir in which all the pipeline’s jobs/processes will run.\n-ansi-log false: Change Nextflow’s progress “logging” type to a format that works with Slurm log files4.\n-resume: Resume the pipeline where it “needs to” (e.g., where it left off) instead of always starting over. (This option won’t make any difference when we run the pipeline for the first time, since there is nothing to resume. Nextflow will even give a warning along these lines, but this is not a problem.)\n\nWith all above-mentioned options, your final nextflow run command is:\n# [Partial shell script code, don't copy or run]\nnextflow run software/nfc-rnaseq/3_14_0 \\\n    --input \"$samplesheet\" \\\n    --fasta \"$fasta\" \\\n    --gtf \"$gtf\" \\\n    --outdir \"$outdir\" \\\n    --remove_ribo_rna \\\n    -work-dir \"$workdir\" \\\n    -profile singularity \\\n    -ansi-log false \\\n    -resume\n\n\n\n5.3 The final script\nBelow is the full code for the script, in which I also added:\n\nOur standard shell script header lines.\n#SBATCH options: note that these are only for the “main” Nextflow job, not for the jobs that Nextflow itself will submit! So we ask for quite a bit of time5, but we don’t need more than the default 1 core and 4 GB of RAM.\nSome echo reporting of arguments/variables, printing the date, etc.\n\nOpen your scripts/nfc-rnaseq.sh script and paste the following into it:\n#!/bin/bash\n#SBATCH --account=PAS2700\n#SBATCH --time=6:00:00\n#SBATCH --mail-type=END,FAIL\n#SBATCH --output=slurm-nfc_rnaseq-%j.out\nset -euo pipefail\n\n# Settings and constants\nWORKFLOW_DIR=software/nfc-rnaseq/3_14_0\n\n# Load the Nextflow Conda environment\nmodule load miniconda3/23.3.1-py310\nconda activate /fs/ess/PAS0471/jelmer/conda/nextflow\nexport NXF_SINGULARITY_CACHEDIR=/fs/ess/PAS0471/containers\n\n# Process command-line arguments\nif [[ ! \"$#\" -eq 5 ]]; then\n    echo \"Error: wrong number of arguments\"\n    echo \"You provided $# arguments, while 5 are required.\"\n    echo \"Usage: nfc-rnaseq.sh &lt;samplesheet&gt; &lt;FASTA&gt; &lt;GTF&gt; &lt;outdir&gt; &lt;workdir&gt;\"\n    exit 1\nfi\nsamplesheet=$1\nfasta=$2\ngtf=$3\noutdir=$4\nworkdir=$5\n\n# Report\necho \"Starting script nfc-rnaseq.sh\"\ndate\necho \"Samplesheet:          $samplesheet\"\necho \"Reference FASTA:      $fasta\"\necho \"Reference GTF:        $gtf\"\necho \"Output dir:           $outdir\"\necho \"Nextflow work dir:    $workdir\"\necho\n\n# Create the output dirs\nmkdir -p \"$outdir\" \"$workdir\"\n\n# Run the workflow\nnextflow run \"$WORKFLOW_DIR\" \\\n    --input \"$samplesheet\" \\\n    --fasta \"$fasta\" \\\n    --gtf \"$gtf\" \\\n    --outdir \"$outdir\" \\\n    --remove_ribo_rna \\\n    -work-dir \"$workdir\" \\\n    -profile singularity \\\n    -ansi-log false \\\n    -resume\n\n# Report\necho \"Done with script nfc-rnaseq.sh\"\ndate"
  },
  {
    "objectID": "week15/w15_1_nfcore.html#running-the-pipeline",
    "href": "week15/w15_1_nfcore.html#running-the-pipeline",
    "title": "Running an nf-core Nextflow pipeline",
    "section": "6 Running the pipeline",
    "text": "6 Running the pipeline\n\n6.1 Submitting your shell script\nBefore you submit the script, check that all variables have been assigned by prefacing the command with echo:\n# [ Run this directly in the terminal]\necho sbatch scripts/nfc-rnaseq.sh \"$samplesheet\" \"$fasta\" \"$gtf\" \"$outdir\" \"$workdir\"\nsbatch scripts/nfc-rnaseq.sh results/nfc-rnaseq/nfc_samplesheet.csv data/ref/GCF_016801865.2.fna data/ref/GCF_016801865.2.gtf results/nfc-rnaseq /fs/scratch/PAS2700/week6/nfc-rnaseq/jelmer\nIf so, you are ready to submit the script as a batch job:\n# [Paste this into run/run.sh and then run it in the terminal]\n# Submit the script to run the pipeline as a batch job\nsbatch scripts/nfc-rnaseq.sh \"$samplesheet\" \"$fasta\" \"$gtf\" \"$outdir\" \"$workdir\"\nSubmitted batch job 27767854\n\n\n\nNot sure your run.sh script is complete, or getting errors? Click for its intended content.\n\n# Activate the Conda environment (or use mine: /fs/ess/PAS0471/jelmer/conda/nextflow)\nmodule load miniconda3/23.3.1-py310\nconda activate nextflow-23.10\n\n# Create an environment variable for the container dir\nexport NXF_SINGULARITY_CACHEDIR=/fs/ess/PAS2700/containers\n\n# Download the nf-core rnaseq pipeline files\nnf-core download rnaseq \\\n    --revision 3.14.0 \\\n    --outdir software/nfc-rnaseq \\\n    --compress none \\\n    --container-system singularity \\\n    --container-cache-utilisation amend \\\n    --download-configuration\n\n# Defining the pipeline outputs\noutdir=results/nfc-rnaseq\nworkdir=/fs/scratch/PAS2700/week6/nfc-rnaseq/$USER\n\n# Defining the pipeline inputs\nsamplesheet=\"$outdir\"/nfc_samplesheet.csv\nfasta=../../garrigos_data/ref/GCF_016801865.2.fna\ngtf=../../garrigos_data/ref/GCF_016801865.2.gtf\n\n# Create the dir that will contain the sample sheet\nmkdir -p \"$outdir\"\n\n# Create the sample sheet for the nf-core pipeline\npython3 software/nfc-rnaseq/3_14_0/bin/fastq_dir_to_samplesheet.py \\\n    --strandedness forward \\\n    --read1_extension \"_R1.fastq.gz\" \\\n    --read2_extension \"_R2.fastq.gz\" \\\n    ../../garrigos_data/fastq \\\n    \"$samplesheet\"\n\n# Create a config file for batch job submissions\necho \"\nprocess.executor='slurm'\nprocess.clusterOptions='--account=PAS2700'\n\" &gt; nextflow.config\n\n# Submit the script to run the pipeline as a batch job\nsbatch scripts/nfc-rnaseq.sh \"$samplesheet\" \"$fasta\" \"$gtf\" \"$outdir\"\n\n\n\n\n6.2 Checking the pipeline’s progress\nLet’s check whether your job has started running, and if so, whether Nextflow has already spawned jobs:\n# [Run this directly in the terminal]\nsqueue -u $USER -l\nMon Mar 25 12:13:38 2024\n      JOBID PARTITION     NAME     USER    STATE   TIME TIME_LIMI  NODES NODELIST(REASON)\n  27767854 serial-40 nfc-rnas   jelmer  RUNNING    1:33   6:00:00      1 p0219\nIn the example output above, the only running job is the one we directly submitted, i.e. the main Nextflow process. The NAME column is the script’s name, nfc-rnaseq.sh (truncated to nfc-rnas).\n\n\n\n\n\n\nSee examples of squeue output that includes Nextflow-submitted jobs (Click to expand)\n\n\n\n\n\nThe top job, with partial name nf-NFCOR, is a job that’s been submitted by Nextflow:\nsqueue -u $USER -l\nMon Mar 25 13:14:53 2024\n             JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n          27767861 serial-40 nf-NFCOR   jelmer  RUNNING       5:41  16:00:00      1 p0053\n          27767854 serial-40 nfc_rnas   jelmer  RUNNING    1:03:48   6:00:00      1 p0219\nUnfortunately, the columns in the output above are quite narrow, so it’s not possible to see which step of the pipeline is being run by that job. The following (awful-looking!) code can be used to make that column much wider, so you can see the job’s full name which makes clear which step is being run (rRNA removal with SortMeRNA):\nsqueue -u $USER --format=\"%.9i %.9P %.60j %.8T %.10M %.10l %.4C %R %.16V\"\nMon Mar 25 13:15:05 2024\n    JOBID PARTITION                                                          NAME    STATE       TIME TIME_LIMIT CPUS NODELIST(REASON)      SUBMIT_TIME\n 27767861 serial-40   nf-NFCORE_RNASEQ_RNASEQ_SORTMERNA_(SRR27866691_SRR27866691)  RUNNING       5:55   16:00:00   12 p0053 2024-03-23T09:37\n 27767854 serial-40                                                    nfc_rnaseq  RUNNING    1:04:02    6:00:00    1 p0219 2024-03-23T09:36\nYou might also catch the pipeline while there are many more jobs running, e.g.:\nMon Mar 25 13:59:50 2024\n             JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n          27823107 serial-40 nf-NFCOR   jelmer  RUNNING       0:13  16:00:00      1 p0091\n          27823112 serial-40 nf-NFCOR   jelmer  RUNNING       0:13  16:00:00      1 p0119\n          27823115 serial-40 nf-NFCOR   jelmer  RUNNING       0:13  16:00:00      1 p0055\n          27823120 serial-40 nf-NFCOR   jelmer  RUNNING       0:13  16:00:00      1 p0055\n          27823070 serial-40 nf-NFCOR   jelmer  RUNNING       0:43  16:00:00      1 p0078\n          27823004 serial-40 nfc-rnas   jelmer  RUNNING       2:13   6:00:00      1 p0146\n          27823083 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0078\n          27823084 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0096\n          27823085 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0096\n          27823086 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0115\n          27823087 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0115\n          27823088 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0123\n          27823089 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0123\n          27823090 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0057\n          27823091 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0057\n          27823092 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0058\n          27823093 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0058\n          27823095 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0118\n          27823099 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0118\n          27823103 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0119\n          27823121 serial-48 nf-NFCOR   jelmer  RUNNING       0:13  16:00:00      1 p0625\n          27823122 serial-48 nf-NFCOR   jelmer  RUNNING       0:13  16:00:00      1 p0744\n          27823123 serial-48 nf-NFCOR   jelmer  RUNNING       0:13  16:00:00      1 p0780\n          27823124 serial-48 nf-NFCOR   jelmer  RUNNING       0:13  16:00:00      1 p0780\n\n\n\nYou can also keep an eye on the pipeline’s progress, and see if there are any errors, by checking the Slurm log file — the top of the file should look like this:\n# You will have a different job ID - replace as appropriate or use Tab completion\n# (We need the -R option to display colors properly)\nless -R slurm-nfc_rnaseq-27767861.out\nStarting script nfc-rnaseq.sh\nMon Mar 25 13:01:30 EDT 2024\nSamplesheet:          results/nfc-rnaseq/nfc_samplesheet.csv\nReference FASTA:      data/ref/GCF_016801865.2.fna\nReference GTF:        data/ref/GCF_016801865.2.gtf\nOutput dir:           results/nfc-rnaseq\nNextflow workdir:     /fs/scratch/PAS2700/week6/nfc-rnaseq/jelmer\n\nN E X T F L O W  ~  version 23.10.1\nWARN: It appears you have never run this project before -- Option `-resume` is ignored\nLaunching `software/nfc-rnaseq/3_14_0/main.nf` [curious_linnaeus] DSL2 - revision: 746820de9b\nWARN: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  Multiple config files detected!\n  Please provide pipeline parameters via the CLI or Nextflow '-params-file' option.\n  Custom config files including those provided by the '-c' Nextflow option can be\n  used to provide any configuration except for parameters.\n\n  Docs: https://nf-co.re/usage/configuration#custom-configuration-files\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n------------------------------------------------------\n                                        ,--./,-.\n        ___     __   __   __   ___     /,-._.--~'\n  |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n  | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                        `._,._,'\n  nf-core/rnaseq v3.14.0\n------------------------------------------------------\nCore Nextflow options\n  runName                   : curious_linnaeus\n  containerEngine           : singularity\n[...output truncated...]\nThe warnings about -resume and config files shown above can be ignored. Some of this output has nice colors that was not shown above:\n\n\n\n\n\nIn the Slurm log file, pipeline progress is shown in the following way — you can only see which jobs are being submitted, not when they finish6:\n[6e/d95b6f] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:PREPARE_GENOME:CUSTOM_GETCHROMSIZES (GCF_016801865.2.fna)\n[ee/eba07d] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:FASTQC (ERR10802864)\n[28/131e16] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:FASTQC (ERR10802863)\n[36/22cf36] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:TRIMGALORE (ERR10802863)\n[59/402585] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:TRIMGALORE (ERR10802864)\n[c3/27b16d] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:FASTQC (ERR10802865)\n[b0/f78597] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:TRIMGALORE (ERR10802865)\n[4c/253978] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:PREPARE_GENOME:GTF_FILTER (GCF_016801865.2.fna)\n[39/264a8e] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:FASTQC (ERR10802866)\n[ec/6aa13c] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:TRIMGALORE (ERR10802866)\n[dd/73a5e4] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:FASTQC (ERR10802868)\n[d8/c1bb3e] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:FASTQC (ERR10802867)\n\n\nYou should also see the following warning among the job submissions (Click to expand)\n\nThis warning can be ignored, the “Biotype QC” is not important and this information is indeed simply missing from our GTF file, there is nothing we can do about that.\nWARN: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  Biotype attribute 'gene_biotype' not found in the last column of the GTF file!\n\n  Biotype QC will be skipped to circumvent the issue below:\n  https://github.com/nf-core/rnaseq/issues/460\n\n  Amend '--featurecounts_group_type' to change this behaviour.\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\nBut if errors occur, they are reported in this file, and there is also a message when the entire pipeline has finished:\n[28/79e801] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:BEDGRAPH_BEDCLIP_BEDGRAPHTOBIGWIG_FORWARD:UCSC_BEDGRAPHTOBIGWIG (ERR10802864)\n[e0/ba48c9] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:BEDGRAPH_BEDCLIP_BEDGRAPHTOBIGWIG_REVERSE:UCSC_BEDGRAPHTOBIGWIG (ERR10802864)\n[62/4f8c0d] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:MULTIQC (1)\n-[nf-core/rnaseq] Pipeline completed successfully -\nDone with script nfc-rnaseq.sh\nMon Mar 25 14:09:52 EDT 2024\n\n\n\n\n\n\n\nWork-dir paths and output\n\n\n\nThe hexadecimals between square brackets (e.g. [62/4f8c0d]) that are printed before each job point to the sub-directory within the pipeline’s work-dir that the this process is running in (every individual process runs in its own dir). This can be handy for troubleshooting, or even just to take a closer look at what exactly the pipeline is running.\n\nFirst, list the contents of the top-level work-dir:\nls /fs/scratch/PAS2700/week6/nfc-rnaseq/$USER\n28  36  39  59  6e  b0  b7  c3  c9  collect-file  d5  d8  da  dd  e0  ec  ee  tmp\nLet’s look at the files for one of the processes (jobs) we saw in the log file, the last FastQC process, which had the workdir reference [d8/c1bb3e] — we’ll use the -a option to ls so hidden files are also listed, some of which are of interest:\n# (The full name of the `c1bb3e` dir is much longer, but these characters uniquely\n#  identify it, so we can use a *)\nls -a /fs/scratch/PAS2700/week6/nfc-rnaseq/jelmer/d8/c1bb3e*\n.               .command.err  .command.run    ERR10802867_1_fastqc.html  ERR10802867_2_fastqc.html  ERR10802867_R1.fastq.gz  versions.yml\n..              .command.log  .command.sh     ERR10802867_1_fastqc.zip   ERR10802867_2_fastqc.zip   ERR10802867_R2.fastq.gz\n.command.begin  .command.out  .command.trace  ERR10802867_1.gz           ERR10802867_2.gz           .exitcode\nTo see exactly how FastQC was run by the pipeline, show the contents of the .command.sh file:\ncat /fs/scratch/PAS2700/week6/nfc-rnaseq/jelmer/d8/c1bb3ea3e697573b4d5860308b2690/.command.sh \n#!/bin/bash -euo pipefail\nprintf \"%s %s\\n\" ERR10802867_R1.fastq.gz ERR10802867_1.gz ERR10802867_R2.fastq.gz ERR10802867_2.gz | while read old_name new_name; do\n    [ -f \"${new_name}\" ] || ln -s $old_name $new_name\ndone\n\nfastqc \\\n    --quiet \\\n    --threads 6 \\\n    ERR10802867_1.gz ERR10802867_2.gz\n\ncat &lt;&lt;-END_VERSIONS &gt; versions.yml\n\"NFCORE_RNASEQ:RNASEQ:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:FASTQC\":\n    fastqc: $( fastqc --version | sed '/FastQC v/!d; s/.*v//' )\nEND_VERSIONS\nLogging and errors would end up in the .command.log, .command.out, and/or .command.err files (when there’s an error, at least part of it will be printed in your Slurm log, but not always the full error).\nThe input files (as links/shortcuts, so the files don’t have to be copied in fill) and output files are also present in this dir, such as ERR10802867_1_fastqc.html in this case."
  },
  {
    "objectID": "week15/w15_1_nfcore.html#the-pipelines-outputs",
    "href": "week15/w15_1_nfcore.html#the-pipelines-outputs",
    "title": "Running an nf-core Nextflow pipeline",
    "section": "7 The pipeline’s outputs",
    "text": "7 The pipeline’s outputs\nYou pipeline run may finish in as little as 15-30 minutes with our small test data set, but this can vary substantially, mostly due to variation in Slurm queue times (the pipeline makes quite large Slurm resource requests!).\nOnce it has finished, you can take a look at the files and dirs in the specified output dir:\nls -lh results/nfc-rnaseq\ntotal 83K\ndrwxr-xr-x   2 jelmer PAS0471  16K Mar 25 13:02 fastqc\ndrwxr-xr-x   2 jelmer PAS0471 4.0K Mar 25 12:58 logs\ndrwxr-xr-x   3 jelmer PAS0471 4.0K Mar 25 13:14 multiqc\n-rw-r--r--   1 jelmer PAS0471 2.0K Mar 25 19:55 nfc_samplesheet.csv\ndrwxr-xr-x   2 jelmer PAS0471 4.0K Mar 25 13:14 pipeline_info\ndrwxr-xr-x 248 jelmer PAS0471  16K Mar 25 13:10 raw\ndrwxr-xr-x   2 jelmer PAS0471 4.0K Mar 25 13:06 sortmerna\ndrwxr-xr-x  33 jelmer PAS0471  16K Mar 25 13:12 star_salmon\ndrwxr-xr-x   3 jelmer PAS0471 4.0K Mar 25 13:02 trimgalore\nThe two most important outputs are:\n\nThe MultiQC report (&lt;outdir&gt;/multiqc/star_salmon/multiqc_report.html): this has lots of QC summaries of the data, both the raw data and the alignments, and even a gene expression PCA plot. Importantly, this file also lists the versions of each piece of software that was used by the pipeline.\nThe gene count table (&lt;outdir&gt;/star_salmon/salmon.merged.gene_counts_length_scaled.tsv): This is what you would use for downstream analysis such as differential expression and functional enrichment analysis.\n\n\n\nOpening the MultiQC report\nIf your run has finished, you can open your MultiQC report — because it’s an HTML file, you’ll have to download it and open it on your own computer. To download the MultiQC HTML file at results/nfc-rnaseq/multiqc/star_salmon/multiqc_report.html, find this file in the VS Code explorer (file browser) on the left, right-click on it, and select Download.... You can download it to any location on your computer. Then find the file on your computer and click on it to open it — it should be opened in your browser.\nAlternatively, you can find a copy of the MultiQC report on this website, and open it in a separate browser tab. There’s a lot of information in the report — if you want to learn more, please go through the self-study section below."
  },
  {
    "objectID": "week15/w15_1_nfcore.html#self-study-i-a-closer-look-at-the-output",
    "href": "week15/w15_1_nfcore.html#self-study-i-a-closer-look-at-the-output",
    "title": "Running an nf-core Nextflow pipeline",
    "section": "8 Self-study I: A closer look at the output",
    "text": "8 Self-study I: A closer look at the output\nIf you’re interested in RNA-seq data analysis, you may want to take a closer look at the outputs of the pipeline, especially the MultiQC report.\n\n\n8.1 The MultiQC report\nHere are some items in that report to pay particular attention to, with example figures from this data set:\nThe General Statistics table (the first section) is very useful, with the following notes:\n\nMost of the table’s content is also in later graphs, but the table allows for comparisons across metrics.\nThe %rRNA (% of reads identified as rRNA and removed by SortMeRNA) can only be found in this table.\nIt’s best to hide the columns with statistics from Samtools, which can be confusing if not downright misleading: click on “Configure Columns” and uncheck all the boxes for stats with Samtools in their name.\nSome stats are for R1 and R2 files only, and some are for each sample as a whole. Unfortunately, this means you get 3 rows per sample in the table.\n\n\n\n\n\n\n\nThe Qualimap &gt; Genomic origin of reads plot shows, for each sample, the proportion of reads mapping to exonic vs. intronic vs. intergenic regions. This is an important QC plot: the vast majority of your reads should be exonic7.\n\n\n\nThis is a good result, with 80-90% of mapped reads in exonic regions.\n\n\n\nThe STAR &gt; Alignment Scores plot shows, for each sample, the percentage of reads that was mapped. Note that “Mapped to multiple loci” reads are also included in the final counts, and that “Unmapped: too short” merely means unmapped, really, and not that the reads were too short.\n\n\n\nThis is a pretty good results, with 80-90% of reads mapped.\n\n\n\nFastQC checks your FASTQ files, i.e. your data prior to alignment. There are FastQC plots both before and after trimming with TrimGalore/Cutadapt. The most important FastQC modules are:\n\nSequence Quality Histograms — You’d like the mean qualities to stay in the “green area”.\nPer Sequence GC Content — Secondary peaks may indicate contamination.\nAdapter Content — Any adapter content should be gone in the post-trimming plot.\n\n\n\n Exercise: Interpreting FastQC results in the MultiQC report\nTake a look at the three FastQC modules discussed above, both before and after trimming.\n\nHas the base quality improved after trimming, and does this look good?\n\n\n\nClick to see the answer\n\n\nPre-trimming graph: The qualities are good overall, but there is more variation that what is usual, and note the poorer qualities in the first 7 or so bases. There is no substantial decline towards the end of the read as one often sees with Illumina data, but this is expected given that the reads are only 75 bp.\n\n\n\n\nPre-trimming (Mean base quality scores: one line is one sample.)\n\n\n\nPost-trimming graph: The qualities have clearly improved. The first 7 or so bases remain of clearly poorer quality, on average.\n\n\n\n\nPost-trimming\n\n\n\n\nDo you have any idea what’s going with the pre-trimming GC content distribution? What about after trimming — does this look good or is there reason to worry?\n\n\n\nClick to see the answer\n\n\nThe pre-trimming GC content is very odd but this is mostly due to a high number of reads with zero and near-zero percent GC content. These are likely reads with only Ns. There are also some reads with near-hundred percent GC content. These are likely artifactual G-only reads that NextSeq/NovaSeq machines can produce.\n\n\n\n\nPre-trimming. One line is one file.\n\n\n\nAfter trimming, things look a lot better but there may be contamination here, given the weird “shoulder” at 30-40% GC.\n\n\n\n\nPost-trimming\n\n\n\n\nDo you know what the “adapters” that FastQC found pre-trimming are? Were these sequences removed by the trimming?\n\n\n\nClick to see the answer\n\n\nPre-trimming, there seem to be some samples with very high adapter content throughout the read. This doesn’t make sense for true adapters, because these are usually only found towards the end of the read, when the read length is longer than the DNA fragment length. If you hover over the lines, you’ll see it says “polyg”. These are artifactual G-only reads that NextSeq/NovaSeq can produce, especially in the reverse reads — and you can see that all of the lines are for reverse-read files indeed.\n\n\n\n\nPre-trimming\n\n\n\nPost-trimming, no adapter content was found.\n\n\n\n\nPost-trimming\n\n\n\n\n\nThe Qualimap &gt; Gene Coverage Profile plot shows average read-depth across the length of genes/transcripts (averaged across all genes), which helps to assess the amount of RNA degradation. For poly-A selected libraries, RNA molecules “begin” at the 3’ end (right-hand side of the graph), so the more degradation there is, the more you expect a higher read-depth towards the 3’ end relative to that at the 5’ end. (Though note that sharp decreases at the very end on each side are expected.)\n\n\n\nThere depth at ~20% (near the 5’ end) is clearly lower than at ~80% (near the 3’ end),indicating some RNA degradation.\n\n\nThe RSeqQC &gt; Infer experiment (library strandedness) plot. If your library is:\n\nUnstranded, there should be similar percentages of Sense and Antisense reads.\nForward-stranded, the vast majority of reads should be Sense.\nReverse-stranded, the vast majority of reads should be Antisense.\n\n\n\n\nThis libary is clearly forward-stranded, as we indicated in our sample sheet.\n\n\nThe STAR_SALMON DESeq2 PCA plot is from a Principal Component Analysis (PCA) run on the final gene count table, showing overall patterns of gene expression similarity among samples.\n\n\n\nThe samples clearly form two distinct groups along PC1.\n\n\nFinally, the section nf-corer/rnaseq Software Versions way at the bottom lists the version of all programs used by the pipeline!\n\n\n\n8.2 The gene count table\nThe gene count table has one row for each gene and one column for each sample, with the first two columns being the gene_id and gene_name8. Each cell’s value contains the read count estimate for a specific gene in a specific sample:\n# [Paste this into the run/run.sh script and run it in the terminal]\n\n# Take a look at the count table:\n# ('column -t' lines up columns, and less's '-S' option turns off line wrapping)\ncounts=results/nfc-rnaseq/star_salmon/salmon.merged.gene_counts_length_scaled.tsv\ncolumn -t \"$counts\" | less -S\ngene_id             gene_name           ERR10802863        ERR10802864        ERR10802865        ERR10802866        ERR10802867        ERR10802868       \nATP6                ATP6                163.611027228009   178.19903533081    82.1025390726658   307.649552934133   225.78249209207    171.251589309856  \nATP8                ATP8                0                  1.01047333891691   0                  0                  0                  0                 \nCOX1                COX1                1429.24769032452   2202.82009602881   764.584344577622   2273.6965332904    2784.47391614249   2000.51277019854  \nCOX2                COX2                116.537361366535   175.137972566817   54.0166352459629   256.592955351283   193.291937038438   164.125833130119  \nCOX3                COX3                872.88670991359    1178.29247734231   683.167933227141   1200.01735304529   1300.3853323715    1229.11746824104  \nCYTB                CYTB                646.028108528182   968.256051104547   529.393909319439   1025.23768317788   1201.46662840336   842.533209911258  \nLOC120412322        LOC120412322        0                  0                  0                  0                  0.995135178345792  0.996805450081561 \nLOC120412324        LOC120412324        37.8326244586681   20.9489661184365   27.6702324729125   48.6417838830061   22.8313729348804   36.87899862428    \nLOC120412325        LOC120412325        3.21074365394071   2.10702898851342   4.40315394778926   5.47978997387391   4.33241716734803   4.23386924919438  \nLOC120412326        LOC120412326        0                  0                  0                  0                  0                  0                 \nLOC120412327        LOC120412327        37.8206758601034   35.9063291323018   38.517771617566    27.7802608986967   37.6979028802121   32.885944667709   \nLOC120412328        LOC120412328        35.0080600370267   20.0019192467143   23.9260736995594   30.0191332346116   21.0383665366408   28.9844776623531  \nLOC120412329        LOC120412329        121.777922287929   112.794544755113   131.434181046282   127.753086659103   114.864750589664   131.589608063253  \nLOC120412330        LOC120412330        42.8505448763697   28.9442284428204   36.6285174684674   46.7310765909945   42.7633834468768   26.9265243413636  \nLOC120412331        LOC120412331        11.013179311581    9.00559907892481   12.9836833055803   13.029954361225    7.02624958751718   16.000552787954   \nLOC120412332        LOC120412332        12.1055360835441   26.1231316926989   21.2767913384733   18.2783703626438   26.4932540325187   22.098808637857   \nLOC120412333        LOC120412333        19.1159998132169   17.0558058070299   12.0965688236319   14.1510477997588   15.2033452089903   9.02624985028677  \nLOC120412334        LOC120412334        9.01332125155807   3.00232591636489   5.99566364212933   11.0306919231504   8.03448732510427   11.0022053123759  \n# [...output truncated...]\n\n\n\n\n\n\nCount table versions\n\n\n\nThe workflow outputs several versions of the count table9, but the one with gene_counts_length_scaled is the one we want:\n\ngene_counts as opposed to transcript_counts for counts that are summed across transcripts for each gene.\nlength for estimates that have been adjusted to account for between-sample differences in mean transcript length (longer transcripts would be expected to produce more reads in sequencing).\nscaled for estimates that have been scaled back using the “library sizes”, per-sample total read counts."
  },
  {
    "objectID": "week15/w15_1_nfcore.html#self-study-ii-reference-genome-files",
    "href": "week15/w15_1_nfcore.html#self-study-ii-reference-genome-files",
    "title": "Running an nf-core Nextflow pipeline",
    "section": "9 Self-study II: Reference genome files",
    "text": "9 Self-study II: Reference genome files\n\n9.1 The genome assembly FASTA\n\nThe FASTA format\nFASTA files contain one or more DNA or amino acid sequences, with no limits on the number of sequences or the sequence lengths. FASTA is the standard format for, e.g.:\n\nGenome assembly sequences\nTranscriptomes and proteomes (all of an organism’s transcripts & amino acid sequences, respectively)\nSequence downloads from NCBI such as a single gene/protein or other GenBank entry\n\nThe following example FASTA file contains two entries:\n&gt;unique_sequence_ID Optional description\nATTCATTAAAGCAGTTTATTGGCTTAATGTACATCAGTGAAATCATAAATGCTAAAAA\n&gt;unique_sequence_ID2\nATTCATTAAAGCAGTTTATTGGCTTAATGTACATCAGTGAAATCATAAATGCTAAATG\nEach entry consists of a header line and the sequence itself. Header lines start with a &gt; (greater-than sign) and are otherwise “free form”, though the idea is that they provide an identifier for the sequence that follows.10\n\n\n\n\n\n\nFASTA file name extensions are variable: .fa, .fasta, .fna, .faa (Click to expand)\n\n\n\n\n\n\n“Generic” extensions are .fasta and .fa (e.g: culex_assembly.fasta)\nAlso used are extensions that explicitly indicate whether sequences are nucleotides (.fna) or amino acids (.faa)\n\n\n\n\n\n\n\nYour Culex genome assembly FASTA\nYour reference genome files are in data/ref:\nls -lh data/ref\n-rw------- 1 jelmer PAS0471 547M Jan 22 12:34 GCF_016801865.2.fna\n-rw------- 1 jelmer PAS0471 123M Jan 22 12:34 GCF_016801865.2.gtf\nWhile we can easily open small to medium-size files in the editor pane, “visual editors” like that do not work well for larger files like these.\nA handy command to view text files of any size is less, which opens them up in a “pager” within your shell – you’ll see what that means if you try it with one of the assembly FASTA file:\nless data/ref/GCF_016801865.2.fna\n&gt;NC_068937.1 Culex pipiens pallens isolate TS chromosome 1, TS_CPP_V2, whole genome shotgun sequence\naagcccttttatggtcaaaaatatcgtttaacttgaatatttttccttaaaaaataaataaatttaagcaaacagctgag\ntagatgtcatctactcaaatctacccataagcacacccctgttcaatttttttttcagccataagggcgcctccagtcaa\nattttcatattgagaatttcaatacaattttttaagtcgtaggggcgcctccagtcaaattttcatattgagaatttcaa\ntacatttttttatgtcgtaggggcgcctccagtcaaattttcatattgagaatttcaatacattttttttaagtcgtagg\nggcgcctccagtcaaattttcatattgagaatttcaatacatttttttaagtcttaggggcgcctccagtcaaattttca\ntattgagaatttcaatacatttttttaagtcgtaggggcgcctccagtcaaattttcatattgagaattttaatacaatt\nttttaaatcctaggggcgccttcagacaaacttaatttaaaaaatatcgctcctcgacttggcgactttgcgactgactg\ncgacagcactaccttggaacactgaaatgtttggttgactttccagaaagagtgcatatgacttgaaaaaaaaagagcgc\nttcaaaattgagtcaagaaattggtgaaacttggtgcaagcccttttatggttaaaaatatcgtttaacttgaatatttt\ntccttaaaaaataaataaatttaagcaaacagctgagtagatgtcatctactcaaatctacccataagcacacccctgga\nCCTAATTCATGGAGGTGAATAGAGCATACGTAAATACAAAACTCATGACATTAGCCTGTAAGGATTGTGTaattaatgca\naaaatattgaTAGAATGAAAGATGCAAGTCccaaaaattttaagtaaatgaATAGTAATCATAAAGATAActgatgatga\n\n\n\n\n\n\nSide note: Lowercase vs. uppercase nucleotide letters? (Click to expand)\n\n\n\n\n\nAs you have probably noticed, nucleotide bases are typically typed in uppercase (A, C, G, T). What does the mixture of lowercase and uppercase bases in the Cx. pipiens assembly FASTA mean, then?\nLowercase bases are what is called “soft-masked”: they are repetitive sequences, and bioinformatics programs will treat them differently than non-repetitive sequences, which are in uppercase.\n\n\n\n\n\n\n\n9.2 The genome annotation GFF/GTF\n\nThe GFF/GTF format\nThe GTF and GFF formats are very similar tab-delimited tabular files that contain genome annotations, with:\n\nOne row for each annotated “genomic feature” (gene, exon, etc.)\nOne column for each piece of information about a feature, like its genomic coordinates\n\nSee the sample below, with an added header line (not normally present) with column names:\nseqname     source  feature start   end     score  strand  frame    attributes\nNC_000001   RefSeq  gene    11874   14409   .       +       .       gene_id \"DDX11L1\"; transcript_id \"\"; db_xref \"GeneID:100287102\"; db_xref \"HGNC:HGNC:37102\"; description \"DEAD/H-box helicase 11 like 1 (pseudogene)\"; gbkey \"Gene\"; gene \"DDX11L1\"; gene_biotype \"transcribed_pseudogene\"; pseudo \"true\"; \nNC_000001   RefSeq  exon    11874   12227   .       +       .       gene_id \"DDX11L1\"; transcript_id \"NR_046018.2\"; db_xref \"GeneID:100287102\"; gene \"DDX11L1\"; product \"DEAD/H-box helicase 11 like 1 (pseudogene)\"; pseudo \"true\"; \nSome details on the more important/interesting columns:\n\nseqname — Name of the chromosome, scaffold, or contig\nfeature — Name of the feature type, e.g. “gene”, “exon”, “intron”, “CDS”\nstart & end — Start & end position of the feature\nstrand — Whether the feature is on the + (forward) or - (reverse) strand\nattribute — A semicolon-separated list of tag-value pairs with additional information\n\n\n\n\nYour Culex GTF file\nFor our Cx. pipiens reference genome, we only have a GTF file.11 Take a look at it, again with less (but now with the -S option):\nless -S data/ref/GCF_016801865.2.gtf\n#gtf-version 2.2\n#!genome-build TS_CPP_V2\n#!genome-build-accession NCBI_Assembly:GCF_016801865.2\n#!annotation-source NCBI RefSeq GCF_016801865.2-RS_2022_12\nNC_068937.1     Gnomon  gene    2046    110808  .       +       .       gene_id \"LOC120427725\"; transcript_id \"\"; db_xref \"GeneID:120427725\"; description \"homeotic protein deformed\"; gbkey \"Gene\"; gene \"LOC120427725\"; gene_biotype \"protein_coding\"; \nNC_068937.1     Gnomon  transcript      2046    110808  .       +       .       gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gbkey \"mRNA\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 25 Proteins\"; product \"homeotic protein deformed, transcript variant X3\"; transcript_biotype \"mRNA\"; \nNC_068937.1     Gnomon  exon    2046    2531    .       +       .       gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 25 Proteins\"; product \"homeotic protein deformed, transcript variant X3\"; transcript_biotype \"mRNA\"; exon_number \"1\"; \nNC_068937.1     Gnomon  exon    52113   52136   .       +       .       gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 25 Proteins\"; product \"homeotic protein deformed, transcript variant X3\"; transcript_biotype \"mRNA\"; exon_number \"2\"; \nNC_068937.1     Gnomon  exon    70113   70962   .       +       .       gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 25 Proteins\"; product \"homeotic protein deformed, transcript variant X3\"; transcript_biotype \"mRNA\"; exon_number \"3\"; \nNC_068937.1     Gnomon  exon    105987  106087  .       +       .       gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 25 Proteins\"; product \"homeotic protein deformed, transcript variant X3\"; transcript_biotype \"mRNA\"; exon_number \"4\"; \nNC_068937.1     Gnomon  exon    106551  106734  .       +       .       gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 25 Proteins\"; product \"homeotic protein deformed, transcript variant X3\"; transcript_biotype \"mRNA\"; exon_number \"5\"; \n\n\n Exercise: Transcripts and genes\nThe GTF file is sorted: all entries from the first line of the table, until you again see “gene” in the third column, belong to the first gene. Can you make sense of all these entries for this gene, given what you know of gene structures? How many transcripts does this gene have?\n\n\n(Click to see some pointers)\n\n\nThe first gene (“LOC120427725”) has 3 transcripts.\nEach transcript has 6-7 exons, 5 CDSs, and a start and stop codon.\n\nBelow, I’ve printed all lines belonging to the first gene:\nNC_068937.1 Gnomon  gene    2046    110808  .   +   .   gene_id \"LOC120427725\"; transcript_id \"\"; db_xref \"GeneID:120427725\"; description \"homeotic protein deformed\"; gbkey \"Gene\"; gene \"LOC120427725\"; gene_biotype \"protein_coding\"; \nNC_068937.1 Gnomon  transcript  2046    110808  .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gbkey \"mRNA\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 25 Proteins\"; product \"homeotic protein deformed, transcript variant X3\"; transcript_biotype \"mRNA\"; \nNC_068937.1 Gnomon  exon    2046    2531    .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 25 Proteins\"; product \"homeotic protein deformed, transcript variant X3\"; transcript_biotype \"mRNA\"; exon_number \"1\"; \nNC_068937.1 Gnomon  exon    52113   52136   .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 25 Proteins\"; product \"homeotic protein deformed, transcript variant X3\"; transcript_biotype \"mRNA\"; exon_number \"2\"; \nNC_068937.1 Gnomon  exon    70113   70962   .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 25 Proteins\"; product \"homeotic protein deformed, transcript variant X3\"; transcript_biotype \"mRNA\"; exon_number \"3\"; \nNC_068937.1 Gnomon  exon    105987  106087  .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 25 Proteins\"; product \"homeotic protein deformed, transcript variant X3\"; transcript_biotype \"mRNA\"; exon_number \"4\"; \nNC_068937.1 Gnomon  exon    106551  106734  .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 25 Proteins\"; product \"homeotic protein deformed, transcript variant X3\"; transcript_biotype \"mRNA\"; exon_number \"5\"; \nNC_068937.1 Gnomon  exon    109296  109660  .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 25 Proteins\"; product \"homeotic protein deformed, transcript variant X3\"; transcript_biotype \"mRNA\"; exon_number \"6\"; \nNC_068937.1 Gnomon  exon    109726  110808  .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 25 Proteins\"; product \"homeotic protein deformed, transcript variant X3\"; transcript_biotype \"mRNA\"; exon_number \"7\"; \nNC_068937.1 Gnomon  CDS 70143   70962   .   +   0   gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_052563405.1\"; exon_number \"3\"; \nNC_068937.1 Gnomon  CDS 105987  106087  .   +   2   gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_052563405.1\"; exon_number \"4\"; \nNC_068937.1 Gnomon  CDS 106551  106734  .   +   0   gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_052563405.1\"; exon_number \"5\"; \nNC_068937.1 Gnomon  CDS 109296  109660  .   +   2   gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_052563405.1\"; exon_number \"6\"; \nNC_068937.1 Gnomon  CDS 109726  110025  .   +   0   gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_052563405.1\"; exon_number \"7\"; \nNC_068937.1 Gnomon  start_codon 70143   70145   .   +   0   gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_052563405.1\"; exon_number \"3\"; \nNC_068937.1 Gnomon  stop_codon  110026  110028  .   +   0   gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_052563405.1\"; exon_number \"7\"; \nNC_068937.1 Gnomon  transcript  5979    110808  .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_039592629.2\"; db_xref \"GeneID:120427725\"; gbkey \"mRNA\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 24 Proteins\"; product \"homeotic protein deformed, transcript variant X2\"; transcript_biotype \"mRNA\"; \nNC_068937.1 Gnomon  exon    5979    6083    .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_039592629.2\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 24 Proteins\"; product \"homeotic protein deformed, transcript variant X2\"; transcript_biotype \"mRNA\"; exon_number \"1\"; \nNC_068937.1 Gnomon  exon    52113   52136   .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_039592629.2\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 24 Proteins\"; product \"homeotic protein deformed, transcript variant X2\"; transcript_biotype \"mRNA\"; exon_number \"2\"; \nNC_068937.1 Gnomon  exon    70113   70962   .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_039592629.2\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 24 Proteins\"; product \"homeotic protein deformed, transcript variant X2\"; transcript_biotype \"mRNA\"; exon_number \"3\"; \nNC_068937.1 Gnomon  exon    105987  106087  .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_039592629.2\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 24 Proteins\"; product \"homeotic protein deformed, transcript variant X2\"; transcript_biotype \"mRNA\"; exon_number \"4\"; \nNC_068937.1 Gnomon  exon    106551  106734  .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_039592629.2\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 24 Proteins\"; product \"homeotic protein deformed, transcript variant X2\"; transcript_biotype \"mRNA\"; exon_number \"5\"; \nNC_068937.1 Gnomon  exon    109296  109660  .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_039592629.2\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 24 Proteins\"; product \"homeotic protein deformed, transcript variant X2\"; transcript_biotype \"mRNA\"; exon_number \"6\"; \nNC_068937.1 Gnomon  exon    109726  110808  .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_039592629.2\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 24 Proteins\"; product \"homeotic protein deformed, transcript variant X2\"; transcript_biotype \"mRNA\"; exon_number \"7\"; \nNC_068937.1 Gnomon  CDS 70143   70962   .   +   0   gene_id \"LOC120427725\"; transcript_id \"XM_039592629.2\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_039448563.1\"; exon_number \"3\"; \nNC_068937.1 Gnomon  CDS 105987  106087  .   +   2   gene_id \"LOC120427725\"; transcript_id \"XM_039592629.2\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_039448563.1\"; exon_number \"4\"; \nNC_068937.1 Gnomon  CDS 106551  106734  .   +   0   gene_id \"LOC120427725\"; transcript_id \"XM_039592629.2\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_039448563.1\"; exon_number \"5\"; \nNC_068937.1 Gnomon  CDS 109296  109660  .   +   2   gene_id \"LOC120427725\"; transcript_id \"XM_039592629.2\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_039448563.1\"; exon_number \"6\"; \nNC_068937.1 Gnomon  CDS 109726  110025  .   +   0   gene_id \"LOC120427725\"; transcript_id \"XM_039592629.2\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_039448563.1\"; exon_number \"7\"; \nNC_068937.1 Gnomon  start_codon 70143   70145   .   +   0   gene_id \"LOC120427725\"; transcript_id \"XM_039592629.2\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_039448563.1\"; exon_number \"3\"; \nNC_068937.1 Gnomon  stop_codon  110026  110028  .   +   0   gene_id \"LOC120427725\"; transcript_id \"XM_039592629.2\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_039448563.1\"; exon_number \"7\"; \nNC_068937.1 Gnomon  transcript  60854   110807  .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_039592628.2\"; db_xref \"GeneID:120427725\"; gbkey \"mRNA\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 24 Proteins\"; product \"homeotic protein deformed, transcript variant X1\"; transcript_biotype \"mRNA\"; \nNC_068937.1 Gnomon  exon    60854   61525   .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_039592628.2\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 24 Proteins\"; product \"homeotic protein deformed, transcript variant X1\"; transcript_biotype \"mRNA\"; exon_number \"1\"; \nNC_068937.1 Gnomon  exon    70113   70962   .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_039592628.2\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 24 Proteins\"; product \"homeotic protein deformed, transcript variant X1\"; transcript_biotype \"mRNA\"; exon_number \"2\"; \nNC_068937.1 Gnomon  exon    105987  106087  .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_039592628.2\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 24 Proteins\"; product \"homeotic protein deformed, transcript variant X1\"; transcript_biotype \"mRNA\"; exon_number \"3\"; \nNC_068937.1 Gnomon  exon    106551  106734  .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_039592628.2\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 24 Proteins\"; product \"homeotic protein deformed, transcript variant X1\"; transcript_biotype \"mRNA\"; exon_number \"4\"; \nNC_068937.1 Gnomon  exon    109296  109660  .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_039592628.2\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 24 Proteins\"; product \"homeotic protein deformed, transcript variant X1\"; transcript_biotype \"mRNA\"; exon_number \"5\"; \nNC_068937.1 Gnomon  exon    109726  110807  .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_039592628.2\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 24 Proteins\"; product \"homeotic protein deformed, transcript variant X1\"; transcript_biotype \"mRNA\"; exon_number \"6\"; \nNC_068937.1 Gnomon  CDS 70143   70962   .   +   0   gene_id \"LOC120427725\"; transcript_id \"XM_039592628.2\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_039448562.1\"; exon_number \"2\"; \nNC_068937.1 Gnomon  CDS 105987  106087  .   +   2   gene_id \"LOC120427725\"; transcript_id \"XM_039592628.2\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_039448562.1\"; exon_number \"3\"; \nNC_068937.1 Gnomon  CDS 106551  106734  .   +   0   gene_id \"LOC120427725\"; transcript_id \"XM_039592628.2\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_039448562.1\"; exon_number \"4\"; \nNC_068937.1 Gnomon  CDS 109296  109660  .   +   2   gene_id \"LOC120427725\"; transcript_id \"XM_039592628.2\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_039448562.1\"; exon_number \"5\"; \nNC_068937.1 Gnomon  CDS 109726  110025  .   +   0   gene_id \"LOC120427725\"; transcript_id \"XM_039592628.2\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_039448562.1\"; exon_number \"6\"; \nNC_068937.1 Gnomon  start_codon 70143   70145   .   +   0   gene_id \"LOC120427725\"; transcript_id \"XM_039592628.2\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_039448562.1\"; exon_number \"2\"; \nNC_068937.1 Gnomon  stop_codon  110026  110028  .   +   0   gene_id \"LOC120427725\"; transcript_id \"XM_039592628.2\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_039448562.1\"; exon_number \"6\";"
  },
  {
    "objectID": "week15/w15_1_nfcore.html#footnotes",
    "href": "week15/w15_1_nfcore.html#footnotes",
    "title": "Running an nf-core Nextflow pipeline",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n Conda environments can also be used, but containers are recommended, and are what we will use.↩︎\n Typically, not all outputs are copied, especially for intermediate steps like read trimming — and pipelines have settings to determine what is copied↩︎\n Somewhat oddly, there is no command-line option available for this: we have to use this environment variable, also when running the pipeline.↩︎\n The default logging does not work well the output goes to a text file, as it will in our case because we will submit the script with the Nextflow command as a Slurm batch job.↩︎\n And for a run of a full data set, you may want to ask even more, e.g. 12-24 hours.↩︎\n The default Nextflow logging (without -ansi-log false) does show when jobs finish, but this would result in very messy output in a Slurm log file.↩︎\n A lot of intronic content may indicate that you have a lot of pre-mRNA in your data; this is more common when your library prep used rRNA depletion instead of poly-A selection. A lot of intergenic content may indicate DNA contamination. Poor genome annotation quality may also contribute to a low percentage of exonic reads. The RSeQC &gt; Read Distribution plot will show this with even more categories, e.g. separately showing UTRs.↩︎\n Which happen to be the same here, but these are usually different.↩︎\n And each version in two formats: .rds (a binary R object file type) and .tsv.↩︎\nNote that because individual sequence entries are commonly spread across multiple lines, FASTA entries do not necessarily cover 2 lines (cf. FASTQ).↩︎\nA GFF file would contain the same information but in a slightly different format. For programs used in RNA-seq analysis, GTF files tend to be the preferred format.↩︎"
  },
  {
    "objectID": "week14/w14_overview.html",
    "href": "week14/w14_overview.html",
    "title": "Week 14: R + Markdown with Quarto",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "week13/w13_overview.html",
    "href": "week13/w13_overview.html",
    "title": "Week 13: R for omics data",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "week07/w7_overview.html#links",
    "href": "week07/w7_overview.html#links",
    "title": "Week 7: Slurm batch jobs at OSC",
    "section": "1 Links",
    "text": "1 Links\n\nLecture pages\n\nTuesday — Runner scripts and pipelines\nTuesday/Thursday — Managing batch jobs in multi-step analyses\n\n\n\nExercises & assignments\n\nExercises for this week"
  },
  {
    "objectID": "week07/w7_overview.html#content-overview",
    "href": "week07/w7_overview.html#content-overview",
    "title": "Week 7: Slurm batch jobs at OSC",
    "section": "2 Content overview",
    "text": "2 Content overview\n\nSome of the things you will learn this week\n\nGet a better understanding of the purpose and mechanics of “runner scripts”.\nWhat challenges you can run into when creating analysis pipelines with regular Bash scripts, including runner scripts.\nWhat “workflow management systems” like Snakemake and Nextflow are, and what the advantages of formal pipelines written with these are.\nWhat the nf-core initiative is, and how you can run one of their pipelines at OSC.\n\n\n\nRequired readings\n\nPerkel 2019, Nature “Toolbox” feature: Workflow systems turn raw data into scientific knowledge\nDi Tommaso et al. 2017, Nature Biotechnology: Nextflow enables reproducible computational workflows\nEwels et al. 2020, Nature Biotechnology: The nf-core framework for community-curated bioinformatics pipelines"
  },
  {
    "objectID": "week07/w7_exercises2.html#introduction-and-setting-up",
    "href": "week07/w7_exercises2.html#introduction-and-setting-up",
    "title": "Week 6 exercises",
    "section": "Introduction and setting up",
    "text": "Introduction and setting up\nThese exercises mainly serve to practice more with organizing your scripts according to the strategy of using:\n\nPrimary shell scripts that typically run a single bioinformatics tool for a single file or sample, and that accept arguments such as input and output dirs or files.\nA runner script that serves as an informal pipeline: it mainly contains code to run all the primary scripts, typically submitting those as batch jobs, and often using loops to do so.\n\nIn the exercises of the past two weeks, you created primary and runner scripts to run FastQC and TrimGalore, and have used these to pre-process the Garrigos et al. RNA-seq reads. Here, you’ll add a few steps to complete this RNA-seq analysis: aligning the reads to the reference genome, creating a gene count table, and summarizing QC metrics with MultiQC. As such, this is a much simpler alternative to the nf-core rnaseq workflow that we ran in class this week.\nAfter doing this, your runner script will more closely resemble something you might use in this course’s final project, or your research projects: it will contain more steps including consecutive ones, where the input of one step is the output of a previous step.\n\n\nSetting up\n\nCreate and move into a dir for these exercises:\n# You should be in /fs/ess/PAS2700/users/$USER\nmkdir week06/exercises\ncd week06/exercises\nCreate dirs for primary scripts and for a runner script, the create the runner script and open it:\nmkdir scripts run\ntouch run/run.sh\nCopy the primary scripts:\ncp -v /fs/ess/PAS2700/share/exercises/week6/scripts/* scripts/\n‘/fs/ess/PAS2700/share/exercises/week6/scripts/fastqc.sh’ -&gt; ‘scripts/fastqc.sh’\n‘/fs/ess/PAS2700/share/exercises/week6/scripts/featurecounts.sh’ -&gt; ‘scripts/featurecounts.sh’\n‘/fs/ess/PAS2700/share/exercises/week6/scripts/multiqc.sh’ -&gt; ‘scripts/multiqc.sh’\n‘/fs/ess/PAS2700/share/exercises/week6/scripts/star_align.sh’ -&gt; ‘scripts/star_align.sh’\n‘/fs/ess/PAS2700/share/exercises/week6/scripts/star_index.sh’ -&gt; ‘scripts/star_index.sh’\n‘/fs/ess/PAS2700/share/exercises/week6/scripts/trimgalore.sh’ -&gt; ‘scripts/trimgalore.sh’"
  },
  {
    "objectID": "week07/w7_exercises2.html#exercise-1-run-fastqc-and-trimgalore-again",
    "href": "week07/w7_exercises2.html#exercise-1-run-fastqc-and-trimgalore-again",
    "title": "Week 6 exercises",
    "section": "Exercise 1: Run FastQC and TrimGalore again",
    "text": "Exercise 1: Run FastQC and TrimGalore again\nA) Paste the following into the run/run.sh runner script, which includes:\n\nA short description of what the script does (modify the date, your name, and the path as needed)\nDefining inputs and the main output as variables (you’ll use the reference genome files in the later exercises)\nRunning FastQC and TrimGalore for all samples using for loops and batch jobs, with code very similar to what you used in last week’s exercises.\n\n# 2024-04-12, Jelmer Poelstra\n# - A runner script to run a simple RNA-seq analysis, creating a gene count table\n#   from RNA-seq reads and a reference genome assembly and annotation.\n# - The steps are read QC and trimming, alignment, and quantification.\n# - The working dir is /fs/ess/PAS2700/users/jelmer/week06/exercises.\n# - I am working on the OSC Pitzer cluster.\n\n# Step 0: setting up\n# A) Define the inputs\nfastq_dir=../../garrigos_data/fastq\nfasta=../../garrigos_data/ref/GCF_016801865.2.fna\ngtf=../../garrigos_data/ref/GCF_016801865.2.gtf\n# B) Define settings\nstrandedness=1       # 1 means forward-stranded (used with featureCounts)\n# C) Define the outputs\ncount_table=results/featurecounts/counts.tsv\n\n# Step 1: QC the reads with FastQC\nfor fastq_file in \"$fastq_dir\"/*fastq.gz; do\n    sbatch scripts/fastqc.sh \"$fastq_file\" results/fastqc\ndone\n\n# Step 2: Trim the reads with TrimGalore\nfor R1 in \"$fastq_dir\"/*_R1.fastq.gz; do\n    sbatch scripts/trimgalore.sh \"$R1\" results/trimgalore\ndone\nB) Take a look at the fastqc.sh and trimgalore.sh scripts and see if you understand everything — there are some additions relative to the scripts you wrote in the last two weeks, for example:\n\nOne best-practice addition is that these scripts print the version of the program that they run at the end: this will end up in the Slurm log file for our records.\nThe if statement that tests whether the correct number of arguments were passed to the script has expanded help text, including an example of how to run the script (and uses the $* variable, which is a string with all arguments passed to the script).\nInside the output dir, a logs dir that you can later move Slurm log files into.\nThe TrimGalore script renames the output files, since TrimGalore gives them unwieldy names that might trip us up in the next step.\n\n\n\nClick to see the code of the fastqc.sh script\n\n#!/bin/bash\n#SBATCH --account=PAS2700\n#SBATCH --mail-type=FAIL\n#SBATCH --output=slurm-fastqc-%j.out\nset -euo pipefail\n\n# Load the software\nmodule load miniconda3/24.1.2-py310\nconda activate /fs/ess/PAS0471/jelmer/conda/fastqc\n\n# Process the command-line arguments\nif [[ ! \"$#\" -eq 2 ]]; then\n    echo \"Error: You provided $# arguments, while 2 are required.\"\n    echo \"Usage: fastqc.sh &lt;FASTQ-file&gt; &lt;output-dir&gt;\"\n    echo \"Example: sbatch fastqc.sh data/fastq/A_R1.fastq.gz results/fastqc\"\n    echo \"Your arguments: $*\"\n    exit 1\nfi\nfastq_file=$1\noutdir=$2\n\n# Report start of script and variables\necho \"# Starting script fastqc.ch\"\ndate\necho \"# Input FASTQ file:   $fastq_file\"\necho \"# Output dir:         $outdir\"\necho\n\n# Create the output dir (with a subdir for Slurm logs)\nmkdir -p \"$outdir\"/logs\n\n# Run FastQC\nfastqc --outdir \"$outdir\" \"$fastq_file\"\n\n# Report software version, end of script, and date\necho\necho \"# Used FastQC version:\"\nfastqc --version\necho \"# Done with script fastqc.sh\"\ndate\necho\n\n\n\nClick to see the code of the trimgalore.sh script\n\n#!/bin/bash\n#SBATCH --account=PAS2700\n#SBATCH --cpus-per-task=8\n#SBATCH --mail-type=FAIL\n#SBATCH --output=slurm-trimgalore-%j.out\nset -euo pipefail\n\n# Load the software\nmodule load miniconda3/24.1.2-py310\nconda activate /fs/ess/PAS0471/jelmer/conda/trimgalore\n\n# Copy the placeholder variables\nif [[ ! \"$#\" -eq 2 ]]; then\n    echo \"Error: You provided $# arguments, while 2 are required.\"\n    echo \"Usage: trimgalore.sh &lt;R1-FASTQ&gt; &lt;outdir&gt;\"\n    echo \"Example: sbatch trimgalore.sh data/fastq/A_R1.fastq.gz results/trimgalore\"\n    echo \"Your arguments: $*\"\n    exit 1\nfi\nR1_in=$1\noutdir=$2\n\n# Infer the R2 FASTQ file name\nR2_in=${R1_in/_R1/_R2}\n\n# Report start of script and variables\necho \"# Starting script trimgalore.sh\"\ndate\necho \"# Input R1 FASTQ file:      $R1_in\"\necho \"# Input R2 FASTQ file:      $R2_in\"\necho \"# Output dir:               $outdir\"\necho\n\n# Create the output dir (with a subdir for Slurm logs)\nmkdir -p \"$outdir\"/logs\n\n# Run TrimGalore\ntrim_galore \\\n    --paired \\\n    --fastqc \\\n    --output_dir \"$outdir\" \\\n    --cores 8 \\\n    \"$R1_in\" \\\n    \"$R2_in\"\n\n# Rename the output files - TrimGalore's output file names are unwieldy\n# with two separate read direction indicators (_R1 and _1).\necho\necho \"# Renaming the output files:\"\nsample_id=$(basename \"$R1_in\" _R1.fastq.gz)\nR1_out_init=\"$outdir\"/\"$sample_id\"_R1_val_1.fq.gz # This will be the initial R1 output file \nR2_out_init=\"$outdir\"/\"$sample_id\"_R2_val_2.fq.gz # This will be the initial R2 output file \nR1_out_final=\"$outdir\"/\"$sample_id\"_R1.fastq.gz\nR2_out_final=\"$outdir\"/\"$sample_id\"_R2.fastq.gz\nmv -v \"$R1_out_init\" \"$R1_out_final\"\nmv -v \"$R2_out_init\" \"$R2_out_final\"\n\n# Report software version, end of script, and date\necho\necho \"# Used TrimGalore version:\"\ntrim_galore --version | grep version\necho \"# Done with script trimgalore.sh\"\ndate\n\nC) Run this code – can you submit both sets of batch jobs (FastQC and TrimGalore) at once, or not?\nD) Monitor the progress of the jobs, and when they are all done, check the output files and move the Slurm log files to the logs dir within the output dir."
  },
  {
    "objectID": "week07/w7_exercises2.html#exercise-2-align-reads-with-star",
    "href": "week07/w7_exercises2.html#exercise-2-align-reads-with-star",
    "title": "Week 6 exercises",
    "section": "Exercise 2: Align reads with STAR",
    "text": "Exercise 2: Align reads with STAR\nYou will align (map) the trimmed reads to the reference genome with the program STAR, which is an aligner specifically designed for RNA-seq data1.\n\nA: Index the reference genomes\nWhen aligning reads, the first step is nearly always to create an “index” of the reference genome2. The script scripts/star_index.sh that you already copied has the code to do this, and takes 3 arguments:\n\nThe reference genome assembly FASTA file\nThe reference genome annotation GTF file\nThe output dir for the index files\n\n\n\nClick to see the code of the star_index.sh script\n\n#!/bin/bash\n#SBATCH --account=PAS2700\n#SBATCH --cpus-per-task=16\n#SBATCH --mail-type=FAIL\n#SBATCH --output=slurm-star_index-%j.out\nset -euo pipefail\n\n# Load the software\nmodule load miniconda3/24.1.2-py310\nconda activate /fs/ess/PAS0471/jelmer/conda/star\n\n# Process the command-line arguments\nif [[ ! \"$#\" -eq 3 ]]; then\n    echo \"Error: You provided $# arguments, while 3 are required.\"\n    echo \"Usage: star_index.sh &lt;assembly-fasta&gt; &lt;annotation-gtf&gt; &lt;outdir&gt;\"\n    echo \"Example: sbatch star_index.sh data/ref/my.fna data/ref/my.gtf results/star/index\"\n    echo \"Your arguments: $*\"\n    exit 1\nfi\nfasta=$1\ngtf=$2\noutdir=$3\n\n# Make sure the nr of threads/cores/cpus is however many the job has\nthreads=$SLURM_CPUS_PER_TASK\n\n# Report start of script and variables\necho \"# Starting script star_index.sh\"\ndate\necho \"# Input assembly FASTA:           $fasta\"\necho \"# Input annotation GTF:           $gtf\"\necho \"# Output dir:                     $outdir\"\necho \"# Number of threads:              $threads\"\necho\n\n# Create the output dir (with a subdir for Slurm logs)\nmkdir -p \"$outdir\"/logs\n\n# Run STAR\n# (Note: keeping this command as simple as possible - if using STAR in your research,\n#  then look into the --sjdbOverhang and --genomeSAindexNbases options as well.)\nSTAR \\\n    --runMode genomeGenerate \\\n    --genomeFastaFiles \"$fasta\" \\\n    --sjdbGTFfile \"$gtf\" \\\n    --genomeDir \"$outdir\" \\\n    --runThreadN \"$threads\"\n\n# Explanation of some of the options used:\n# --runMode genomeGenerate  =&gt; Instead of aligning, the \"run mode\" is to generate a genome index\n\n# Report software version, end of script, and date\necho\necho \"# Used STAR version:\"\nSTAR --version\necho \"# Done with script star_index.sh\"\ndate\n\n Take a look at the scripts/star_index.sh script. In your runner script, write code to submit the script as a batch job, passing the appropriate arguments to the script. Then submit the job, monitor its progress, and when its done, check the output files and move the Slurm log file to to the logs dir within the output dir.\n\n\n\nB: Align the reads\nNow you can align the reads to the reference genome (index). The script scripts/star_align.sh does this for one sample (two FASTQ files, R1 and R2) at a time, and takes 4 arguments:\n\nAn R1 FASTQ file (trimmed; the output from trimgalore.sh)\nThe reference genome index dir (that star_index.sh created)\nThe reference genome annotation GTF file\nAn output dir (this can be the same for each sample)\n\nThe main output files are alignment files in the BAM format.\n\n\nClick to see the code of the star_align.sh script\n\n#!/bin/bash\n#SBATCH --account=PAS2700\n#SBATCH --cpus-per-task=8\n#SBATCH --mail-type=FAIL\n#SBATCH --output=slurm-star_align-%j.out\nset -euo pipefail\n\n# Load the software\nmodule load miniconda3/24.1.2-py310\nconda activate /fs/ess/PAS0471/jelmer/conda/star\n\n# Process the command-line arguments\nif [[ ! \"$#\" -eq 4 ]]; then\n    echo \"Error: You provided $# arguments, while 4 are required.\"\n    echo \"Usage: star_align.sh &lt;R1-FASTQ&gt; &lt;genome-index-dir&gt; &lt;annotation-gtf&gt; &lt;outdir&gt;\"\n    echo \"Example: sbatch star_align.sh data/fastq/A_R1.fastq.gz results/star/index data/ref/my.gtf results/star\"\n    echo \"Your arguments: $*\"\n    exit 1\nfi\nR1_in=$1\nindex_dir=$2\ngtf=$3\noutdir=$4\n\n# Make sure the nr of threads/cores/cpus is however many the job has\nthreads=$SLURM_CPUS_PER_TASK\n\n# Infer the R2 FASTQ file name\nR2_in=${R1_in/_R1/_R2}\n\n# Infer the \"sample ID\" - we need this for the output file specification\nsample_id=$(basename \"$R1_in\" _R1.fastq.gz)\n\n# Report start of script and variables\necho \"# Starting script star_align.sh\"\ndate\necho \"# Input R1 FASTQ file:            $R1_in\"\necho \"# Input R2 FASTQ file:            $R2_in\"\necho \"# Input genome index:             $index_dir\"\necho \"# Input annotation GTF:           $gtf\"\necho \"# Output dir:                     $outdir\"\necho \"# Number of threads:              $threads\"\necho\n\n# Create the output dir (with a subdir for Slurm logs)\nmkdir -p \"$outdir\"/logs\n\n# Run STAR\n# (Note: keeping this command as simple as possible - if using STAR in your research,\n#  then look into, e.g., the --alignIntronMin, --alignIntronMax and --outFilterMultimapNmax\n#  options as well.)\nSTAR \\\n    --readFilesIn \"$R1_in\" \"$R2_in\" \\\n    --genomeDir \"$index_dir\" \\\n    --sjdbGTFfile \"$gtf\" \\\n    --runThreadN \"$threads\" \\\n    --readFilesCommand zcat \\\n    --outFileNamePrefix \"$outdir\"/\"$sample_id\"_ \\\n    --outSAMtype BAM SortedByCoordinate\n\n# Explanation of some of the options used:\n# --readFilesCommand zcat                     =&gt; Tell STAR that the FASTQ files are gzipped\n# --outSAMtype BAM SortedByCoordinate         =&gt; Request a sorted BAM file as output (instead of unsorted SAM)\n# --outFileNamePrefix \"$outdir\"/\"$sample_id\"_ =&gt; Specify not just an outdir but a \"sample ID\" prefix, otherwise\n#                                                the BAM file would have a generic file name that does not ID the file/sample \n\n# Report software version, end of script, and date\necho\necho \"# Used STAR version:\"\nSTAR --version\necho \"# Done with script star_align.sh\"\ndate\n\n Take a look at the scripts/star_align.sh script. In your runner script, write a for loop to submit the script as a separate batch job for each sample. Then submit the jobs, monitor their progress, and when they are all done, check the output files and move the Slurm log files to to the logs dir within the output dir."
  },
  {
    "objectID": "week07/w7_exercises2.html#exercise-3-quantify-gene-counts-with-featurecounts",
    "href": "week07/w7_exercises2.html#exercise-3-quantify-gene-counts-with-featurecounts",
    "title": "Week 6 exercises",
    "section": "Exercise 3: Quantify gene counts with featureCounts",
    "text": "Exercise 3: Quantify gene counts with featureCounts\nNext, you’ll create a gene count table with featureCounts (part of the “subread” program). The script scripts/featurecounts.sh only needs to be run once, and takes the following arguments:\n\nA dir with BAM files (featureCounts will use all BAM files in this dir in a single run)\nThe reference genome annotation GTF file.\nA number indicating the RNA-seq library strandedness (0 for unstranded, 1 for forward-stranded (our case), 2 for reverse-stranded — you should use the variable we defined at the top of the runner script).\nAn output file name for the count table, e.g. results/featurecounts/counts.tsv.\n\n\n\nClick to see the code of the featurecounts.sh script\n\n#!/bin/bash\n#SBATCH --account=PAS2700\n#SBATCH --cpus-per-task=8\n#SBATCH --mail-type=FAIL\n#SBATCH --output=slurm-featurecounts-%j.out\nset -euo pipefail\n\n# Load the software\nmodule load miniconda3/24.1.2-py310\nconda activate /fs/ess/PAS0471/jelmer/conda/subread\n\n# Process the command-line arguments\nif [[ ! \"$#\" -eq 4 ]]; then\n    echo \"Error: You provided $# arguments, while 4 are required.\"\n    echo \"Usage: featurecounts.sh &lt;input-dir&gt; &lt;annotation-gtf&gt; &lt;strandedness&gt; &lt;outfile&gt;\"\n    echo \"Example: sbatch featurecounts.sh results/star data/ref/my.gtf 2 results/featurecounts/counts.tsv\"\n    echo \"Your arguments: $*\"\n    exit 1\nfi\nindir=$1\ngtf=$2\nstrandedness=$3\noutfile=$4\n\n# Make sure the nr of threads/cores/cpus is however many the job has\nthreads=$SLURM_CPUS_PER_TASK\n\n# Report start of script and variables\necho \"# Starting script featurecounts.sh\"\ndate\necho \"# Input dir:                      $indir\"\necho \"# Input annotation GTF:           $gtf\"\necho \"# Strandedness:                   $strandedness\"\necho \"# Output file:                    $outfile\"\necho \"# Number of threads:              $threads\"\necho\n\n# Create the output dir (with a subdir for Slurm logs)\noutdir=$(dirname \"$outfile\")\nmkdir -p \"$outdir\"/logs\n\n# Run featureCounts\nfeatureCounts \\\n    -a \"$gtf\" \\\n    -o \"$outfile\" \\\n    -s \"$strandedness\" \\\n    -p \\\n    --countReadPairs \\\n    -C \\\n    -M \\\n    -T \"$threads\" \\\n    \"$indir\"/*bam\n\n# Explanation of some of the options used:\n# -s 2              =&gt; Reverse-stranded library like TruSeq\n# -p                =&gt; paired-end reads\n# --countReadPairs  =&gt; Count read pairs, not reads\n# -C                =&gt; Don't count pairs with discordant mates\n# -M                =&gt; Count multi-mapping reads\n\n# Report software version, end of script, and date\necho\necho \"# Used featureCounts/subread version:\"\nfeatureCounts -v\necho \"# Done with script featurecounts.sh\"\ndate\n\n Take a look at the scripts/featurecounts.sh script. In your runner script, add code to submit the featureCounts script as a batch job. Then submit the job, monitor its progress, and when it’s done, check the output files and move the Slurm log file to the logs dir within the output dir."
  },
  {
    "objectID": "week07/w7_exercises2.html#exercise-4-summarize-qc-metrics-with-multiqc",
    "href": "week07/w7_exercises2.html#exercise-4-summarize-qc-metrics-with-multiqc",
    "title": "Week 6 exercises",
    "section": "Exercise 4: Summarize QC metrics with MultiQC",
    "text": "Exercise 4: Summarize QC metrics with MultiQC\nFinally, you’ll create a report with QC metrics using MultiQC. The scripts/multiqc.sh script will run MultiQC and takes two arguments:\n\nAn input dir, which should simply be the results dir: MultiQC will recursively search this dir for log files that it can recognize, and include these in its report.\nAn output dir for the MultiQC report.\n\n\n\nClick to see the code of the multiqc.sh script\n\n#!/bin/bash\n#SBATCH --account=PAS2700\n#SBATCH --mail-type=FAIL\n#SBATCH --output=slurm-multiqc-%j.out\nset -euo pipefail\n\n# Load the software\nmodule load miniconda3/24.1.2-py310\nconda activate /fs/ess/PAS0471/jelmer/conda/multiqc\n\n# Process the command-line arguments\nif [[ ! \"$#\" -eq 2 ]]; then\n    echo \"Error: You provided $# arguments, while 2 are required.\"\n    echo \"Usage: multiqc.sh &lt;input-dir&gt; &lt;output-dir&gt;\"\n    echo \"Example: sbatch multiqc.sh results/ results/multiqc\"\n    echo \"Your arguments: $*\"\n    exit 1\nfi\nindir=$1\noutdir=$2\n\n# Report start of script and variables\necho \"# Starting script multiqc.sh\"\ndate\necho \"# Input dir:                      $indir\"\necho \"# Output file:                    $outdir\"\necho\n\n# Create the output dir (with a subdir for Slurm logs)\nmkdir -p \"$outdir\"/logs\n\n# Run MultiQC\nmultiqc \\\n    --outdir \"$outdir\" \\\n    --interactive \\\n    --force \\\n    \"$indir\"\n\n# Explanation of some of the options used:\n# --interactive     =&gt; Always use interactive plots\n# --force           =&gt; Overwrite existing MultiQC reports in output dir\n\n# Report software version, end of script, and date\necho\necho \"# Used MultiQC version:\"\nmultiqc --version\necho \"# Done with script multiqc.sh\"\ndate\n\n Take a look at the scripts/multiqc.sh script. In your runner script, add code to submit the MultiQC script as a batch job. Then submit the job, monitor its progress, and when it’s done, check the output files and move the Slurm log file to to the logs dir within the output dir. Make sure also to download the MultiQC report to your computer, opening it, and taking a look at the metrics that are reported.\n\n\n\n\n\n\n\nFastQC results in the MultiQC report\n\n\n\nOne thing to realize in the MultiQC report is that unfortunately, the pre- and post-trimming FastQC results are all presented together (but they can be distinguished based on the file names, with _val in trimmed file names). If you needed to take a closer look at this for your own research, a work-around would be to run MultiQC separately on the two FastQC dirs.\n\n\n\n\n\n\n\n\n\nTake another look at your runner script\n\n\n\nNow that you’re done, take a close look at your runner script and think about whether this structure makes sense you to, and how you could do something similar in your final project and your own research."
  },
  {
    "objectID": "week07/w7_exercises2.html#solutions",
    "href": "week07/w7_exercises2.html#solutions",
    "title": "Week 6 exercises",
    "section": "Solutions",
    "text": "Solutions\n\nExercise 1\n\n\nC) solution (click to expand)\n\nYes, you can run these two steps at the same time: both use the raw FASTQ files as the input, so the steps don’t depend on each other.\n\n\n\nD) solution 1: check the queue (click to expand)\n\n\nCheck the Slurm queue:\nsqueue -u $USER -l\n# [Output not shown]\n\n\n\n\nD) solution 2: Check the FastQC output (click to expand)\n\n# Take a close look at 1 or 2 log files with 'less':\nless slurm-fastqc*\n# [Showing one output file by means of example]\n# Starting script fastqc.ch\nFri Apr 12 09:34:40 EDT 2024\n# Input FASTQ file:   ../../garrigos_data/fastq/ERR10802863_R1.fastq.gz\n# Output dir:         results/fastqc\n\napplication/gzip\nStarted analysis of ERR10802863_R1.fastq.gz\nApprox 5% complete for ERR10802863_R1.fastq.gz\nApprox 10% complete for ERR10802863_R1.fastq.gz\nApprox 15% complete for ERR10802863_R1.fastq.gz\nApprox 20% complete for ERR10802863_R1.fastq.gz\nApprox 25% complete for ERR10802863_R1.fastq.gz\nApprox 30% complete for ERR10802863_R1.fastq.gz\nApprox 35% complete for ERR10802863_R1.fastq.gz\nApprox 40% complete for ERR10802863_R1.fastq.gz\nApprox 45% complete for ERR10802863_R1.fastq.gz\nApprox 50% complete for ERR10802863_R1.fastq.gz\nApprox 55% complete for ERR10802863_R1.fastq.gz\nApprox 60% complete for ERR10802863_R1.fastq.gz\nApprox 65% complete for ERR10802863_R1.fastq.gz\nApprox 70% complete for ERR10802863_R1.fastq.gz\nApprox 75% complete for ERR10802863_R1.fastq.gz\nApprox 80% complete for ERR10802863_R1.fastq.gz\nApprox 85% complete for ERR10802863_R1.fastq.gz\nApprox 90% complete for ERR10802863_R1.fastq.gz\nApprox 95% complete for ERR10802863_R1.fastq.gz\nApprox 100% complete for ERR10802863_R1.fastq.gz\nAnalysis complete for ERR10802863_R1.fastq.gz\n  \n# Used FastQC version:\nFastQC v0.12.1\n# Done with script fastqc.sh\nFri Apr 12 09:34:47 EDT 2024\n# Optional: check the last lines of each Slurm log file\n# (And/or check your email for failed job messages)\ntail slurm-fastqc*\n# [Output not shown, each file should end with \"Done with script\" line and the date]\n# Move the Slurm log files to an output dir:\nmv slurm-fastqc* results/fastqc/logs/\n# Check the main FastQC output files:\nls -lh results/fastqc\ntotal 48M\n-rw-r--r-- 1 jelmer PAS0471 718K Apr 12 09:34 ERR10802863_R1_fastqc.html\n-rw-r--r-- 1 jelmer PAS0471 364K Apr 12 09:34 ERR10802863_R1_fastqc.zip\n-rw-r--r-- 1 jelmer PAS0471 688K Apr 12 09:34 ERR10802863_R2_fastqc.html\n-rw-r--r-- 1 jelmer PAS0471 363K Apr 12 09:34 ERR10802863_R2_fastqc.zip\n-rw-r--r-- 1 jelmer PAS0471 714K Apr 12 09:34 ERR10802864_R1_fastqc.html\n-rw-r--r-- 1 jelmer PAS0471 366K Apr 12 09:34 ERR10802864_R1_fastqc.zip\n-rw-r--r-- 1 jelmer PAS0471 695K Apr 12 09:34 ERR10802864_R2_fastqc.html\n-rw-r--r-- 1 jelmer PAS0471 351K Apr 12 09:34 ERR10802864_R2_fastqc.zip\n-rw-r--r-- 1 jelmer PAS0471 713K Apr 12 09:34 ERR10802865_R1_fastqc.html\n-rw-r--r-- 1 jelmer PAS0471 367K Apr 12 09:34 ERR10802865_R1_fastqc.zip\n# [...output truncated...]\n\n\n\nD) solution 3: Check the TrimGalore output (click to expand)\n\n# Take a close look at 1 or 2 log files with 'less':\nless slurm-trimgalore*\n# [Showing the top of one output file by means of example]\n# Starting script trimgalore.sh\nFri Apr 12 09:35:18 EDT 2024\n# Input R1 FASTQ file:      ../../garrigos_data/fastq/ERR10802863_R1.fastq.gz\n# Input R2 FASTQ file:      ../../garrigos_data/fastq/ERR10802863_R2.fastq.gz\n# Output dir:               results/trimgalore\n\nPath to Cutadapt set as: 'cutadapt' (default)\n# [...output truncated...]\n# Check the main TrimGalore output files:\nls -lh results/trimgalore\ntotal 951M\n-rw-r--r-- 1 jelmer PAS0471  20M Apr 12 09:35 ERR10802863_R1.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 2.4K Apr 12 09:35 ERR10802863_R1.fastq.gz_trimming_report.txt\n-rw-r--r-- 1 jelmer PAS0471 674K Apr 12 09:35 ERR10802863_R1_val_1_fastqc.html\n-rw-r--r-- 1 jelmer PAS0471 349K Apr 12 09:35 ERR10802863_R1_val_1_fastqc.zip\n-rw-r--r-- 1 jelmer PAS0471  21M Apr 12 09:35 ERR10802863_R2.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 2.4K Apr 12 09:35 ERR10802863_R2.fastq.gz_trimming_report.txt\n-rw-r--r-- 1 jelmer PAS0471 676K Apr 12 09:35 ERR10802863_R2_val_2_fastqc.html\n-rw-r--r-- 1 jelmer PAS0471 341K Apr 12 09:35 ERR10802863_R2_val_2_fastqc.zip\n# [...output truncated...]\n\n\n\n\nExercise 2\n\n\nRunner script solution: index (click to expand)\n\n# Step 3: Align the reads with STAR\n# A) Index the reference genomes\nsbatch scripts/star_index.sh \"$fasta\" \"$gtf\" results/star/index\n\n\n\nRunner script solution: align (click to expand)\n\n# B) Align the reads to the index\nfor R1 in results/trimgalore/*R1.fastq.gz; do\n    sbatch scripts/star_align.sh \"$R1\" results/star/index \"$gtf\" results/star\ndone\n\n\n\nIndexing Slurm log (click to expand)\n\n# Starting script star_index.sh\nFri Apr 12 10:37:44 EDT 2024\n# Input assembly FASTA:           ../../garrigos_data/ref/GCF_016801865.2.fna\n# Input annotation GTF:           ../../garrigos_data/ref/GCF_016801865.2.gtf\n# Output dir:                     results/star/index\n# Number of threads:              16\n\n        /fs/ess/PAS0471/jelmer/conda/star/bin/STAR-avx2 --runMode genomeGenerate --genomeFastaFiles ../../garrigos_data/ref/GCF_016801865.2.fna --sjdbGTFfile ../../garrigos_data/ref/GCF_016801865.2.gtf --genomeDir results/star/index --runThreadN 16\n        STAR version: 2.7.11b   compiled: 2024-03-19T08:38:59+0000 :/opt/conda/conda-bld/star_1710837244939/work/source\nApr 12 10:37:44 ..... started STAR run\nApr 12 10:37:44 ... starting to generate Genome files\nApr 12 10:37:51 ..... processing annotations GTF\n!!!!! WARNING: --genomeSAindexNbases 14 is too large for the genome size=566354144, which may cause seg-fault at the mapping step. Re-run genome generation with recommended --genomeSAindexNbases 13\nApr 12 10:37:55 ... starting to sort Suffix Array. This may take a long time...\nApr 12 10:37:58 ... sorting Suffix Array chunks and saving them to disk...\nApr 12 10:39:06 ... loading chunks from disk, packing SA...\nApr 12 10:39:16 ... finished generating suffix array\nApr 12 10:39:16 ... generating Suffix Array index\nApr 12 10:40:36 ... completed Suffix Array index\nApr 12 10:40:36 ..... inserting junctions into the genome indices\nApr 12 10:41:05 ... writing Genome to disk ...\nApr 12 10:41:06 ... writing Suffix Array to disk ...\nApr 12 10:41:07 ... writing SAindex to disk\nApr 12 10:41:08 ..... finished successfully\n\n# Used STAR version:\n2.7.11b\n# Done with script star_index.sh\nFri Apr 12 10:41:08 EDT 2024\n\n\n\nExample alignment Slurm log (click to expand)\n\n# Starting script star_align.sh\nFri Apr 12 10:41:45 EDT 2024\n# Input R1 FASTQ file:            results/trimgalore/ERR10802863_R1.fastq.gz\n# Input R2 FASTQ file:            results/trimgalore/ERR10802863_R2.fastq.gz\n# Input genome index:             results/star/index\n# Input annotation GTF:           ../../garrigos_data/ref/GCF_016801865.2.gtf\n# Output dir:                     results/star\n# Number of threads:              8\n\n        /fs/ess/PAS0471/jelmer/conda/star/bin/STAR-avx2 --readFilesIn results/trimgalore/ERR10802863_R1.fastq.gz results/trimgalore/ERR10802863_R2.fastq.gz --genomeDir results/star/index --sjdbGTFfile ../../garrigos_data/ref/GCF_016801865.2.gtf --runThreadN 8 --readFilesCommand zcat --outFileNamePrefix results/star/ERR10802863_ --outSAMtype BAM SortedByCoordinate\n        STAR version: 2.7.11b   compiled: 2024-03-19T08:38:59+0000 :/opt/conda/conda-bld/star_1710837244939/work/source\nApr 12 10:41:45 ..... started STAR run\nApr 12 10:41:45 ..... loading genome\nApr 12 10:41:48 ..... processing annotations GTF\nApr 12 10:41:50 ..... inserting junctions into the genome indices\nApr 12 10:42:06 ..... started mapping\nApr 12 10:42:21 ..... finished mapping\nApr 12 10:42:21 ..... started sorting BAM\nApr 12 10:42:22 ..... finished successfully\n\n# Used STAR version:\n2.7.11b\n# Done with script star_align.sh\nFri Apr 12 10:42:22 EDT 2024\n\n\n\n\nExercise 3\n\n\nRunner script solution (click to expand)\n\n# Step 4: Create a gene count table with featureCounts\nsbatch scripts/featurecounts.sh results/star \"$gtf\" \"$strandedness\" \"$count_table\"\n\n\n\nfeatureCounts Slurm log (click to expand)\n\n# Starting script featurecounts.sh\nFri Apr 12 10:45:05 EDT 2024\n# Input dir:                      results/star\n# Input annotation GTF:           ../../garrigos_data/ref/GCF_016801865.2.gtf\n# Strandedness:                   1\n# Output file:                    results/featurecounts/counts.tsv\n# Number of threads:              8\n\n\n        ==========     _____ _    _ ____  _____  ______          _____  \n        =====         / ____| |  | |  _ \\|  __ \\|  ____|   /\\   |  __ \\ \n          =====      | (___ | |  | | |_) | |__) | |__     /  \\  | |  | |\n            ====      \\___ \\| |  | |  _ &lt;|  _  /|  __|   / /\\ \\ | |  | |\n              ====    ____) | |__| | |_) | | \\ \\| |____ / ____ \\| |__| |\n        ==========   |_____/ \\____/|____/|_|  \\_\\______/_/    \\_\\_____/\n      v2.0.6\n\n//========================== featureCounts setting ===========================\\\\\n||                                                                            ||\n||             Input files : 22 BAM files                                     ||\n||                                                                            ||\n||                           ERR10802863_Aligned.sortedByCoord.out.bam        ||\n||                           ERR10802864_Aligned.sortedByCoord.out.bam        ||\n||                           ERR10802865_Aligned.sortedByCoord.out.bam        ||\n||                           ERR10802866_Aligned.sortedByCoord.out.bam        ||\n||                           ERR10802867_Aligned.sortedByCoord.out.bam        ||\n||                           ERR10802868_Aligned.sortedByCoord.out.bam        ||\n||                           ERR10802869_Aligned.sortedByCoord.out.bam        ||\n||                           ERR10802870_Aligned.sortedByCoord.out.bam        ||\n||                           ERR10802871_Aligned.sortedByCoord.out.bam        ||\n||                           ERR10802874_Aligned.sortedByCoord.out.bam        ||\n||                           ERR10802875_Aligned.sortedByCoord.out.bam        ||\n||                           ERR10802876_Aligned.sortedByCoord.out.bam        ||\n||                           ERR10802877_Aligned.sortedByCoord.out.bam        ||\n||                           ERR10802878_Aligned.sortedByCoord.out.bam        ||\n||                           ERR10802879_Aligned.sortedByCoord.out.bam        ||\n||                           ERR10802880_Aligned.sortedByCoord.out.bam        ||\n||                           ERR10802881_Aligned.sortedByCoord.out.bam        ||\n||                           ERR10802882_Aligned.sortedByCoord.out.bam        ||\n||                           ERR10802883_Aligned.sortedByCoord.out.bam        ||\n||                           ERR10802884_Aligned.sortedByCoord.out.bam        ||\n||                           ERR10802885_Aligned.sortedByCoord.out.bam        ||\n||                           ERR10802886_Aligned.sortedByCoord.out.bam        ||\n||                                                                            ||\n||             Output file : counts.tsv                                       ||\n||                 Summary : counts.tsv.summary                               ||\n||              Paired-end : yes                                              ||\n||        Count read pairs : yes                                              ||\n||              Annotation : GCF_016801865.2.gtf (GTF)                        ||\n||      Dir for temp files : results/featurecounts                            ||\n||                                                                            ||\n||                 Threads : 8                                                ||\n||                   Level : meta-feature level                               ||\n||      Multimapping reads : counted                                          ||\n|| Multi-overlapping reads : not counted                                      ||\n||   Min overlapping bases : 1                                                ||\n||                                                                            ||\n\\\\============================================================================//\n\n//================================= Running ==================================\\\\\n||                                                                            ||\n|| Load annotation file GCF_016801865.2.gtf ...                               ||\n||    Features : 166177                                                       ||\n||    Meta-features : 18855                                                   ||\n||    Chromosomes/contigs : 162                                               ||\n||                                                                            ||\n|| Process BAM file ERR10802863_Aligned.sortedByCoord.out.bam...              ||\n||    Strand specific : stranded                                              ||\n||    Paired-end reads are included.                                          ||\n||    Total alignments : 434318                                               ||\n||    Successfully assigned alignments : 401890 (92.5%)                       ||\n||    Running time : 0.01 minutes                                             ||\n||                                                                            ||\n|| Process BAM file ERR10802864_Aligned.sortedByCoord.out.bam...              ||\n||    Strand specific : stranded                                              ||\n||    Paired-end reads are included.                                          ||\n||    Total alignments : 453285                                               ||\n||    Successfully assigned alignments : 420396 (92.7%)                       ||\n||    Running time : 0.01 minutes                                             ||\n||                                                                            ||\n|| Process BAM file ERR10802865_Aligned.sortedByCoord.out.bam...              ||\n||    Strand specific : stranded                                              ||\n||    Paired-end reads are included.                                          ||\n||    Total alignments : 450789                                               ||\n||    Successfully assigned alignments : 409131 (90.8%)                       ||\n||    Running time : 0.01 minutes                                             ||\n||                                                                            ||\n|| Process BAM file ERR10802866_Aligned.sortedByCoord.out.bam...              ||\n||    Strand specific : stranded                                              ||\n||    Paired-end reads are included.                                          ||\n||    Total alignments : 453068                                               ||\n||    Successfully assigned alignments : 420634 (92.8%)                       ||\n||    Running time : 0.01 minutes                                             ||\n||                                                                            ||\n|| Process BAM file ERR10802867_Aligned.sortedByCoord.out.bam...              ||\n||    Strand specific : stranded                                              ||\n||    Paired-end reads are included.                                          ||\n||    Total alignments : 469293                                               ||\n||    Successfully assigned alignments : 438336 (93.4%)                       ||\n||    Running time : 0.01 minutes                                             ||\n||                                                                            ||\n|| Process BAM file ERR10802868_Aligned.sortedByCoord.out.bam...              ||\n||    Strand specific : stranded                                              ||\n||    Paired-end reads are included.                                          ||\n||    Total alignments : 453389                                               ||\n||    Successfully assigned alignments : 410776 (90.6%)                       ||\n||    Running time : 0.01 minutes                                             ||\n||                                                                            ||\n|| Process BAM file ERR10802869_Aligned.sortedByCoord.out.bam...              ||\n||    Strand specific : stranded                                              ||\n||    Paired-end reads are included.                                          ||\n||    Total alignments : 446533                                               ||\n||    Successfully assigned alignments : 413794 (92.7%)                       ||\n||    Running time : 0.01 minutes                                             ||\n||                                                                            ||\n|| Process BAM file ERR10802870_Aligned.sortedByCoord.out.bam...              ||\n||    Strand specific : stranded                                              ||\n||    Paired-end reads are included.                                          ||\n||    Total alignments : 456614                                               ||\n||    Successfully assigned alignments : 423864 (92.8%)                       ||\n||    Running time : 0.01 minutes                                             ||\n||                                                                            ||\n|| Process BAM file ERR10802871_Aligned.sortedByCoord.out.bam...              ||\n||    Strand specific : stranded                                              ||\n||    Paired-end reads are included.                                          ||\n||    Total alignments : 442307                                               ||\n||    Successfully assigned alignments : 403651 (91.3%)                       ||\n||    Running time : 0.01 minutes                                             ||\n||                                                                            ||\n|| Process BAM file ERR10802874_Aligned.sortedByCoord.out.bam...              ||\n||    Strand specific : stranded                                              ||\n||    Paired-end reads are included.                                          ||\n||    Total alignments : 441439                                               ||\n||    Successfully assigned alignments : 396114 (89.7%)                       ||\n||    Running time : 0.01 minutes                                             ||\n||                                                                            ||\n|| Process BAM file ERR10802875_Aligned.sortedByCoord.out.bam...              ||\n||    Strand specific : stranded                                              ||\n||    Paired-end reads are included.                                          ||\n||    Total alignments : 397679                                               ||\n||    Successfully assigned alignments : 372151 (93.6%)                       ||\n||    Running time : 0.01 minutes                                             ||\n||                                                                            ||\n|| Process BAM file ERR10802876_Aligned.sortedByCoord.out.bam...              ||\n||    Strand specific : stranded                                              ||\n||    Paired-end reads are included.                                          ||\n||    Total alignments : 445342                                               ||\n||    Successfully assigned alignments : 414676 (93.1%)                       ||\n||    Running time : 0.01 minutes                                             ||\n||                                                                            ||\n|| Process BAM file ERR10802877_Aligned.sortedByCoord.out.bam...              ||\n||    Strand specific : stranded                                              ||\n||    Paired-end reads are included.                                          ||\n||    Total alignments : 356283                                               ||\n||    Successfully assigned alignments : 332904 (93.4%)                       ||\n||    Running time : 0.01 minutes                                             ||\n||                                                                            ||\n|| Process BAM file ERR10802878_Aligned.sortedByCoord.out.bam...              ||\n||    Strand specific : stranded                                              ||\n||    Paired-end reads are included.                                          ||\n||    Total alignments : 403889                                               ||\n||    Successfully assigned alignments : 379018 (93.8%)                       ||\n||    Running time : 0.01 minutes                                             ||\n||                                                                            ||\n|| Process BAM file ERR10802879_Aligned.sortedByCoord.out.bam...              ||\n||    Strand specific : stranded                                              ||\n||    Paired-end reads are included.                                          ||\n||    Total alignments : 399408                                               ||\n||    Successfully assigned alignments : 373530 (93.5%)                       ||\n||    Running time : 0.01 minutes                                             ||\n||                                                                            ||\n|| Process BAM file ERR10802880_Aligned.sortedByCoord.out.bam...              ||\n||    Strand specific : stranded                                              ||\n||    Paired-end reads are included.                                          ||\n||    Total alignments : 447356                                               ||\n||    Successfully assigned alignments : 413845 (92.5%)                       ||\n||    Running time : 0.01 minutes                                             ||\n||                                                                            ||\n|| Process BAM file ERR10802881_Aligned.sortedByCoord.out.bam...              ||\n||    Strand specific : stranded                                              ||\n||    Paired-end reads are included.                                          ||\n||    Total alignments : 388963                                               ||\n||    Successfully assigned alignments : 364413 (93.7%)                       ||\n||    Running time : 0.01 minutes                                             ||\n||                                                                            ||\n|| Process BAM file ERR10802882_Aligned.sortedByCoord.out.bam...              ||\n||    Strand specific : stranded                                              ||\n||    Paired-end reads are included.                                          ||\n||    Total alignments : 432376                                               ||\n||    Successfully assigned alignments : 397751 (92.0%)                       ||\n||    Running time : 0.01 minutes                                             ||\n||                                                                            ||\n|| Process BAM file ERR10802883_Aligned.sortedByCoord.out.bam...              ||\n||    Strand specific : stranded                                              ||\n||    Paired-end reads are included.                                          ||\n||    Total alignments : 431581                                               ||\n||    Successfully assigned alignments : 392518 (90.9%)                       ||\n||    Running time : 0.01 minutes                                             ||\n||                                                                            ||\n|| Process BAM file ERR10802884_Aligned.sortedByCoord.out.bam...              ||\n||    Strand specific : stranded                                              ||\n||    Paired-end reads are included.                                          ||\n||    Total alignments : 396408                                               ||\n||    Successfully assigned alignments : 371702 (93.8%)                       ||\n||    Running time : 0.01 minutes                                             ||\n||                                                                            ||\n|| Process BAM file ERR10802885_Aligned.sortedByCoord.out.bam...              ||\n||    Strand specific : stranded                                              ||\n||    Paired-end reads are included.                                          ||\n||    Total alignments : 411164                                               ||\n||    Successfully assigned alignments : 378424 (92.0%)                       ||\n||    Running time : 0.01 minutes                                             ||\n||                                                                            ||\n|| Process BAM file ERR10802886_Aligned.sortedByCoord.out.bam...              ||\n||    Strand specific : stranded                                              ||\n||    Paired-end reads are included.                                          ||\n||    Total alignments : 444531                                               ||\n||    Successfully assigned alignments : 416370 (93.7%)                       ||\n||    Running time : 0.01 minutes                                             ||\n||                                                                            ||\n|| Write the final count table.                                               ||\n|| Write the read assignment summary.                                         ||\n||                                                                            ||\n|| Summary of counting results can be found in file \"results/featurecounts/c  ||\n|| ounts.tsv.summary\"                                                         ||\n||                                                                            ||\n\\\\============================================================================//\n\n\n# Used featureCounts/Subread version:\n\nfeatureCounts v2.0.6\n\n# Done with script featurecounts.sh\nFri Apr 12 10:45:21 EDT 2024\n\n\n\n\nExercise 4\n\n\nRunner script solution (click to expand)\n\n# Step 5: QC summaries with MultiQC\nsbatch scripts/multiqc.sh results results/multiqc\n\n\n\nMultiQC Slurm log file (click to expand)\n\n# Starting script multiqc.sh\nFri Apr 12 11:43:45 EDT 2024\n# Input dir:                      results\n# Output file:                    results/multiqc\n\n\n  /// MultiQC 🔍 | v1.17\n\n|           multiqc | Search path : /fs/ess/PAS0471/jelmer/teach/courses/pracs-sp24/week6/exercises/results\n|         searching | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 635/635  \n|    feature_counts | Found 22 reports\n|              star | Found 22 reports\n|          cutadapt | Found 44 reports\n|            fastqc | Found 88 reports\n|           multiqc | Report      : results/multiqc/multiqc_report.html\n|           multiqc | Data        : results/multiqc/multiqc_data\n|           multiqc | MultiQC complete\n\n# Used MultiQC version:\nmultiqc, version 1.17\n# Done with script multiqc.sh\nFri Apr 12 11:44:30 EDT 2024\n\nHere is a copy of the MultiQC report that I got.\n\n\n\nComplete final runner script\n\n\nComplete final runner script (click to expand)\n\n#!/bin/bash\n\n# 2024-04-12, Jelmer Poelstra\n# - A runner script to run a simple RNA-seq analysis, creating a gene count table\n#   from RNA-seq reads and a reference genome assembly and annotation.\n# - The steps are read QC and trimming, alignment, and quantification.\n# - The working dir is /fs/ess/PAS2700/users/jelmer/week06/exercises.\n# - I am working on the OSC Pitzer cluster.\n\n# Step 0: setting up\n# A) Define the inputs\nfastq_dir=../../garrigos_data/fastq\nfasta=../../garrigos_data/ref/GCF_016801865.2.fna\ngtf=../../garrigos_data/ref/GCF_016801865.2.gtf\n# B) Define settings\nstrandedness=1\n# C) Define the outputs\ncount_table=results/featurecounts/counts.tsv\n\n# Step 1: QC the reads with FastQC\nfor fastq_file in \"$fastq_dir\"/*fastq.gz; do\n    sbatch scripts/fastqc.sh \"$fastq_file\" results/fastqc\ndone\n  \n# Step 2: Trim the reads with TrimGalore\nfor R1 in \"$fastq_dir\"/*_R1.fastq.gz; do\n    sbatch scripts/trimgalore.sh \"$R1\" results/trimgalore\ndone\n\n# Step 3: Align the reads with STAR\n# A) Index the reference genomes\nsbatch scripts/star_index.sh \"$fasta\" \"$gtf\" results/star/index\n# B) Align the reads to the index\nfor R1 in results/trimgalore/*R1.fastq.gz; do\n    sbatch scripts/star_align.sh \"$R1\" results/star/index \"$gtf\" results/star\ndone\n\n# Step 4: Create a gene count table with featureCounts\nsbatch scripts/featurecounts.sh results/star \"$gtf\" \"$strandedness\" \"$count_table\"\n\n# Step 5: QC summaries with MultiQC\nsbatch scripts/multiqc.sh results results/multiqc"
  },
  {
    "objectID": "week07/w7_exercises2.html#footnotes",
    "href": "week07/w7_exercises2.html#footnotes",
    "title": "Week 6 exercises",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n RNA-seq read alignment needs to be “splice-aware”: when the reads are mapped to the genome, introns are included in the reference but are (mostly) not present in the reads, as the reads are mostly mature mRNA with the introns spliced out. Therefore, a read may align as several disjunct fragments across introns!↩︎\nThe aligner will then align the reads to this index rather than the genome itself, which is much more efficient. These indices are aligner-specific, and can even differ among versions of the same aligner.↩︎"
  },
  {
    "objectID": "week07/w7_exercises.html#exercise-0-lecture-page-exercises",
    "href": "week07/w7_exercises.html#exercise-0-lecture-page-exercises",
    "title": "Week 7 exercises",
    "section": "Exercise 0: Lecture page exercises",
    "text": "Exercise 0: Lecture page exercises\nDo any exercises from the lecture pages that we did not get to in class."
  },
  {
    "objectID": "week07/w7_exercises.html#exercise-1-create-a-fastqc-conda-environment",
    "href": "week07/w7_exercises.html#exercise-1-create-a-fastqc-conda-environment",
    "title": "Week 7 exercises",
    "section": "Exercise 1: Create a FastQC Conda environment",
    "text": "Exercise 1: Create a FastQC Conda environment\nThe version of FastQC on OSC (0.11.8) is not the latest one (0.12.1). Let’s assume that you really need the latest version for your analysis:\n\nCreate a Conda environment for FastQC and install FastQC version 0.12.1 into it.\nActivate the environment and check that you have the correct version of FastQC."
  },
  {
    "objectID": "week07/w7_exercises.html#exercise-2-fastqc-with-multiple-cores",
    "href": "week07/w7_exercises.html#exercise-2-fastqc-with-multiple-cores",
    "title": "Week 7 exercises",
    "section": "Exercise 2: FastQC with multiple cores",
    "text": "Exercise 2: FastQC with multiple cores\nAs a starting point, copy the FastQC shell script from the last exercise on the Slurm batch job page:\n# After this, your script will be at 'scripts/fastqc.sh'\ncp -v ../class_slurm/scripts/fastqc.sh scripts\n\n\nIf you don’t have it, here’s the contents of the starting FastQC script (Click to expand)\n\n#!/bin/bash\n#SBATCH --account=PAS2700\n#SBATCH --output=slurm-fastqc-%j.out\n#SBATCH --mail-type=END,FAIL\n\nset -euo pipefail\n\n# Load the OSC module for FastQC\nmodule load fastqc\n\n# Copy the placeholder variables\nfastq_file=$1\noutdir=$2\n\n# Initial reporting\necho \"# Starting script fastqc.ch\"\ndate\necho \"# Input FASTQ file:   $fastq_file\"\necho \"# Output dir:         $outdir\"\necho\n\n# Create the output dir if needed\nmkdir -p \"$outdir\"\n\n# Run FastQC\nfastqc --outdir=\"$outdir\" \"$fastq_file\"\n\n# Final reporting\necho\necho \"# Done with script fastqc.sh\"\ndate\n\n\nUse your brand new Conda FastQC environment in the script instead of OSC’s module.\nChange the #SBATCH options in the script so that Slurm only sends you an email if the job fails.\nChange the #SBATCH options in the script so that your batch job will reserve 8 cores.\nChange the FastQC command in the script so that FastQC will use all 8 reserved cores. (Run fastqc --help to find the relevant option.)\nSubmit the script as a batch job with input FASTQ file ERR10802863_R1.fastq.gz as before.\nYou’ll be running FastQC for all files next, so let’s remove this Slurm log file and all FastQC output files."
  },
  {
    "objectID": "week07/w7_exercises.html#exercise-3-fastqc-batch-jobs-in-a-loop",
    "href": "week07/w7_exercises.html#exercise-3-fastqc-batch-jobs-in-a-loop",
    "title": "Week 7 exercises",
    "section": "Exercise 3: FastQC batch jobs in a loop",
    "text": "Exercise 3: FastQC batch jobs in a loop\n\nLoop over all Garrigos et al. FASTQ files, submitting your Ex. 2 FastQC script as a batch job for each file.\nCheck the Slurm queue immediately after running the loop, and keep checking it every couple of seconds until all your FastQC jobs have disappeared from the list (i.e., are done).\nYou now arguably have too many Slurm log files to go through, so what to do next?\n\nThe Slurm email feature should help: check that you didn’t get any emails about failed jobs.\nAnother quick check is to run the tail command with a glob (using *) so it will print the last lines of each of the output files. Try this and scroll through the tail output: it should be pretty easy to spot any files that don’t end the way the should.\nCheck the FastQC output files as well.\n\nIt can be good to keep your Slurm log files, but things will soon get very messy if you keep them all in your working dir: create a dir called logs in the FastQC output dir, and move all Slurm log files into that dir."
  },
  {
    "objectID": "week07/w7_exercises.html#exercise-4-trimgalore-batch-jobs-in-a-loop",
    "href": "week07/w7_exercises.html#exercise-4-trimgalore-batch-jobs-in-a-loop",
    "title": "Week 7 exercises",
    "section": "Exercise 4: TrimGalore batch jobs in a loop",
    "text": "Exercise 4: TrimGalore batch jobs in a loop\nAs a starting point, take your TrimGalore shell script from last week’s exercises, and save it as scripts/trimgalore.sh.\n\n\nIf you don’t have it, here’s the contents of the starting TrimGalore script (Click to expand)\n\n#!/bin/bash\nset -euo pipefail\n\n# Load TrimGalore\nmodule load miniconda3/23.3.1-py310\nconda activate /fs/ess/PAS0471/jelmer/conda/trimgalore\n\n# Copy the placeholder variables\nR1_in=$1\noutdir=$2\n\n# Infer the R2 FASTQ file name\nR2_in=${R1_in/_R1/_R2}\n\n# Report\necho \"# Starting script trimgalore.sh\"\ndate\necho \"# Input R1 FASTQ file:      $R1_in\"\necho \"# Input R2 FASTQ file:      $R2_in\"\necho \"# Output dir:               $outdir\"\necho\n\n# Create the output dir\nmkdir -p \"$outdir\"\n\n# Run TrimGalore\ntrim_galore \\\n    --paired \\\n    --fastqc \\\n    --output_dir \"$outdir\" \\\n    \"$R1_in\" \\\n    \"$R2_in\"\n\n# Report\necho\necho \"# Done with script trimgalore.sh\"\ndate\n\n\nThe script currently uses one of my Conda environments. Switch to your own that you created in class this week.\nAdd #SBATCH options to the top of the TrimGalore shell script to specify:\n\nThe account/project\nThe number of cores (8)\nThe Slurm log file name\nSomething to make Slurm email you upon job failure\n\nUse the TrimGalore option --cores to make it use all 8 reserved cores.\nSubmit the script as a batch job for sample ERR10802863, and check that everything went well. Then remove all outputs (Slurm log files and TrimGalore output files) from this test-run.\nRun TrimGalore for all FASTQ file pairs from the Garrigos et al. data, and check that everything went well.\nMove the Slurm log files into a dir logs in the TrimGalore output dir.\n\n\n\n\n\n\n\n\nSaving time\n\n\n\nCompared to running TrimGalore last week, you now saved time in two ways:\n\nYou used 8 cores, decreasing the running time — for example, for the first sample, from nearly a minute to 17 seconds.\nYou ran TrimGalore simultaneously instead of consecutively for each sample. If Slurm/OSC started these jobs quickly, as is usual, then running TrimGalore on all files should have cost well under a minute instead of over 20 minutes."
  },
  {
    "objectID": "week07/w7_exercises.html#exercise-5-create-a-nextflow-conda-environment",
    "href": "week07/w7_exercises.html#exercise-5-create-a-nextflow-conda-environment",
    "title": "Week 7 exercises",
    "section": "Exercise 5: Create a Nextflow Conda environment",
    "text": "Exercise 5: Create a Nextflow Conda environment\nNext week, we’ll work with Nextflow and the affiliated community project nf-core. Create a single Conda environment with both Nextflow and the nf-core tools."
  },
  {
    "objectID": "week07/w7_exercises.html#solutions",
    "href": "week07/w7_exercises.html#solutions",
    "title": "Week 7 exercises",
    "section": "Solutions",
    "text": "Solutions\n\nExercise 1\n\n\n1 - Create a FastQC Conda environment\n\nWhen you don’t specify the version, the latest one should be installed — at least when you’re using a “fresh” environment (there could be complicating factors if you already had other software in the same environment):\nmodule load miniconda3/23.3.1-py310\nconda create -y -n fastqc -c bioconda fastqc\nBut to be more explicit, you could replace the fastqc at the end of the line with fastqc=0.12.1.\n\n\n\n2 - Activate the environment and check the version\n\nconda activate fastqc\nfastqc --version\nFastQC v0.12.1\n\n\n\n\nExercise 2\n\n\n1 - Use your Conda environment\n\nReplace the line module load fastqc with:\nmodule load miniconda3/23.3.1-py310\nconda activate fastqc\n\n\n\n2 - Only email upon failure\n\n#SBATCH --mail-type=FAIL\n\n\n\n3 - Reserve 8 cores\n\n#SBATCH --cpus-per-task=8\n\n\n\n4 - Make FastQC use 8 cores\n\nThe relevant FastQC option is --threads (recall to consider the terms threads, cores, and CPUs as synonyms in this context):\nfastqc --threads 8 --outdir \"$outdir\" \"$fastq_file\"\n\n\n\n5 - Submit the script\n\nfastq_file=../../garrigos_data/fastq/ERR10802863_R1.fastq.gz\nsbatch scripts/fastqc.sh \"$fastq_file\" results/fastqc\nSubmitted batch job 12431988\n\n\n\n6 - Clean up\n\nrm slurm-fastqc*\nrm results/fastqc/*\n\n\n\n\nExercise 3\n\n\n1 - Submit a FastQC batch job for each FASTQ file\n\nfor fastq_file in ../../garrigos_data/fastq/*fastq.gz; do\n   sbatch scripts/fastqc.sh \"$fastq_file\" results/fastqc\ndone\nSubmitted batch job 27902555\nSubmitted batch job 27902556\nSubmitted batch job 27902557\nSubmitted batch job 27902558\nSubmitted batch job 27902559\nSubmitted batch job 27902560\nSubmitted batch job 27902561\n[...output truncated...]\n\n\n\n2 - Check the Slurm queue\n\nsqueue -u $USER -l\n\n\n\n3 - Check the output\n\nOnce all jobs have at least started running, you should see a lot of Slurm log files in your working dir:\nls\nresults                    slurm-fastqc-27902531.out  slurm-fastqc-27902543.out  slurm-fastqc-27902555.out\nrun                        slurm-fastqc-27902532.out  slurm-fastqc-27902544.out  slurm-fastqc-27902556.out\nscripts                    slurm-fastqc-27902533.out  slurm-fastqc-27902545.out  slurm-fastqc-27902557.out\nslurm-fastqc-27902522.out  slurm-fastqc-27902534.out  slurm-fastqc-27902546.out  slurm-fastqc-27902558.out\nslurm-fastqc-27902523.out  slurm-fastqc-27902535.out  slurm-fastqc-27902547.out  slurm-fastqc-27902559.out\nslurm-fastqc-27902524.out  slurm-fastqc-27902536.out  slurm-fastqc-27902548.out  slurm-fastqc-27902560.out\nslurm-fastqc-27902525.out  slurm-fastqc-27902537.out  slurm-fastqc-27902549.out  slurm-fastqc-27902561.out\nslurm-fastqc-27902526.out  slurm-fastqc-27902538.out  slurm-fastqc-27902550.out  slurm-fastqc-27902562.out\nslurm-fastqc-27902527.out  slurm-fastqc-27902539.out  slurm-fastqc-27902551.out  slurm-fastqc-27902563.out\nslurm-fastqc-27902528.out  slurm-fastqc-27902540.out  slurm-fastqc-27902552.out  slurm-fastqc-27902564.out\nslurm-fastqc-27902529.out  slurm-fastqc-27902541.out  slurm-fastqc-27902553.out  slurm-fastqc-27902565.out\nslurm-fastqc-27902530.out  slurm-fastqc-27902542.out  slurm-fastqc-27902554.out\nHere is how you can see the last lines of each of these files with tail:\ntail slurm-fastqc*\n==&gt; slurm-fastqc-27902563.out &lt;==\nApprox 80% complete for ERR10802885_R2.fastq.gz\nApprox 85% complete for ERR10802885_R2.fastq.gz\nApprox 90% complete for ERR10802885_R2.fastq.gz\nApprox 95% complete for ERR10802885_R2.fastq.gz\nApprox 100% complete for ERR10802885_R2.fastq.gz\nAnalysis complete for ERR10802885_R2.fastq.gz\n\n# Done with script fastqc.sh\nSun Mar 31 16:30:54 EDT 2024\n\n==&gt; slurm-fastqc-27902564.out &lt;==\nApprox 80% complete for ERR10802886_R1.fastq.gz\nApprox 85% complete for ERR10802886_R1.fastq.gz\nApprox 90% complete for ERR10802886_R1.fastq.gz\nApprox 95% complete for ERR10802886_R1.fastq.gz\nApprox 100% complete for ERR10802886_R1.fastq.gz\nAnalysis complete for ERR10802886_R1.fastq.gz\n\n# Done with script fastqc.sh\nSun Mar 31 16:30:54 EDT 2024\n\n==&gt; slurm-fastqc-27902565.out &lt;==\nApprox 80% complete for ERR10802886_R2.fastq.gz\nApprox 85% complete for ERR10802886_R2.fastq.gz\nApprox 90% complete for ERR10802886_R2.fastq.gz\nApprox 95% complete for ERR10802886_R2.fastq.gz\nApprox 100% complete for ERR10802886_R2.fastq.gz\nAnalysis complete for ERR10802886_R2.fastq.gz\n\n# Done with script fastqc.sh\nSun Mar 31 16:30:54 EDT 2024\n\n# [...output truncated...]\nFinally, to check the FastQC output files:\nls -lh results/fastqc\ntotal 48M\n-rw-rw----+ 1 poelstra PAS0471 718K Mar 31 16:29 ERR10802863_R1_fastqc.html\n-rw-rw----+ 1 poelstra PAS0471 364K Mar 31 16:29 ERR10802863_R1_fastqc.zip\n-rw-rw----+ 1 poelstra PAS0471 688K Mar 31 16:29 ERR10802863_R2_fastqc.html\n-rw-rw----+ 1 poelstra PAS0471 363K Mar 31 16:29 ERR10802863_R2_fastqc.zip\n-rw-rw----+ 1 poelstra PAS0471 714K Mar 31 16:29 ERR10802864_R1_fastqc.html\n-rw-rw----+ 1 poelstra PAS0471 366K Mar 31 16:29 ERR10802864_R1_fastqc.zip\n-rw-rw----+ 1 poelstra PAS0471 695K Mar 31 16:29 ERR10802864_R2_fastqc.html\n-rw-rw----+ 1 poelstra PAS0471 351K Mar 31 16:29 ERR10802864_R2_fastqc.zip\n-rw-rw----+ 1 poelstra PAS0471 713K Mar 31 16:29 ERR10802865_R1_fastqc.html\n-rw-rw----+ 1 poelstra PAS0471 367K Mar 31 16:29 ERR10802865_R1_fastqc.zip\n-rw-rw----+ 1 poelstra PAS0471 698K Mar 31 16:29 ERR10802865_R2_fastqc.html\n-rw-rw----+ 1 poelstra PAS0471 358K Mar 31 16:29 ERR10802865_R2_fastqc.zip\n-rw-rw----+ 1 poelstra PAS0471 718K Mar 31 16:29 ERR10802866_R1_fastqc.html\n# [...output truncated...]\n\n\n\n4 - Clean the Slurm log files\n\nmkdir results/fastqc/logs\nmv slurm-fastqc* results/fastqc/logs\n\n\n\n\nExercise 4\n\n\n1 - Switch the Conda environment\n\nReplace the line conda activate /fs/ess/PAS0471/jelmer/conda/trimgalore with:\nconda activate trim-galore-0.6.10\n\n\n\n2 - #SBATCH options\n\n#SBATCH --account=PAS2700\n#SBATCH --cpus-per-task=8\n#SBATCH --output=slurm-trimgalore-%j.out\n#SBATCH --mail-type=FAIL\n\n\n\n3 - Make TrimGalore use the reserved 8 cores\n\nSimply add --cores 8 to your TrimGalore command.\n\n\n\n4 - Submit the script once, then clean up\n\nfastq_file=../../garrigos_data/fastq/ERR10802863_R1.fastq.gz\nsbatch scripts/trimgalore.sh \"$fastq_file\" results/trimgalore\nTo check that everything went well, look at the Slurm log files and the TrimGalore output files in results/trimgalore.\nTo remove the outputs from this test:\nrm slurm-trimgalore*\nrm results/trimgalore/*\n\n\n\n5 - Submit the script for all samples\n\nfor fastq_R1 in ../../garrigos_data/fastq/*R1.fastq.gz; do\n   sbatch scripts/trimgalore.sh \"$fastq_R1\" results/trimgalore\ndone\nTo check that everything went well, check your email and look at the TrimGalore output files in results/trimgalore. You can also run tail slurm-trimgalore* to check the last lines of each Slurm log file.\n\n\n\n6 - Move the Slurm log files\n\nmkdir results/trimgalore/logs\nmv slurm-trimgalore* results/trimgalore/logs\n\n\n\n\nExercise 5\n\n\nCreate a Nextflow + nf-core Conda environment\n\nmodule load miniconda3/23.3.1-py310\nconda create -y -n nextflow-23.10 -c bioconda nextflow=23.10.1 nf-core=2.13.1\nCheck that it works:\nconda activate nextflow-23.10\nnextflow -v\nnextflow version 23.10.1.5891\nnf-core --version\n                                          ,--./,-.\n          ___     __   __   __   ___     /,-._.--~\\\n    |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n    | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                          `._,._,'\n\n    nf-core/tools version 2.13.1 - https://nf-co.re\n\nnf-core, version 2.13.1"
  },
  {
    "objectID": "week09/w9_exercises.html",
    "href": "week09/w9_exercises.html",
    "title": "Week 9 Exercises: Nextflow",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "week08/w8_1_genAI.html",
    "href": "week08/w8_1_genAI.html",
    "title": "Using generative AI when coding",
    "section": "",
    "text": "Overview\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "week08/w8_overview.html#content-overview",
    "href": "week08/w8_overview.html#content-overview",
    "title": "Week 8: Using generative AI when coding",
    "section": "1 Content overview",
    "text": "1 Content overview"
  },
  {
    "objectID": "week08/w8_overview.html#readings",
    "href": "week08/w8_overview.html#readings",
    "title": "Week 8: Using generative AI when coding",
    "section": "2 Readings",
    "text": "2 Readings"
  },
  {
    "objectID": "week06/w6_3_software.html#loading-software-at-osc-with-lmod-modules",
    "href": "week06/w6_3_software.html#loading-software-at-osc-with-lmod-modules",
    "title": "Using software at OSC",
    "section": "1 Loading software at OSC with Lmod modules",
    "text": "1 Loading software at OSC with Lmod modules\nOSC administrators manage software with the “Lmod” system of software modules. For us users, this means that even though a lot of software is installed, most of it can only be used after we explicitly load it. That may seem like a drag, but on the upside, this practice enables the use of different versions of the same software, and of mutually incompatible software on a single system.\nWe can load, unload, and search for available software modules using the module command and its sub-commands.\n\n1.1 Checking whether a program is available\nThe OSC website has a list of installed software. You can also search for available software in the shell using two subtly different module commands:\n\nmodule spider lists all installed modules.\nmodule avail lists modules that can be directly loaded given the current environment (i.e., taking into account which other software has been loaded).\n\nSimply running module spider or module avail spits out complete lists of installed/available programs — it is more useful to add a search term as an argument. Below, we’ll search for the Conda distribution “miniconda”:\nmodule spider miniconda\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  miniconda3:\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n     Versions:\n        miniconda3/4.10.3-py37\n        miniconda3/4.12.0-py38\n        miniconda3/4.12.0-py39\n        miniconda3/23.3.1-py310\n\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  For detailed information about a specific \"miniconda3\" module (including how to load the modules) use the module's full name.\n  For example:\n\n     $ module spider miniconda3/4.12.0-py39\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nmodule avail miniconda\n------------------------------------------------------------------------------------------------------ /apps/lmodfiles/Core -------------------------------------------------------------------------------------------------------\n   miniconda3/4.10.3-py37 (D)    miniconda3/4.12.0-py38    miniconda3/4.12.0-py39    miniconda3/23.3.1-py310\n\n  Where:\n   D:  Default Module\nAs stated at the bottom of the output below, the (D) in the module avail output above marks the default version of the program: this is the version of the program that will be loaded if we don’t specify a version ourselves (see examples below). The module spider command does not provide this information.\n\n\n\n\n\n\nBoth of these search commands are case-insensitive, but module load (below) is not\n\n\n\n\n\n\n\n\n\n1.2 Loading and unloading software\nAll other Lmod software functionality is also accessed using module commands. For instance, to make a program available to you, use the load command:\n# Load a module:\nmodule load miniconda3               # Load the default version\nmodule load miniconda3/23.3.1-py310  # Load a specific version\n\n\n\n\n\n\nIt can be good to specify the version even when you want the default (Click to expand)\n\n\n\n\n\nThis is especially true inside a shell script — when using the module load command, specifying a version would:\n\nEnsure that when you run the same script a year later, the same version would be used (assuming it hasn’t been removed) — otherwise, it’s possible a newer version would has been installed in the meantime, which might produce different results.\nMake it easy to see which version you used, which is something you typically want to know and report in your paper.\n\n\n\n\n\n\n\n\n\n\nModules do not remain loaded across separate shell sessions\n\n\n\nModule loading does not persist across shell sessions. Whenever you get a fresh shell session (including but not limited to after logging into OSC again), you will have to reload any modules you want to use!\n\n\n\nTo check which modules are loaded, use module list. Its output also includes automatically loaded modules — for example, after loading miniconda3/23.3.1-py310, it should list miniconda3 as the 9th entry2:\nmodule list\nCurrently Loaded Modules:\n  1) xalt/latest               3) intel/19.0.5     5) modules/sp2020     7) git/2.18.0              9) miniconda3/23.3.1-py310\n  2) gcc-compatibility/8.4.0   4) mvapich2/2.3.3   6) project/ondemand   8) app_code_server/4.8.3\nOccasionally, when you run into conflicting (mutually incompatible) modules, it can be useful to unload modules, which you can do with module unload or module purge:\nmodule unload miniconda3    # Unload a specific module\nmodule purge                # Unload all modules\n\n\n\n\n\n\n\nAlways include the module load command in your shell script\n\n\n\nWhen you run a program that is loaded with Lmod in your shell script, always include the module load command in the script, and it is best to do so way at the top of the script:\n#!/bin/bash\nset -euo pipefail\n\n# Load software\nmodule load fastqc/0.11.8\n\n\n\n\n Exercise: Load a BLAST module\nBLAST is a very widely used alignment tool, often used to identify sequences that are similar to a query sequence. There is not just a web version on NCBI’s website, but also a BLAST command-line tool.\n\nUse module avail to check if BLAST is installed at OSC, and if so, which versions. (Note: you’ll also see results for the related module blast-database — ignore those.)\nLoad the default BLAST version by not specifying a version, and then check which version was loaded and if that matches the module avail output.\nLoad the latest version of BLAST without unloading the earlier version first. What output do you get?\n\n\n\nClick here for the solutions\n\n\nCheck the BLAST modules:\nmodule avail BLAST\n---------------------------------------------------------------------------------- /apps/lmodfiles/Core -- --------------------------------------------------------------------------------\n   blast-database/2018-08 (D)    blast-database/2020-04    blast-database/2022-06    blast/2.8.0+         blast/2.11.0+\n   blast-database/2019-09        blast-database/2021-05    blast-database/2023-06    blast/2.10.0+ (D)    blast/2.13.0+\n\n  Where:\n   D:  Default Module\nLoad the default version:\nmodule load BLAST\nmodule list\nCurrently Loaded Modules:\n  1) xalt/latest               3) intel/19.0.5     5) modules/sp2020     7) git/2.18.0              9) miniconda3/23.3.1-py310\n  2) gcc-compatibility/8.4.0   4) mvapich2/2.3.3   6) project/ondemand   8) app_code_server/4.8.3  10) blast/2.10.0+\nblast/2.10.0+ was loaded, which matches what module avail claimed with its (D) marker for the default version.\nLoad the latest version:\nmodule load blast/2.13.0+\nThe following have been reloaded with a version change:\n  1) blast/2.10.0+ =&gt; blast/2.13.0+\nLmod detected that you tried to load a different version of a software that was already loaded, so it changes the version and tells you about it.\n\n\n\n\n Bonus exercise: STAR and module availability\n\nUse module spider to check which versions of STAR, an RNA-seq read alignment program, have been installed at OSC. Compare this output with that of module avail.\nTry to load the most recent version of STAR that module spider listed (this should fail).\nFollow the instructions in the error message to again try and load OSC’s most recent version of STAR.\nSearch the internet to see what the most recent version of STAR is.\n\n\n\nClick here for the solutions\n\n\nCheck the versions of STAR — it looks like 2.7.9a is installed but we can’t load it for some reason:\nmodule spider star\n--------------------------------------------------------------------------------------------------------------------------------   ------------------------------------------------------\n  star:\n--------------------------------------------------------------------------------------------------------------------------------   ------------------------------------------------------\n     Versions:\n        star/2.5.2a\n        star/2.7.9a\nmodule avail star\n---------------------------------------------------------------------------------- /apps/lmodfiles/Core    ----------------------------------------------------------------------------------\n   star/2.5.2a\nA first stubborn attempt to load the most recent one:\nmodule load star/2.7.9a\nLmod has detected the following error:  These module(s) exist but cannot be loaded as requested: \"star/2.7.9a\"\n   Try: \"module spider star/2.7.9a\" to see how to load the module(s).\nFollow the instructions in the above error message to try and load it again:\nmodule spider star/2.7.9a\n--------------------------------------------------------------------------------------------------------------------------------   ------------------------------------------------------\n  star: star/2.7.9a\n--------------------------------------------------------------------------------------------------------------------------------   ------------------------------------------------------\n\n    You will need to load all module(s) on any one of the lines below before the \"star/2.7.9a\" module is available to load.\n\n      gnu/10.3.0\nmodule load gnu/10.3.0\nLmod is automatically replacing \"intel/19.0.5\" with \"gnu/10.3.0\".\n\nThe following have been reloaded with a version change:\n  1) mvapich2/2.3.3 =&gt; mvapich2/2.3.6\nmodule load star/2.7.9a\nThe last command prints no output, which is generally good news, and indeed, it seems to have worked:\nSTAR --version\n2.7.9a\nMost recent version of STAR:\nAs of March 2024, it looks like that’s version 2.7.11b (https://github.com/alexdobin/STAR)."
  },
  {
    "objectID": "week06/w6_3_software.html#when-software-isnt-installed-at-osc",
    "href": "week06/w6_3_software.html#when-software-isnt-installed-at-osc",
    "title": "Using software at OSC",
    "section": "2 When software isn’t installed at OSC",
    "text": "2 When software isn’t installed at OSC\nIt’s not uncommon that software you need for your project is not installed at OSC, or that you need a more recent version of the software than what is available. In that case, the following two are generally your best options:\n\nConda, which creates software “environments” you can activate much like we did with Lmod modules.\nContainers, which are self-contained software environments that include operating systems, akin to mini virtual machines. Docker containers are most well-known, but OSC uses Apptainer (formerly known as Singularity).\n\nConda and containers are useful not only at OSC, where they bypass issues with dependencies and administrator privileges, but more generally for reproducible software environments. They also make it easy to access different versions of the same software, or use mutually incompatible software.\nIn this session, you will learn how to use Conda, and the self-study reading at the bottom of the page covers using containers.\n\n\n\n\n\n\n\nOther options to install software / get it installed\n\n\n\n\nSend an email to OSC Help. They might be able to help you with your installation, or in case of commonly used software, might be willing to perform a system-wide installation (that is, making it available through Lmod / module commands).\n“Manually” install the software, which in the best case involves downloading a directly functioning binary (executable), but more commonly requires you to “compile” (build) the program. This is sometimes straightforward but can also become extremely tricky, especially at OSC where you don’t have “administrator privileges”3 and will often have difficulties with “dependencies”4."
  },
  {
    "objectID": "week06/w6_3_software.html#conda-basics",
    "href": "week06/w6_3_software.html#conda-basics",
    "title": "Using software at OSC",
    "section": "3 Conda basics",
    "text": "3 Conda basics\nThe Conda software can create so-called environments in which you can install one or more software packages.\nAs you’ll learn below, as long as a program is available in one of the online Conda repositories (and this is nearly always the case for open-source bioinformatics programs), then installing it is quite straightforward, doesn’t require admin privileges, and is done with a procedure that is nearly identical regardless of the program you are installing.\nA Conda environment is “just” a directory that includes the executable (binary) files for the program(s) in question. I have a collection of Conda environments that anyone can use, and we can list these environments simply with ls:\nls /fs/ess/PAS0471/jelmer/conda\nabricate-1.0.1  clonalframeml        kraken2            picard                        salmon\nagat-0.9.1      codan-1.2            kraken-biom        pilon-1.24                    samtools\nalv             cogclassifier        krona              pkgs                          scoary\namrfinderplus   cutadapt             liftoff-1.6.3      plasmidfinder-2.1.6           seqkit\nantismash       deeploc              links-2.0.1        plink2                        seqtk\nariba-2.14.6    deeptmhmm            lissero            porechop                      shoot\nastral-5.7.8    deeptmhmm2           longstitch-1.0.3   prokka                        signalp-6.0\naswcli          diamond              mafft              pseudofinder                  sistr-1.1.1\nbactopia        dwgsim               maskrc-svg         purge_dups-1.2.6              smap\nbactopia3       eggnogmapper         mbar24             pycoqc-2.5.2                  smap_dev\nbactopia-dev    emboss               medaka-1.7.2       qiime2-2023.7                 smartdenovo-env\nbakta           entap-0.10.8         metaxa-2.2.3       qiime2-amplicon-2024.2        snippy-4.6.0\nbase            entrez-direct        methylpy           qualimap-env                  snpeff\nbbmap           evigene              minibusco          quast-5.0.2                   snp-sites-2.5.1\nbcftools        fastp                minimap2-2.24      quickmerge-env                soapdenovo-trans-1.0.4\nbedops          fastqc               mlst               racon-1.5.0                   sortmerna-env\nbedtools        fastq-dl             mlst_check         ragtag-2.1.0                  sourmash\nbioawk          fasttree-2.1.11      mobsuite           rascaf                        spades-3.15.5\nbioconvert      filtlong-env         multiqc            rcorrector-1.0.5              sra-tools\nbiopython       flye-2.9.1           mummer4            r-dartr                       star\nbit             fmlrc2-0.1.7         muscle             r-deseq                       subread-2.0.1\nblast           gcta                 nanolyse-1.2.1     recognizer-1.8.3              taxonkit\nbowtie1         geofetch             nanoplot           repeatmasker-4.1.2.p1         tgsgapcloser\nbowtie2         gffread-0.12.7       nanopolish-0.13.2  repeatmodeler-2.0.3           tracy-0.7.1\nbracken         gget                 ncbi-datasets      resfinder                     transabyss-2.0.1\nbraker2-env     gubbins              nextdenovo-env     resistomeanalyzer-2018.09.06  transdecoder-5.5.0\nbusco           hisat2               nextflow           rgi-5.2.1                     treetime\nbusco2          hmmer                nextflow-22.10     r-metabar                     trimgalore\nbusco3          interproscan-5.55    nf-core            rnaquast-2.2.1                trimmomatic-0.39\nbwa-0.7.17      iqtree               orna-2.0           roary-3.13                    trinity-2.13.2\ncabana          justorthologs-0.0.2  orthofinder        r-rnaseq                      unicycler\ncactus          kallisto-0.48.0      orthofisher        rsem-1.3.3                    virema\ncgmlst          kat-2.4.2            panaroo            rseqc-env                     virulencefinder\ncheckm-1.2.0    knsp-3.1             parsnp             r_tree                        wtdbg-2.5\nclinker         kofamscan            phylofisher        sabre-1.0\nThis is organized similarly to the Lmod modules in that there’s generally one separate environment for one program, and the environment is named after that program.\n(The naming of these environments is unfortunately not entirely consistent: many environments include the version number of the program, but others do not. For environments without version numbers, I try to have them contain the most recent version of a software5.)\n\n\n3.1 Activating Conda environments\nBefore you can activate Conda environments, you always need to load OSC’s Miniconda module first, and we will load the most recent one:\nmodule load miniconda3/23.3.1-py310\nAs mentioned above, these environments are (de)activated much like with the Lmod system. But while the term “load” is used for Lmod modules, the term “activate” is used for Conda environments — it means the same thing.\nAlso like Lmod, there is a main command (conda) and several sub-commands (deactivate, create, install, update). For example, to activate an environment:\nconda activate /fs/ess/PAS0471/jelmer/conda/multiqc\n(/fs/ess/PAS0471/jelmer/conda/multiqc) [jelmer@p0085 jelmer]$\n\n\n\n\n\n\nConda environment indicator!\n\n\n\nWhen you have an active Conda environment, its name is displayed in front of your prompt, as depicted above with (multiqc).\nBecause the MultiQC environment you just loaded is not your own, the full path to the environment is shown (making the prompt rather long…). But when you load your own environment, only the name will be shown, like so:\n(multiqc) [jelmer@p0085 jelmer]$\n\n\nAfter you have activated the MultiQC environment, you should be able to use the program. To test this, simply run the multiqc command with the --help option:\nmultiqc --help\n /// MultiQC 🔍 | v1.17\n                                                                                              \n Usage: multiqc [OPTIONS] [ANALYSIS DIRECTORY]\n \n MultiQC aggregates results from bioinformatics analyses across many samples into a    \n single report.                                                                        \n[...output truncated...]\n\nUnlike Lmod / module load, Conda will by default only keep a single environment active. Therefore, when you have one environment activate and then activate another. For example, after activating the TrimGalore environment, the MultiQC environment is no longer active:\nconda activate /fs/ess/PAS0471/jelmer/conda/trimgalore\n  \nmultiqc --help\nbash: multiqc: command not found...\n\n\n\n\n\n\n\nThe --stack option does enable you having multiple Conda environments active (Click to expand)\n\n\n\n\n\n\nActivate the TrimGalore environment, if it isn’t already active:\nconda activate /fs/ess/PAS0471/jelmer/conda/trimgalore\n“Stack” the MultiQC environment:\nconda activate --stack /fs/ess/PAS0471/jelmer/conda/multiqc\nCheck that you can use both programs — output not shown, but both should successfully print help info:\nmultiqc --help\n\ntrim_galore --help\n\n\n\n\n\n\n\n3.2 Lines to add to your shell script\nLike for Lmod modules, you’ll have to load Conda environments in every shell session that you want to use them — they don’t automatically reload.\nConda environments loaded in your interactive shell environment do “carry over” to the environment in which your script runs (even when you submit them to the Slurm queue with sbatch). However, it is good practice to always include the necessary code to load/activate programs in your shell scripts:\n#!/bin/bash\nset -euo pipefail\n\n# Load software\nmodule load miniconda3/23.3.1-py310\nconda activate /fs/ess/PAS0471/jelmer/conda/multiqc\n\n\n\n\n\n\n\nPerils of Conda environments inside scripts\n\n\n\nProblems can occur when you have a Conda environment active in your interactive shell while you submit a script as a batch job that activates a different environment. Therefore, it is generally a good idea not to have any Conda environments active in your interactive shell when submitting batch jobs6. To deactivate the currently active Conda environment, simply type conda deactivate without any arguments:\nconda deactivate"
  },
  {
    "objectID": "week06/w6_3_software.html#creating-your-own-conda-environments",
    "href": "week06/w6_3_software.html#creating-your-own-conda-environments",
    "title": "Using software at OSC",
    "section": "4 Creating your own Conda environments",
    "text": "4 Creating your own Conda environments\n\n4.1 One-time Conda configuration\nBefore you can create our own environments, you first have to do some one-time configuration7. The configuration will set the Conda “channels” (basically, software repositories) that we want to use when we install programs, including the relative priorities among channels (since one program may be available from multiple channels).\nWe can do this configuration with the config sub-command — run the following in your shell:\nconda config --add channels defaults     # Added first =&gt; lowest priority\nconda config --add channels bioconda\nconda config --add channels conda-forge  # Added last =&gt; highest priority\nLet’s check whether the configuration was successfully saved:\nconda config --get channels\n--add channels 'defaults'   # lowest priority\n--add channels 'bioconda'\n--add channels 'conda-forge'   # highest priority\n\n\n\n4.2 Example: Creating an environment for TrimGalore\nWe will now create a Conda environment with the program TrimGalore installed, which you used in last week’s exercises, and which does not have a system-wide installation at OSC. Here is the command to all at once create a new Conda environment and install TrimGalore into that environment:\n# [Don't run this - we'll modify this a bit below]\nconda create -y -n trim-galore -c bioconda trim-galore\nLet’s break that command down:\n\ncreate is the Conda sub-command to create a new environment.\nWhen adding -y, Conda will not ask us for confirmation to install.\nFollowing the -n option, you can specify the name you would like the environment to have: we used trim-galore. You can use whatever name you like for the environment, but a descriptive yet concise name is a good idea. For single-program environments, it makes sense to simply name it after the program.\nThe -c option is to specify a “channel” (repository) from which to install, here bioconda8.\nThe trim-galore argument at the end of the line simply tells Conda to install the package of that name.\n\n\nBy default, Conda will install the latest available version of a program. If you create an entirely new environment for a program, like we’re doing here, that default should always apply — but if you’re installing into an environment that already contains programs, it’s possible that due to compatibility issues, it will install a different version.\nIf you want to be explicit about the version you want to install, add the version number after = following the package name, and you may then also want to include that version number in the Conda environment’s name — try this:\nconda create -y -n trim-galore-0.6.10 -c bioconda trim-galore=0.6.10\nCollecting package metadata (current_repodata.json): done  \nSolving environment: done\n# [...truncated...]\nThere should be a lot of output, with many packages that are being downloaded (these are all “dependencies” of TrimGalore), but if it works, you should see this before you get your prompt back:\nDownloading and Extracting Packages\n\nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done                                                                                                                   \n#                        \n# To activate this environment, use                          \n#\n#     $ conda activate trim-galore-0.6.10                          \n#\n# To deactivate an active environment, use\n#\n#     $ conda deactivate\nNow, you should be able to activate the environment (using just its name – see the box below):\nconda activate trim-galore-0.6.10  \nLet’s test if we can run TrimGalore — note, the command is trim_galore:\ntrim_galore --help\n USAGE:\n\ntrim_galore [options] &lt;filename(s)&gt;\n\n-h/--help               Print this help message and exits.\n# [...truncated...]\n\n\n\n\n\n\n\nSpecifying the full path to the environment dir (Click to expand)\n\n\n\n\n\nYou may have noticed above that we merely gave the environment a name (trim-galore or trim-galore-0.6.10), and did not tell it where to put this environment. Similarly, we were able to activate the environment with just its name. Conda assigns a personal default directory for its environments, somewhere in your Home directory.\nYou can install environments in a different location with the -p (instead of -n) option — for example:\n# [Don't run this]\nconda create -y -p /fs/scratch/PAS2700/$USER/conda/trim-galore -c bioconda trim-galore\nAnd when you want to load someone else’s Conda environments, you’ll always have to specify the full path to environment’s dir, like you did when loading one of my Conda environments above.\n\n\n\n\n\n\n4.3 Finding Conda installation info online\nMinor variations on the conda create command above can be used to install almost any program for which a Conda package is available, which is the vast majority of open-source bioinformatics programs!\nHowever, you may wonder how you would know:\n\nWhether the program is available and what the name of its Conda package is\nWhich Conda channel we should use\nWhich versions are available\n\nTo find this out, my strategy is to simply Google the program name together with “conda”, e.g. “cutadapt conda” if I wanted to install the Cutadapt program. Let’s see that in action:\n\n\n\n\n\nClick on that first link (in my experience, it is always the first Google hit):\n\n\n\n\n\n\n\n\n4.4 Building the installation command from the online info\nYou can take the top of the two example installation commands as a template, here: conda install -c bioconda cutadapt. You may notice the install subcommand, which we haven’t yet seen. This would install Cutadapt into the currently activated Conda environment. Since our strategy here is to create a separate environment for each program, just installing a program into whatever environment is currently active is not a great idea.\nYou can use the install command with a new environment, but then you would first have to create an “empty” environment, and then run the install command. However, we saw above that we can do all of this in a single command. To build this create-plus-install command, all we need to do is replace install in the example command on the Conda website by create -y -n &lt;env-name&gt;. Then, our full command (without version specification) will be:\n# [Don't run this - example command]\nconda create -y -n cutadapt -c bioconda cutadapt\nTo see which version of the software will be installed by default, and to see which older versions are available:\n\n\n\n\n\nFor almost any other program, you can use the exact same procedure to find the Conda package and install it!\n\n\n\n\n\n\nMore Conda commands to manage your environments (Click to expand)\n\n\n\n\n\n\nRemove an environment entirely:\nconda env remove -n cutadapt\nList all your conda environments:\nconda env list\nList all packages (programs) installed in an environment — due to dependencies, this can be a long list, even if you only actively installed one program:\nconda list -p /fs/ess/PAS0471/jelmer/conda/multiqc\nExport a plain-text “YAML” file that contains the instructions to recreate your currently-active environment (useful for reproducibility!)\nconda env export &gt; my_env.yml\nAnd you can use the following to create a Conda environment from such a YAML file:\nconda env create -n my_env --force --file my_env.yml\n\n\n\n\n\n\n\n4.5 Organizing your Conda environments\nThere are two reasonable alternative way to organize your Conda environments:\n\nHave one environment per program (my preference)\n\nEasier to keep an overview of what you have installed\nNo need to reinstall the same program across different projects\nLess risk of running into problems with your environment due to mutually incompatible software and complicated dependency situations\n\nHave one environment per research project\n\nYou just need to activate that one environment when you’re working on your project.\nEasier when you need to share your entire project with someone else (or yourself) on a different (super)computer.\n\n\nEven though it might seem easier, a third alternative, to simply install all programs across all projects in one single environment, is not recommended. This doesn’t benefit reproducibility, and your environment is likely to stop functioning properly sooner or later."
  },
  {
    "objectID": "week06/w6_3_software.html#self-study-using-apptainer-containers",
    "href": "week06/w6_3_software.html#self-study-using-apptainer-containers",
    "title": "Using software at OSC",
    "section": "5 Self-study: Using Apptainer containers",
    "text": "5 Self-study: Using Apptainer containers\nContainers are an alternative to Conda to use programs that don’t have system-wide installations at OSC.\nContainers are similar to Virtual Machines and different from Conda environments in that they come with an entire operating system. This makes creating your own container “image” (see box below on terminology) much more involved than creating a Conda environment, and we will not cover that here.\nHowever, pre-existing container images are available for most bioinformatics programs, and these can be easily found, downloaded, and used.\n\n\n\n\n\n\nContainer terminology\n\n\n\n\nContainer image: File (Apptainer) or files (Docker) that contain the container application.\nContainer (sensu stricto): A running container image.\nDefinition file (Apptainer) / Dockerfile (Docker): A plain text file that contains the recipe to build a container image.\n\n\n\nAmong container platforms, Apptainer (formerly known as Singularity) and especially Docker are the most widely used ones. At supercomputers like OSC, however, only Apptainer containers can be used. Luckily, the Apptainer program can work with Docker container images: it will convert them on the fly.\n\n\n5.1 Finding container images online\nThere are several online repositories with publicly available container images, but I would recommend BioContainers (https://biocontainers.pro/registry) or Quay.io (https://quay.io/biocontainers).\nFor example, let’s look on the BioContainers website for a TrimGalore container image:\n\n\n\n\n\n\nThe search result on the BioContainers website after entering “trim galore” in the search box.\n\n\nClick on the only entry that is shown, trim-galore, which will get you to a page like this:\n\n\n\n\n\nThe website also includes Conda installation instructions — to see the container results, scroll down to:\n\n\n\n\n\n\nAfter scrolling down on the results page, you should see a recent available container image.\nNote that the command shown is singularity run, but we will use the more up-to-date apptainer run.\n\n\nThe version tag that is shown (0.6.9--hdfd78af_0 above) pertains to the version of TrimGalore, but the result that is shown here is not will always the container image(s) with the most recent version. To see a list of all available images, click on the Packages and Containers tab towards the top, and then sort by Last Update:\n\n\n\nThe logo with the large S depicts Singularity/Apptainer containers.\n\n\nWhenever you find both a Singularity/Apptainer and a Docker image for your program, use the Singularity/Apptainer image. This is because those don’t have to be converted, while Docker images do. But when the version you want is only available as a Docker image, that will work too: as mentioned above, it will be automatically converted to the proper format.\n\n\n\n5.2 Running a container image\nWhen you’ve found a container image that you want to use, copy its URL from the BioContainers website. For example, for the most recent TrimGalore version as of March 2024: https://depot.galaxyproject.org/singularity/trim-galore:0.6.10--hdfd78af_0.\nYou could also copy the full command — however, we will modify that in two ways, using:\n\nThe more up-to-date apptainer command9\nThe exec subcommand instead of run, allowing us to enter a custom command to run in the container10.\n\nAs such, our “base” command to run TrimGalore in the container will be as follows:\n# [Don't run this, we'll need to add a TrimGalore command]\napptainer exec https://depot.galaxyproject.org/singularity/trim-galore:0.6.10--hdfd78af_0\n\n\n\n\n\n\nNeed to use a Docker container? You can’t use the Docker URL as-is. (Click to expand)\n\n\n\n\n\nIf you want to use a Docker container, the listed quasi-URL on BioContainers will start with “quay.io”. In your apptainer exec command, you need to preface this URL with docker://. For instance:\napptainer exec docker://quay.io/biocontainers/trim-galore:0.6.10--hdfd78af_0\n\n\n\nAfter the code above, we would finish our command by simply entering a TrimGalore command in the exact same way as we would when running TrimGalore outside of the context of a container. For example, to just print the help info like we’ve been doing before, the TrimGalore command is:\ntrim_galore --help\nAnd to run that inside the container, our full command will be:\napptainer exec https://depot.galaxyproject.org/singularity/trim-galore:0.6.10--hdfd78af_0 \\\n    trim_galore --help\nINFO:    Downloading network image\n321.4MiB / 321.4MiB [===================================================================================================================================] 100 % 3.0 MiB/s 0s\nWARNING: Environment variable LD_PRELOAD already has value [], will not forward new value [/apps/xalt/xalt/lib64/libxalt_init.so] from parent process environment\n\n USAGE:\n\ntrim_galore [options] &lt;filename(s)&gt;\n\n-h/--help               Print this help message and exits.\n# [...truncated...]\n\n\n\n\n\n\nThe Apptainer/Singularity software does not need to be loaded at OSC, it is always automatically loaded.\n\n\n\n\n\n\nSo, all that is different from running a program inside a container instead of a a locally installed program, is that you prefix your command with apptainer exec &lt;URL&gt;.\nThe first time you run this command, the container will be downloaded, which can take a few minutes (by default it will be downloaded to ~/.apptainer/cache, but you can change this by setting the $APPTAINER_CACHEDIR environment variable). After that, the downloaded image will be used and the command should be executed about as instantaneously as when running TrimGalore outside of a container.\n(You will keep seeing the warning WARNING: Environment variable LD_PRELOAD [...] whenever you run a container, but this is nothing to worry about.)\n\n\n\n\n\n\nWhen to use a Container versus Conda\n\n\n\nWhen you need multiple programs in quick succession or in a single command (e.g., you’re piping the output of one program into a second program), it can be more convenient to have those programs installed in a single environment or container. Pre-built multi-program containers are not as easy to find. And since building your own Conda environment is easier than building your own container, this is a situation where you might prefer Conda."
  },
  {
    "objectID": "week06/w6_3_software.html#footnotes",
    "href": "week06/w6_3_software.html#footnotes",
    "title": "Using software at OSC",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n And with Git we saw another kind of behavior, where the automatically available version is very old, but we can load a more recent version.↩︎\n This may vary over time and also depends on whether you run this in the VS Code Server terminal — some of the loaded modules are related to that.↩︎\nWhen your personal computer asks you to “authenticate” while you are installing something, you are authenticating yourself as a user with administrator privileges. At OSC, you don’t have such privileges.↩︎\nOther software upon which the software that you are trying to install depends.↩︎\n It isn’t feasible to keep separate environments around for many different versions of a program, mostly because Conda environments contain a very large number of files, and OSC has file number quotas. This is why I have in many cases chosen the strategy of just updating the version within the same environment.↩︎\n Unless you first deactivate any active environments in your script.↩︎\nThat is, these settings will be saved somewhere in your OSC home directory, and you never have to set them again unless you need to make changes.↩︎\n Given that you’ve done some config above, this is not always necessary, but it can be good to be explicit.↩︎\nThough note that as of March 2024, the singularity command does still work, and it will probably continue to work for a while.↩︎\n The run subcommand would only run some preset default action, which is rarely useful for our purposes.↩︎"
  },
  {
    "objectID": "week06/w6_overview.html#links",
    "href": "week06/w6_overview.html#links",
    "title": "Week 5: Software usage and batch jobs at OSC",
    "section": "1 Links",
    "text": "1 Links\n\nLecture pages\n\nTuesday – More about OSC\nTuesday/Thursday – Data management\nThursday – Using software at OSC\n\n\n\nExercises & assignments\n\nExercises for this week"
  },
  {
    "objectID": "week06/w6_overview.html#content-overview",
    "href": "week06/w6_overview.html#content-overview",
    "title": "Week 5: Software usage and batch jobs at OSC",
    "section": "2 Content overview",
    "text": "2 Content overview\nThis week, we’ll start with an introduction to OSC and supercomputers. Then, we’ll see how we can submit shell scripts to OSC’s queue for compute jobs with the widely-used Slurm resource manager. Finally, we will learn about installing and managing software with Conda, and loading pre-installed software with the “module” system.\nSome of the things you will learn this week:\n\nWhen and why we need to run our analyses on supercomputers.\nKey terminology around supercomputers.\nDifferent ways to access and transfer data to and from OSC.\nDifferent ways to start “compute jobs”: via OnDemand, with interactive jobs, and with scripts.\nSome strategies around requesting appropriate resources for your compute jobs.\nThe Slurm/sbatch syntax to request specific resources for your compute jobs.\nHow to monitor Slurm jobs.\nHow to load pre-installed software at OSC with the “module” system.\nHow to install and manage software with Conda.\n\n\n2.1 Optional readings\n\nBuffalo Chapter 4: “Working with Remote Machines”"
  },
  {
    "objectID": "week06/w6_4_bonus_ssh.html#basic-ssh-connection-in-a-terminal",
    "href": "week06/w6_4_bonus_ssh.html#basic-ssh-connection-in-a-terminal",
    "title": "Bonus: Connecting to OSC through SSH",
    "section": "1 Basic SSH connection in a terminal",
    "text": "1 Basic SSH connection in a terminal\nTo connect to OSC or other remote computers without using a web portal like OnDemand, you can use SSH. You can do so via the ssh command if you have a Linux or a Mac computer, since these two operating systems are both Unix-based and have built-in terminals with Unix shells.\nHere, I’ll briefly demonstrate how to use the ssh command. On your own computer, open a terminal application and input the command ssh &lt;user&gt;@&lt;host&gt;, where:\n\n&lt;user&gt; should be replaced by your OSC username, and\n&lt;host&gt; should be replaced by the name of the computer you want to connect to:\n\npitzer.osc.edu to connect to the Pitzer cluster\nowens.osc.edu to connect to the Owens cluster\n\n\nFor example, if I (username jelmer) wanted to log in to the Pitzer cluster, I would use:\nssh jelmer@pitzer.osc.edu\nThe authenticity of host 'pitzer.osc.edu' can't be established.\nRSA key fingerprint is 2a:b6:f6:8d:9d:c2:f8:2b:8c:c5:03:06:a0:f8:59:12.\nAre you sure you want to continue connecting (yes/no)?\nIf this is the first time you are connecting to Pitzer via SSH, you’ll encounter a message similar to the one above. While the phrase “The authenticity of host ‘pitzer.osc.edu’ can’t be established.” sounds ominous, you will always get this warning when you attempt to connect to a remote computer for the first time, and you should type yes to proceed (you then won’t see this message again).\nYou should now be prompted for your password. Type it in carefully because no characters or even *s will appear on the screen, and then press Enter.\njelmer@pitzer.osc.edu's password:\nIf you entered your password correctly, your shell is now connected to OSC rather than operating on your own computer. That is, you’ll have shell access very much in the same way as when using the “Pitzer Shell Access” button on OSC OnDemand. (The key difference between SSH-ing in this way rather than using OnDemand is that the terminal is not running inside your browser, which can be convenient.)\n\n\n\n\n\n\nSSH shortcuts!\n\n\n\nIf you use SSH a lot to connect to OSC, typing ssh &lt;username&gt;@pitzer.osc.edu every time and then providing your password can get pretty tedious. The next two sections will show you how to make this go faster."
  },
  {
    "objectID": "week06/w6_4_bonus_ssh.html#avoid-being-prompted-for-your-osc-password",
    "href": "week06/w6_4_bonus_ssh.html#avoid-being-prompted-for-your-osc-password",
    "title": "Bonus: Connecting to OSC through SSH",
    "section": "2 Avoid being prompted for your OSC password",
    "text": "2 Avoid being prompted for your OSC password\nIf you take the following steps, you will not be prompted for your OSC password every time you log in using SSH. Both steps should be done in a terminal on your local machine:\n\nGenerate a public-private SSH key-pair:\nssh-keygen -t rsa\nYou’ll get some output and will then be asked several questions, but in each case, you can just press Enter to select the default answer.\nTransfer the public key to OSC’s Pitzer cluster:\n\n\nWindowsMac\n\n\ncat ~/.ssh/id_rsa.pub | ssh &lt;user&gt;@pitzer.osc.edu 'mkdir -p .ssh && cat &gt;&gt; .ssh/authorized_keys'\n\n\n# Replace &lt;user&gt; by your username, e.g. \"ssh-copy-id jelmer@pitzer.osc.edu\"\nssh-copy-id &lt;user&gt;@pitzer.osc.edu\n\n\n\nAll done! Test if it works by running:\n# Try connecting to Pitzer (once again, replace '&lt;user&gt;' by your username):\nssh &lt;user&gt;@pitzer.osc.edu\n\n\n\n\n\n\n\nThe same commands work for Owens, just replace pitzer with owens in the commands above.\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional instructions: See this Tecmint post in case you’re struggling to get this to work."
  },
  {
    "objectID": "week06/w6_4_bonus_ssh.html#use-a-shorter-name-for-your-ssh-connection",
    "href": "week06/w6_4_bonus_ssh.html#use-a-shorter-name-for-your-ssh-connection",
    "title": "Bonus: Connecting to OSC through SSH",
    "section": "3 Use a shorter name for your SSH connection",
    "text": "3 Use a shorter name for your SSH connection\nYou can easily set up alternative ways of referring to you SSH connection (i.e., “aliases”), such as shortening jelmer@pitzer.osc.edu to jp, by saving these aliases in a text file ~/.ssh/config, as shown below.\nThese two steps should both be done on your local machine:\n\nCreate a file called ~/.ssh/config:\ntouch ~/.ssh/config\nOpen the file in a text editor and add your alias(es) in the following format:\nHost &lt;arbitrary-alias-name&gt;    \n   HostName &lt;remote-address&gt;\n   User &lt;username&gt;\nFor instance, my file contains the following so as to connect to Pizer with jp and to Owens with jo:\nHost jp\n   HostName pitzer.osc.edu\n   User jelmer\n\nHost jo\n   HostName owens.osc.edu\n   User jelmer\n\nNow, you just need to use your, preferably very short, alias to log in — and if you did the previous no-password setup, you won’t even be prompted for your password!\nssh jp\n\n\n\n\n\n\n\nThese shortcuts also work with scp and rsync!\n\n\n\nFor example:\nrsync ~/scripts op:/fs/scratch/PAS0471"
  },
  {
    "objectID": "week06/w6_4_bonus_ssh.html#set-up-your-local-vs-code-to-ssh-tunnel-into-osc",
    "href": "week06/w6_4_bonus_ssh.html#set-up-your-local-vs-code-to-ssh-tunnel-into-osc",
    "title": "Bonus: Connecting to OSC through SSH",
    "section": "4 Set up your local VS Code to SSH tunnel into OSC",
    "text": "4 Set up your local VS Code to SSH tunnel into OSC\nIf you want to use VS Code to write code, have a shell, and interact with files at OSC directly, you don’t necessarily need to use the VS Code (Code Server) in OSC OnDemand. You can also make your local VS Code installation “SSH tunnel” into OSC.\nThis is a more convenient way of working because it’s quicker to start, will never run out of alotted time, and because you are not working inside a browser, you have more screen space and no keyboard shortcut interferences.\nThe set-up is pretty simple (see also these instructions if you get stuck), and should also work on Windows:\n\nIf necessary, install VS Code (instructions for Windows / Mac / Linux) on your computer, and open it.\nInstall the VS Code “Remote Development extension pack”: open the Extensions side bar (click the icon with the four squares in the far left), and in that side bar, search for “Remote Development extension pack”. That extension should appear with an Install button next to it: click that.\nOpen the Command Palette (F1 or Ctrl+ShiftP) and start typing “Remote SSH”.\nThen, select Remote-SSH: Add New SSH Host… and specify your SSH connection: for Pitzer, this is ssh &lt;osc-username&gt;@pitzer.osc.edu, e.g. ssh jelmer@pitzer.osc.edu (you’ll have to do this separately for Owens if you want to be able to connect to both this way).\nIn the “Select SSH configuration file to update” dialog, just select the first (top) option that shows up.\nYou’ll get a “Host Added!” pop-up in the bottom-right of your screen: in that pop-up, click Connect.\nIf you did the no-password setup described above (recommended!), you shouldn’t be prompted for a password and VS Code will connect to OSC!\n\nIf you’re asked about the operating system of the host, select Linux, which is the operating system of the OSC clusters.\nIf you’re asked whether you “Trust the Authors” of such-and-such directory, click Yes.\nIf there is a pop-up in the bottom-right asking you to update Git, click No and check “Don’t show again”.\n\n\n\n\n\n\n\n\n\nYou are now on a login node!\n\n\n\nJust be aware that you’ll now be on a Login node (and not on a Compute node like when you use VS Code through OnDemand), so avoid running analyses directly in the terminal, and so on."
  },
  {
    "objectID": "week01/w1_01_course-intro.html#introductions-jelmer-instructor",
    "href": "week01/w1_01_course-intro.html#introductions-jelmer-instructor",
    "title": "Course Intro",
    "section": "Introductions: Jelmer (instructor)",
    "text": "Introductions: Jelmer (instructor)\n\nLead of the CFAES Bioinformatics and Microscopy cores\n\nPart of what was until recently called the Molecular & Cellular Imaging Center (MCIC)\nWe are now grouped under CFAES Analytical Resources, core facilities providing services in molecular biology, high-throughput sequencing, bioinformatics, microscopy, and soil analyses.\n\n\n\n\n\nWhat I work on\n\nThe majority of my time is spent providing research assistance,\nworking with grad students and postdocs on omics data\nTeaching, such as this course, workshops, Code Club (https://osu-codeclub.github.io)\n\n\n\n\n\n\nBackground in animal evolutionary genomics & speciation\nIn my free time, I enjoy bird watching – locally & all across the world"
  },
  {
    "objectID": "week01/w1_01_course-intro.html#introductions-ta-co-instructor",
    "href": "week01/w1_01_course-intro.html#introductions-ta-co-instructor",
    "title": "Course Intro",
    "section": "Introductions: TA / co-instructor",
    "text": "Introductions: TA / co-instructor\nTBA"
  },
  {
    "objectID": "week01/w1_01_course-intro.html#introductions-you",
    "href": "week01/w1_01_course-intro.html#introductions-you",
    "title": "Course Intro",
    "section": "Introductions: You",
    "text": "Introductions: You\n\nName\nLab and Department\nResearch interests and/or current research topics\nSomething about you that is not work-related, such as a hobby or fun fact"
  },
  {
    "objectID": "week01/w1_01_course-intro.html#the-core-goals-of-this-course",
    "href": "week01/w1_01_course-intro.html#the-core-goals-of-this-course",
    "title": "Course Intro",
    "section": "The core goals of this course",
    "text": "The core goals of this course\nLearning skills that will enable you to:\n\nDo your research more reproducibly and efficiently (e.g. by using code)\nWork with large-scale “omics” datasets and do applied bioinformatics\n\n\n\nTo do so, this course will focus primarily on what we may call “foundational computational skills” rather than on specific applications. For example, you will learn to:\n\nCode in the Unix shell and R languages\nOrganize, document, and manage your project data, code, and results\nWork with a remote supercomputer\nWrite automated analysis pipelines"
  },
  {
    "objectID": "week01/w1_01_course-intro.html#course-background-i-reproducibility",
    "href": "week01/w1_01_course-intro.html#course-background-i-reproducibility",
    "title": "Course Intro",
    "section": "Course background I: Reproducibility",
    "text": "Course background I: Reproducibility\nTwo related ideas:\n\nGetting same results with an independent experiment (replicable)\nGetting same results given the same data (reproducible)\n\n\nOur focus is on #2."
  },
  {
    "objectID": "week01/w1_01_course-intro.html#course-background-i-reproducibility-cont.",
    "href": "week01/w1_01_course-intro.html#course-background-i-reproducibility-cont.",
    "title": "Course Intro",
    "section": "Course background I: Reproducibility (cont.)",
    "text": "Course background I: Reproducibility (cont.)\n\n“The most basic principle for reproducible research is: Do everything via code.”\n—Karl Broman\n\n\nAdditionally, also important for reproducibility are:\n\nProject organization and documentation (week 3)\nManaging and sharing your data and code (for code: Git & GitHub, week 4)\n\n\n\n\n\n\n\n\nAnother motivator: working reproducibly will benefit future you!"
  },
  {
    "objectID": "week01/w1_01_course-intro.html#course-background-ii-efficiency-and-automation",
    "href": "week01/w1_01_course-intro.html#course-background-ii-efficiency-and-automation",
    "title": "Course Intro",
    "section": "Course background II: Efficiency and automation",
    "text": "Course background II: Efficiency and automation\nUsing code enables you to work more efficiently and automatically —\nparticularly useful when having to:\n\nDo repetitive tasks\nRecreate a figure or redo an analysis after adding a sample\nRedo a project after uncovering a mistake in the first data processing step."
  },
  {
    "objectID": "week01/w1_01_course-intro.html#course-background-iiia-omics-data",
    "href": "week01/w1_01_course-intro.html#course-background-iiia-omics-data",
    "title": "Course Intro",
    "section": "Course background IIIa: Omics data",
    "text": "Course background IIIa: Omics data\nOmics data is increasingly important in biology, and most notably includes the study of:\n\nDNA at the (near) whole-genome level: Genomics\nExpressed RNA at the (near) whole-transcriptome level: Transcriptomics\nExpressed protein at the (near) whole-proteome level: Proteomics\n\n\n\n\n\n\n\n\nThe next lecture will introduce omics data in more detail."
  },
  {
    "objectID": "week01/w1_01_course-intro.html#course-background-iiib-omics-data-analysis",
    "href": "week01/w1_01_course-intro.html#course-background-iiib-omics-data-analysis",
    "title": "Course Intro",
    "section": "Course background IIIb: Omics data analysis",
    "text": "Course background IIIb: Omics data analysis\nExamples in the course will involve the analyses of nucleotide sequencing-based data (genomics and transcriptomics), which in many cases can be divided into two subsequent stages:\n\nAlgorithmic/bioinformatic data processing\n\nFor example: alignment of reads to a genome\nThis is typically done in a Unix shell environment using a supercomputer\n\nBiostatistical downstream analysis\n\n\nFor example: comparing expression levels of genes between treatments\nThis is typically done in R and can be done on a laptop\n\n\n\n\n\n\n\n\nWhat this course does and does not focus on\n\n\n\nWhile we’ll be using some example omics datasets, this course will not comprehensively cover specific omics analyses — our focus is much more on foundational computational skills.\nA highly recommended follow-up course to learn omics analysis specifics:\nGenome Analytics (HCS 7004) by Jonathan Fresnedo-Ramirez"
  },
  {
    "objectID": "week01/w1_01_course-intro.html#course-background-iv-applied-bioinformatics",
    "href": "week01/w1_01_course-intro.html#course-background-iv-applied-bioinformatics",
    "title": "Course Intro",
    "section": "Course background IV: Applied bioinformatics",
    "text": "Course background IV: Applied bioinformatics\nAlso: computational biology\nTBA"
  },
  {
    "objectID": "week01/w1_01_course-intro.html#the-unix-shell-shell-scripts",
    "href": "week01/w1_01_course-intro.html#the-unix-shell-shell-scripts",
    "title": "Course Intro",
    "section": "The Unix shell & shell scripts",
    "text": "The Unix shell & shell scripts\nThe Unix shell (or the “Terminal”) is a command-line interface to computers.\nBeing able to use the Unix shell is a fundamental skill when working with omics data, for example because many of the specialized analysis software must be run using the shell.\n\n\nYou’ll spend a lot of time with the Unix shell, starting next week.\nYou’ll also write shell scripts, and will use an editor called VS Code for this and other purposes.\n\n\n\n\n\n\n\n\n\n\nBash (shell language)\n\n\n\n\n\n\n\n\nVS Code"
  },
  {
    "objectID": "week01/w1_01_course-intro.html#project-organization-documentation",
    "href": "week01/w1_01_course-intro.html#project-organization-documentation",
    "title": "Course Intro",
    "section": "Project organization & documentation",
    "text": "Project organization & documentation\nGood project organization & documentation is a necessary starting point for reproducible research.\n\n\nYou’ll learn best practices for project organization, file naming, etc.\nYou’ll learn how to manage your data and software\nTo document and report what you are doing, you’ll use Markdown files.\n\n\n\n\n\n\n\n\nMarkdown"
  },
  {
    "objectID": "week01/w1_01_course-intro.html#version-control-with-git-and-github",
    "href": "week01/w1_01_course-intro.html#version-control-with-git-and-github",
    "title": "Course Intro",
    "section": "Version control with Git and GitHub",
    "text": "Version control with Git and GitHub\nUsing version control, you can more effectively keep track of project progress, collaborate, share code, revisit earlier versions, and undo.\n\n\nGit is the version control software we will use,\nand GitHub is the website that hosts Git projects (repositories).\nYou’ll also use Git + GitHub to hand in your graded assignments."
  },
  {
    "objectID": "week01/w1_01_course-intro.html#high-performance-computing-with-osc",
    "href": "week01/w1_01_course-intro.html#high-performance-computing-with-osc",
    "title": "Course Intro",
    "section": "High-performance computing with OSC",
    "text": "High-performance computing with OSC\nThanks to supercomputer resources, you can work with very large datasets at speed — running up to 100s of analyses in parallel, and using much larger amounts of memory and storage space than a personal computer has.\n\n\n\nWe will use OSC throughout the course, and you’ll get a brief intro to it this week\nIn week 5, you’ll learn how to manage data and software at OSC (e.g. with Conda)\nIn week 6, you’ll learn to submit shell scripts as OSC “batch jobs” with Slurm"
  },
  {
    "objectID": "week01/w1_01_course-intro.html#automated-workflow-management",
    "href": "week01/w1_01_course-intro.html#automated-workflow-management",
    "title": "Course Intro",
    "section": "Automated workflow management",
    "text": "Automated workflow management\nOmics data analyses typically consist of many consecutive steps.\nUsing a workflow written with a workflow manager, you can run and rerun an entire analysis pipeline with a single command (and much more).\n\n\nYou’ll use the workflow language Nextflow to build your pipelines\nYou will also learn how to use comprehensive, best-practice omics data Nextflow pipelines produced by the nf-core initiative"
  },
  {
    "objectID": "week01/w1_01_course-intro.html#the-r-language",
    "href": "week01/w1_01_course-intro.html#the-r-language",
    "title": "Course Intro",
    "section": "The R language",
    "text": "The R language\nWhile the Unix shell, and software that is run through the Unix shell, is best used for the initial (algorithmic) processing steps of omics data, R is probably the most prominent language in the more “downstream” and often statistical analysis and visualization of omics data.\nIn this course, you will learn the basics of R, how to visualize data in R, and how you can use specialized packages for omics data analyis.\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR vs. Python\n\n\nPython is also commonly used but I believe that altogether, R is a far better choice for this course. On the other, Python is a great follow-up language to learn for those seeking to specialize in bioinformatics."
  },
  {
    "objectID": "week01/w1_01_course-intro.html#using-generative-ai-to-help-with-coding",
    "href": "week01/w1_01_course-intro.html#using-generative-ai-to-help-with-coding",
    "title": "Course Intro",
    "section": "Using generative AI to help with coding",
    "text": "Using generative AI to help with coding\n\nTBA"
  },
  {
    "objectID": "week01/w1_01_course-intro.html#zoom",
    "href": "week01/w1_01_course-intro.html#zoom",
    "title": "Course Intro",
    "section": "Zoom",
    "text": "Zoom\n\nBe muted by default, but feel free to unmute yourself to ask questions any time.\nQuestions can also be asked in the chat.\nHaving your camera turned on as much as possible is appreciated!\n\n\n\n“Screen real estate” — large/multiple monitors or multiple devices best.\nBe ready to share your screen."
  },
  {
    "objectID": "week01/w1_01_course-intro.html#participatory-live-coding",
    "href": "week01/w1_01_course-intro.html#participatory-live-coding",
    "title": "Course Intro",
    "section": "Participatory live coding",
    "text": "Participatory live coding\nTBA"
  },
  {
    "objectID": "week01/w1_01_course-intro.html#websites-books",
    "href": "week01/w1_01_course-intro.html#websites-books",
    "title": "Course Intro",
    "section": "Websites & Books",
    "text": "Websites & Books\n\nInfo about CarmenCanvas website TBA\n\n\n\n\nThis “GitHub website” (https://mcic-osu.github.io/pracs-au25) contains:\n\nOverviews of each week & readings\nSlide decks and lecture pages\nAssignments\nExercises\n\n\n\n\n\n\nI am only using slides for this course intro and for the very next lecture on omics data. Other material will be presented via “regular” pages on the GitHub website, as this works better for the interactive live coding we’ll be doing."
  },
  {
    "objectID": "week01/w1_01_course-intro.html#quick-tour-of-the-github-website",
    "href": "week01/w1_01_course-intro.html#quick-tour-of-the-github-website",
    "title": "Course Intro",
    "section": "Quick tour of the GitHub website",
    "text": "Quick tour of the GitHub website\nTBA"
  },
  {
    "objectID": "week01/w1_01_course-intro.html#readings",
    "href": "week01/w1_01_course-intro.html#readings",
    "title": "Course Intro",
    "section": "Readings",
    "text": "Readings\nMost weeks, additional readings are optional because you are always expected to reread and practice more with the material that we go through in class.\nAdditionally, anything on a given page that we do not get to in class automatically turns into required self-study material.\n\nSeveral weeks will have 1 or 2 papers as required reading.\nMost weeks have as optional readings chapters from the following two books:\n\nComputing Skills for Biologists (“CSB”; Allesina & Wilmes 2019)\nBioinformatics Data Skills (“Buffalo”; Buffalo 2015)"
  },
  {
    "objectID": "week01/w1_01_course-intro.html#office-hours",
    "href": "week01/w1_01_course-intro.html#office-hours",
    "title": "Course Intro",
    "section": "Office hours",
    "text": "Office hours\nTBA"
  },
  {
    "objectID": "week01/w1_01_course-intro.html#computer-requirements",
    "href": "week01/w1_01_course-intro.html#computer-requirements",
    "title": "Course Intro",
    "section": "Computer requirements",
    "text": "Computer requirements\nTBA"
  },
  {
    "objectID": "week01/w1_01_course-intro.html#what-your-grade-is-made-up-of",
    "href": "week01/w1_01_course-intro.html#what-your-grade-is-made-up-of",
    "title": "Course Intro",
    "section": "What your grade is made up of",
    "text": "What your grade is made up of\nYou can earn a total of 100 points across 6 assignments and 4 final project checkpoints."
  },
  {
    "objectID": "week01/w1_01_course-intro.html#graded-assignments",
    "href": "week01/w1_01_course-intro.html#graded-assignments",
    "title": "Course Intro",
    "section": "Graded assignments",
    "text": "Graded assignments\nThese are due on Mondays and are worth 10 points each:\n\nShell basics (due week 3)\nMarkdown & Git (due week 5)\nShell scripting (due week 6)\nOSC batch jobs (due week 8)\nNextflow (due week 11)\nR (due week 15)\n\nThe first one is submitted through CarmenCanvas, while all others are submitted via GitHub so you can get more practice with that."
  },
  {
    "objectID": "week01/w1_01_course-intro.html#final-project",
    "href": "week01/w1_01_course-intro.html#final-project",
    "title": "Course Intro",
    "section": "Final project",
    "text": "Final project\nPlan and implement a small computational project, with the following checkpoints:\n\nI: Proposal (due week 13 – 5 points)\nII: Draft (due week 15 – 5 points)\nIII: Oral presentations on Zoom (week 16 – 10 points)\nIV: Final submission (due Dec 15 – 20 points)\n\n\n\n\n\n\n\n\n\nData sets for the final project\n\n\nIt is ideal if you have/develop your own idea for a data set and analysis — for example, that way you may do something that’s directly useful for your own research.\nIf not, I can provide you with this.\nMore information about the final project will follow later in the course."
  },
  {
    "objectID": "week01/w1_01_course-intro.html#using-generative-ai-for-graded-assignments",
    "href": "week01/w1_01_course-intro.html#using-generative-ai-for-graded-assignments",
    "title": "Course Intro",
    "section": "Using generative AI for graded assignments",
    "text": "Using generative AI for graded assignments\nTBA"
  },
  {
    "objectID": "week01/w1_01_course-intro.html#ungraded-homework",
    "href": "week01/w1_01_course-intro.html#ungraded-homework",
    "title": "Course Intro",
    "section": "Ungraded homework",
    "text": "Ungraded homework\n\nWeekly readings\nWeekly exercises — I recommend doing these on Fridays after the week’s session.\nMiscellaneous small assignments such as surveys and account setup.\n\n\n\n\n\n\n\n\nWeekly materials & homework\n\n\nI will try add the materials for each week on the preceding Friday — at the least the week’s overview and readings.\nNone of this homework had to be handed in."
  },
  {
    "objectID": "week01/w1_01_course-intro.html#weekly-recitation-on-monday",
    "href": "week01/w1_01_course-intro.html#weekly-recitation-on-monday",
    "title": "Course Intro",
    "section": "Weekly recitation on Monday",
    "text": "Weekly recitation on Monday\nWe will have an optional but highly recommended weekly recitation meeting on Mondays, during which we go over the exercises for the preceding week.\n\n\n\n\n\n\n\nPractice is key!\n\n\nThis course is intended to be highly practical, and if you don’t practice the skills we will focus on by yourself, you will not get much out of it.\n\n\n\n\nPlease indicate your availability here: TBA"
  },
  {
    "objectID": "week01/w1_01_course-intro.html#rest-of-this-week",
    "href": "week01/w1_01_course-intro.html#rest-of-this-week",
    "title": "Course Intro",
    "section": "Rest of this week",
    "text": "Rest of this week\n\nIntroduction to omics data\n\n\n\nIntroduction to the Ohio Supercomputer Center (OSC)\n\n\n\nHomework:\n\nTBA"
  },
  {
    "objectID": "week01/w1_02_omics-data.html#the-main-omics-data-types",
    "href": "week01/w1_02_omics-data.html#the-main-omics-data-types",
    "title": "Omics data",
    "section": "The main omics data types",
    "text": "The main omics data types\n\n\nCopyright ThermoFisher"
  },
  {
    "objectID": "week01/w1_02_omics-data.html#the-main-omics-data-types-1",
    "href": "week01/w1_02_omics-data.html#the-main-omics-data-types-1",
    "title": "Omics data",
    "section": "The main omics data types",
    "text": "The main omics data types\n\nGenomics (including metagenomics) DNA\nEpigenomics DNA modifications\nTranscriptomics RNA\nProteomics Proteins\nMetabolomics Metabolites\n\n\n\n\n\n\n\nNote\n\n\nShould be large-scale, e.g. “genomics” is largely at the “whole-genome” level.\n\n\n\nBoth genomics and transcriptomics data, in the broad definitions above, is produced by high-throughput sequencing technologies.\nThat will be the focus of this lecture and will be used in examples throughout the course."
  },
  {
    "objectID": "week01/w1_02_omics-data.html#what-does-sequencing-refer-to",
    "href": "week01/w1_02_omics-data.html#what-does-sequencing-refer-to",
    "title": "Omics data",
    "section": "What does sequencing refer to?",
    "text": "What does sequencing refer to?\nThe shorthand sequencing, like in “high-throughput sequencing”, generally refers to determining the nucleotide sequence of fragments of DNA.\n\n\n\n\n\n\n\n\nWhat about RNA or proteins?\n\n\n\nRNA is usually reverse transcribed to DNA (cDNA) prior to sequencing, as in nearly all “RNA-seq”.\nDirect RNA sequencing is possible with one of the sequencing technologies we’ll discuss, but this is under development and not yet widely used.\n\n\n\nProtein sequencing requires different technology altogether, such as mass spectrometry, and is not further discussed in this lecture."
  },
  {
    "objectID": "week01/w1_02_omics-data.html#sequencing-technologies-overview",
    "href": "week01/w1_02_omics-data.html#sequencing-technologies-overview",
    "title": "Omics data",
    "section": "Sequencing technologies: overview",
    "text": "Sequencing technologies: overview\nSanger sequencing (since 1977)\nSequences a single, typically PCR-amplified, short-ish (≤900 bp) DNA fragment at a time\n\n\nHigh-throughput sequencing (HTS, since 2005)\nSequences hundreds of thousand to billions, usually randomly selected, DNA fragments at a time\n\nSequenced DNA fragments are referred to as “reads”."
  },
  {
    "objectID": "week01/w1_02_omics-data.html#sequencing-cost-through-time",
    "href": "week01/w1_02_omics-data.html#sequencing-cost-through-time",
    "title": "Omics data",
    "section": "Sequencing cost through time",
    "text": "Sequencing cost through time\n\nhttps://www.genome.gov/about-genomics/fact-sheets/Sequencing-Human-Genome-cost"
  },
  {
    "objectID": "week01/w1_02_omics-data.html#examples-of-hts-applications",
    "href": "week01/w1_02_omics-data.html#examples-of-hts-applications",
    "title": "Omics data",
    "section": "Examples of HTS applications",
    "text": "Examples of HTS applications\n\nWhole-genome assembly\n\n\n\nVariant analysis (for population genetics/genomics, molecular evolution, GWAS, etc.):\n\nWhole-genome “resequencing”\nReduced-representation libraries (e.g. RADseq, GBS)\n\n\n\n\n\nRNA-seq (transcriptome analysis)\n\n\n\n\nOther functional sequencing methods like methylation sequencing, ChIP-seq, etc.\n\n\n\n\nMicrobial community characterization\n\nMetabarcoding\nShotgun metagenomics"
  },
  {
    "objectID": "week01/w1_02_omics-data.html#hts-applications-cont.",
    "href": "week01/w1_02_omics-data.html#hts-applications-cont.",
    "title": "Omics data",
    "section": "HTS applications (cont.)",
    "text": "HTS applications (cont.)"
  },
  {
    "objectID": "week01/w1_02_omics-data.html#examples-of-hts-analyses",
    "href": "week01/w1_02_omics-data.html#examples-of-hts-analyses",
    "title": "Omics data",
    "section": "Examples of HTS analyses",
    "text": "Examples of HTS analyses\n\nAlgorithmic/bioinformatics stage:\n\nRead QC\nRead trimming\nRead alignment and classification\nRead assembly\nGenotype calling\n\n\n\n\n\nBiostatistical stage:\n\nDifferential abundance among groups\nClustering/ordination and network analyses\nGWAS\nFunctional enrichment"
  },
  {
    "objectID": "week01/w1_02_omics-data.html#the-main-hts-technologies",
    "href": "week01/w1_02_omics-data.html#the-main-hts-technologies",
    "title": "Omics data",
    "section": "The main HTS technologies",
    "text": "The main HTS technologies\n\n\nShort-read HTS\n\nProduces up to billions of 50-300 bp highly accurate reads\nMarket dominated by Illumina\nSince 2005 — technology fairly stable\n(AKA Next-Generation Sequencing - NGS)\n\n\n\n\nLong-read HTS\n\nReads much longer than in NGS but fewer, less accurate, and more costly per base\nMainly Oxford Nanopore Technologies (ONT) and Pacific Biosciences (PacBio)\nSince 2011 — remains under rapid development\n\n\n\n\n\n\n\n\n\n\nShort videos explaining the technology (90 s - 5 m each)\n\n\n\nIllumina: https://www.youtube.com/watch?v=fCd6B5HRaZ8\nNanopore: https://www.youtube.com/watch?v=RcP85JHLmnI\nPacBio: https://www.youtube.com/watch?v=_lD8JyAbwEo"
  },
  {
    "objectID": "week01/w1_02_omics-data.html#read-lengths",
    "href": "week01/w1_02_omics-data.html#read-lengths",
    "title": "Omics data",
    "section": "Read lengths",
    "text": "Read lengths\n\nShort-read (Illumina) HTS: 50-300 bp reads\nLong-read HTS: longer & more variable read lengths (PacBio: 10-50 kbp, ONT: 10-100+ kbp)\n\n\n\n\n\nWhen are longer reads useful?\n\n\nGenome assembly\nHaplotype and large structural variant calling\nTranscript isoform identification\nTaxonomic identification of single reads (microbial metabarcoding)\n\n\n\n\n\n\n\nWhen does read length not matter (as much)?\n\n\nSNP variant analysis\nRead-as-a-tag: the goal is just to know a read’s origin in a reference genome, like in counting applications such as RNA-seq"
  },
  {
    "objectID": "week01/w1_02_omics-data.html#error-rates",
    "href": "week01/w1_02_omics-data.html#error-rates",
    "title": "Omics data",
    "section": "Error rates",
    "text": "Error rates\nCurrently, no sequencing technology is error-free.\n\nIllumina error rates are mostly below 0.1%\nTBA\n\n\n\n\n\n\n\nError rates are changing\n\n\nError rates in one recent type of PacBio sequencing where individual fragments are sequenced multiple times (“HiFi”) are now lower than in Illumina.\nError rates of ONT sequencing are also continuously decreasing.\n\n\n\n\n\n\n\n\n\n\n\nQuality scores in sequence data\n\n\nWhen you get sequences from a high-throughput sequencer, base calls have typically already been made. Every base is also accompanied by a quality score (inversely related to the estimated error probability). We’ll talk about those in some more detail in a bit."
  },
  {
    "objectID": "week01/w1_02_omics-data.html#libraries-and-library-prep",
    "href": "week01/w1_02_omics-data.html#libraries-and-library-prep",
    "title": "Omics data",
    "section": "Libraries and library prep",
    "text": "Libraries and library prep\nWe will talk a but about Illumina library prep because this is the most common type of sequencing, and because throughout the course, we will use Illumina read files as examples.\nIn a sequencing context, a “library” is a collection of nucleic acid fragments ready for sequencing.\n\n\nIn Illumina and other HTS libraries, these fragments number in the millions or billions and are often randomly generated from input such as genomic DNA:\n\n\n\n\n\n\n\n\nAn overview of the library prep procedure. This is typically done for you by a sequencing facility or company."
  },
  {
    "objectID": "week01/w1_02_omics-data.html#a-closer-look-at-the-processed-dna-fragments",
    "href": "week01/w1_02_omics-data.html#a-closer-look-at-the-processed-dna-fragments",
    "title": "Omics data",
    "section": "A closer look at the processed DNA fragments",
    "text": "A closer look at the processed DNA fragments\nAs shown in the previous slide, after library prep, each DNA fragment is flanked by several types of short sequences that together make up the “adapters”:\n\n\n\n\n\n\n\n\n\nMultiplexing!\n\n\nUsing the indices/barcodes in adapters, up to 96 samples can be multiplexed into a single library."
  },
  {
    "objectID": "week01/w1_02_omics-data.html#paired-end-vs.-single-end-sequencing",
    "href": "week01/w1_02_omics-data.html#paired-end-vs.-single-end-sequencing",
    "title": "Omics data",
    "section": "Paired-end vs. single-end sequencing",
    "text": "Paired-end vs. single-end sequencing\nDNA fragments can be sequenced from both ends as shown below —\nthis is called “paired-end” (PE) sequencing:\n\n\n\n\n\n\n\nWhen sequencing is instead single-end (SE), no reverse read is produced:"
  },
  {
    "objectID": "week01/w1_02_omics-data.html#insert-size-variation",
    "href": "week01/w1_02_omics-data.html#insert-size-variation",
    "title": "Omics data",
    "section": "Insert size variation",
    "text": "Insert size variation\nThe size of the DNA fragment can vary – both by design and because of limited precision in size selection. In some cases, it is:\n\nShorter than the combined read length, leading to overlapping reads (this can be useful):\n\n\n\n\n\n\n\n\nShorter than the single read length, leading to “adapter read-through” (i.e., the ends of the resulting reads will consist of adapter sequence, which should be removed):"
  },
  {
    "objectID": "week01/w1_02_omics-data.html#the-sequencing-process",
    "href": "week01/w1_02_omics-data.html#the-sequencing-process",
    "title": "Omics data",
    "section": "The sequencing process",
    "text": "The sequencing process\n\nhttps://www.youtube.com/watch?v=fCd6B5HRaZ8"
  },
  {
    "objectID": "week01/w1_02_omics-data.html#genomes",
    "href": "week01/w1_02_omics-data.html#genomes",
    "title": "Omics data",
    "section": "Genomes",
    "text": "Genomes\nMany HTS applications either require a “reference genome” or involve its production.\n\n\nWhat exactly does “reference genome” refer to? It usually includes:\n\nAssembly\nA representation of most of the genome DNA sequence: the genome assembly\nAnnotation\nThe “annotation” that provides the locations of genes and other genomic features, as well as functional information on these features\n\n\n\n\n\n\n\n\n\nTaxonomic identity\n\n\nReference genomes are typically needed and used at the species level.\n\nIf needed, it is often possible to work with reference genomes of closely related species\nConversely, multiple reference genomes may exist, e.g. for different subspecies"
  },
  {
    "objectID": "week01/w1_02_omics-data.html#overview",
    "href": "week01/w1_02_omics-data.html#overview",
    "title": "Omics data",
    "section": "Overview",
    "text": "Overview\nAll common seqeunce/genomic data files are plain-text. The main types are:\n\nFASTA\nSimple sequence files, where each entry contains a header and a DNA/AA sequence.\nVersatile, can contain a few short sequences, entire genome assemblies, proteomes, and alignments.\n\n\n\n\nFASTQ\nThe standard format for HTS reads — contains a quality score for each nucleotide.\nSAM/BAM\nAn alignment format for HTS reads.\n\n\n\n\n\nGTF/GFF\nTables (tab-delimited) with information such as genomic coordinates on “genomic features” such as genes and exons. The files contain reference genome annotations."
  },
  {
    "objectID": "week01/w1_02_omics-data.html#fasta-files",
    "href": "week01/w1_02_omics-data.html#fasta-files",
    "title": "Omics data",
    "section": "FASTA files",
    "text": "FASTA files\nFASTA files contain one or more DNA or amino acid sequences, with no limits on the number of sequences or the sequence lengths.\nThe following example FASTA file contains two entries:\n&gt;unique_sequence_ID Optional description\nATTCATTAAAGCAGTTTATTGGCTTAATGTACATCAGTGAAATCATAAATGCTAAAAA\n&gt;unique_sequence_ID2\nATTCATTAAAGCAGTTTATTGGCTTAATGTACATCAGTGAAATCATAAATGCTAAATG\n\n\nEach entry contains a header and the sequence itself, where:\n\nHeader lines start with a &gt; and provide identifying information for the sequence\nThe sequence is often spread across multiple lines with a fixed width"
  },
  {
    "objectID": "week01/w1_02_omics-data.html#fastq",
    "href": "week01/w1_02_omics-data.html#fastq",
    "title": "Omics data",
    "section": "FASTQ",
    "text": "FASTQ\nFASTQ is the standard format for HTS reads.\nEach read forms one FASTQ entry and is represented by four lines, which contain, respectively:\n\n\nA header that starts with @ and e.g. uniquely identifies the read\nThe sequence itself\nA + (plus sign)\nOne-character quality scores for each base (hence FASTQ as in “Q” for “quality”)"
  },
  {
    "objectID": "week01/w1_02_omics-data.html#fastq-quality-scores",
    "href": "week01/w1_02_omics-data.html#fastq-quality-scores",
    "title": "Omics data",
    "section": "FASTQ quality scores",
    "text": "FASTQ quality scores\nThe quality scores we saw in the read on the previous slide represent an estimate of the error probability of the base call.\nSpecifically, they correspond to a numeric “Phred” quality score (Q), which is a function of the estimated probability that a base call is erroneous (P):\n\nQ = -10 * log10(P)\n\n\n\nFor some specific probabilities and their rough qualitative interpretation for Illumina data:\n\n\n\nPhred quality score\nError probability\nRough interpretation\n\n\n\n\n10\n1 in 10\nterrible\n\n\n20\n1 in 100\nbad\n\n\n30\n1 in 1,000\ngood\n\n\n40\n1 in 10,000\nexcellent"
  },
  {
    "objectID": "week01/w1_02_omics-data.html#fastq-quality-scores-cont.",
    "href": "week01/w1_02_omics-data.html#fastq-quality-scores-cont.",
    "title": "Omics data",
    "section": "FASTQ quality scores (cont.)",
    "text": "FASTQ quality scores (cont.)\nThis numeric quality score is represented in FASTQ files not by the number itself, but by a corresponding “ASCII character”.\nThis allows for a single-character representation of each possible score — as a consequence, each quality score character can conveniently correspond to (& line up with) a base character in the read.\n\n\n\n\nPhred quality score\nError probability\nASCII character\n\n\n\n\n10\n1 in 10\n+\n\n\n20\n1 in 100\n5\n\n\n30\n1 in 1,000\n?\n\n\n40\n1 in 10,000\nI\n\n\n\n\n\n\n\n\n\nA rule of thumb\n\n\nIn practice, you almost never have to manually check the quality scores of bases in FASTQ files, but if you do, a rule of thumb is that letter characters are good (Phred of 32 and up)."
  },
  {
    "objectID": "week01/w1_02_omics-data.html#fastq-cont.",
    "href": "week01/w1_02_omics-data.html#fastq-cont.",
    "title": "Omics data",
    "section": "FASTQ (cont.)",
    "text": "FASTQ (cont.)\nFASTQ files have no size limit, but in paired-end (PE) sequencing, forward and reverse reads are split into two files:\nforward reads contain R1 and reverse reads contain R2 in the file name.\nFor example, having paired-end FASTQ files for 2 samples could look like this:\n# A listing of (unusually simple) file names:\nsample1_R1.fastq.gz\nsample1_R2.fastq.gz\nsample2_R1.fastq.gz\nsample2_R2.fastq.gz"
  },
  {
    "objectID": "week01/w1_02_omics-data.html#gffgtf",
    "href": "week01/w1_02_omics-data.html#gffgtf",
    "title": "Omics data",
    "section": "GFF/GTF",
    "text": "GFF/GTF\nTBA"
  },
  {
    "objectID": "week01/w1_02_omics-data.html#sequencing-technology-development-timeline",
    "href": "week01/w1_02_omics-data.html#sequencing-technology-development-timeline",
    "title": "Omics data",
    "section": "Sequencing technology development timeline",
    "text": "Sequencing technology development timeline\n\n\nModified after Pereira et al. 2020"
  },
  {
    "objectID": "week01/w1_02_omics-data.html#overcoming-sequencing-errors",
    "href": "week01/w1_02_omics-data.html#overcoming-sequencing-errors",
    "title": "Omics data",
    "section": "Overcoming sequencing errors",
    "text": "Overcoming sequencing errors\nSequencing every bases multiple times, i.e. having a &gt;1x so-called “depth of coverage” allows to infer the correct sequence:\n\n\n\n\nOvercoming sequencing errors is made more challenging by natural genetic variation among and within individuals.\nTypical depths of coverage: at least 50-100x for genome assembly; 10-30x for resequencing."
  },
  {
    "objectID": "week01/w1_02_omics-data.html#genome-size-variation",
    "href": "week01/w1_02_omics-data.html#genome-size-variation",
    "title": "Omics data",
    "section": "Genome size variation",
    "text": "Genome size variation\n\n\n\n\n\n\n\nhttps://en.wikipedia.org/wiki/Genome_size"
  },
  {
    "objectID": "week01/w1_02_omics-data.html#growth-of-genome-databases",
    "href": "week01/w1_02_omics-data.html#growth-of-genome-databases",
    "title": "Omics data",
    "section": "Growth of genome databases",
    "text": "Growth of genome databases\n\n\n\nKonkel & Slot 2023"
  },
  {
    "objectID": "ref/file-transfer.html#footnotes",
    "href": "ref/file-transfer.html#footnotes",
    "title": "Other OSC file transfer options",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n This may be different at other supercomputer centers: there are no inherent transfer size limitations to these commands.↩︎\nFor simplicity, these commands are copying between local dirs, which is also possible with rsync.↩︎"
  },
  {
    "objectID": "ref/about.html",
    "href": "ref/about.html",
    "title": "About this site and course",
    "section": "",
    "text": "This is the GitHub website for the course Practical Computing Skills for Omics Data (PLNTPTH 5006), a 3-credit course taught at Ohio State University during the Fall semester of 2025.\nThe course is taught by by Jelmer Poelstra from OSU’s Molecular and Cellular Imaging Center (MCIC) for the Department of Plant Pathology.\n\n\nCourse description\nAs datasets have rapidly grown larger in biology, coding has been recognized as an increasingly important skill for biologists. This is especially true in “omics” research with data from e.g. genomics and transcriptomics, which typically cannot be analyzed on a desktop computer, where cutting-edge software has a command-line interface, and where workflows can include many steps that need to be coordinated.\nIn this course, students will gain hands-on experience with a set of general and versatile tools for data-intensive research in omics and beyond. The course will focus on foundational skills such as working in the Unix shell and writing shell scripts, managing software and submitting jobs at a compute cluster (the Ohio Supercomputer Center), and building flexible, automated workflows. Additionally, the course will cover reproducibly organizing, documenting, and version-controlling research projects. Taken together, this course will allow students to reproduce their own research, and enable others to reproduce their research, with as little as a single command.\n\n\n\nMore information\n\nTopics taught\n\nUnix shell: basics, scripting, running command-line programs\nR: Basics, data wrangling & visualization, Quarto, and specifics to omics data\nSupercomputer usage: basics, software, and running Slurm batch jobs\nVersion control with Git and GitHub\nReproducibility including project documentation with Markdown, project file organization, and data management\nRunning and building Nextflow pipelines\n\n\n\nCourse books\n\nAllesina S, Wilmes M (2019). Computing Skills for Biologists. Princeton UP.\nBuffalo V (2015). Bioinformatics Data Skills: Reproducible and Robust Research with Open Source Tools. O’Reilly Media, Inc.\n\n\n\nCarmenCanvas website\nIf you are a student in this course, you should also refer to the CarmenCanvas site for this course (link TBA).\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "ref/shell.html",
    "href": "ref/shell.html",
    "title": "shell",
    "section": "",
    "text": "TBA\n\n\n\n Back to top"
  },
  {
    "objectID": "ref/readings.html",
    "href": "ref/readings.html",
    "title": "An overview of course readings",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "ref/glossary.html",
    "href": "ref/glossary.html",
    "title": "Course Glossary",
    "section": "",
    "text": "Buffalo Book: Bioinformatics Data Skills (Buffalo 2015)\nCSB Book: Computing Skills for Biologists (Allesina & Wilmes 2019)\ngit Software for version control\nGithub A website that hosts git projects, which are known as repositories\nMarkdown A simple text markup language (think LaTeX or HTML but much simpler)\nOSC The Ohio Supercomputer Center\nshell The Unix shell is a command-line interface to your operating system that runs within a terminal.\nBash There are several shell flavors, and in this course, we will be working with the bash shell.\nNextflow\nSLURM The compute job scheduling system used at the Ohio Supercomputer Center\nUnix A family of operating systems that includes Mac and Linux, but not Windows\nCLI Command-line Interface – a software interface with which one interacts by typing commands (cf. GUI)\ndir Directory, which is Unix-speak for a folder on your computer\nGUI Graphical User Interface – a visual software interface with which one interacts by clicking\nHard-coding\n\n\n\n\n Back to top"
  },
  {
    "objectID": "ref/resources.html",
    "href": "ref/resources.html",
    "title": "Further learning resources",
    "section": "",
    "text": "This list of further resources is organized by the main topics covered in this course – see the Contents on the left-hand site."
  },
  {
    "objectID": "ref/resources.html#shell",
    "href": "ref/resources.html#shell",
    "title": "Further learning resources",
    "section": "1 Shell",
    "text": "1 Shell\n\n1.1 Books\nThe first two are available online at the OSU library:\n\nThe Linux Command Line: A Complete Introduction (William Shotts, 2019)\nLinux Command Line and Shell Scripting Bible (Christine Bresnahan, 2015)\nCommand Line Kung Fu: Bash Scripting Tricks, Linux Shell Programming Tips, and Bash One-liners (Jason Cannon, 2014)\n\n\n\n1.2 Online guides and tutorials\n\nNice collections of one-liners, mostly for bioinformatics\n\nBy Bonnie I-Man Ng\nBy Stephen Turner\nBy Tommy Ming\nBy Amy Williams\n\nThe Bash Guide by Maarten Billemont with separate very useful FAQ and pitfalls pages.\nBash Guide for Beginners by Machtel Garrels.\n\n\n\n1.3 Further reading\n\nTen simple rules for getting started with command-line bioinformatics (Brandies & Hogg 2021, PLoS Computational Biology)\nFive reasons why researchers should learn to love the command line (Perkel 2021, Nature)"
  },
  {
    "objectID": "ref/resources.html#reproducibility-and-best-practices",
    "href": "ref/resources.html#reproducibility-and-best-practices",
    "title": "Further learning resources",
    "section": "2 Reproducibility and best practices",
    "text": "2 Reproducibility and best practices\n\nGood enough practices in scientific computing (Wilson et al. 2017, PLoS Computational Biology)\nStreamlining data-intensive biology with workflow systems (Reiter et al. 2021, GigaScience)\nReproducible Research: A Retrospective (Peng & Hicks 2021, Annu Rev Public Health)\nTen Simple Rules for Reproducible Computational Research (Sandve et al. 2013, PLoS Computational Biology)\nThe plain person’s guide to plain text social science (Kieran Healy)"
  },
  {
    "objectID": "ref/resources.html#git",
    "href": "ref/resources.html#git",
    "title": "Further learning resources",
    "section": "3 Git",
    "text": "3 Git\n\n3.1 Books\nI wouldn’t necessarily recommend diving so deep into Git as to read a book about it, but this book provides an excellent reference, is quite accessible, and is freely available online:\n\nPro Git (Chacon & Straub, 2014)\n\n\n\n3.2 Online guides and tutorials\n\nGeneral\n\nAn overview of Git tutorials\nHappy Git and GitHub for the useR\nSomewhat R-centric, but a very accessible introduction to Git.\nGit best practices by Seth Robertson\nAtlassian Git tutorials\nAtlassian is behind Bitbucket, an alternative to GitHub that also hosts Git repositories, and its Git tutorials are very useful.\nA quick GitHub overview of some Git and GitHub functionality including branching and Pull Requests, and how to do these things in your browser at GitHub.\n\n\n\nInteractive practice\n\nGit-it – a small application to learn and practice Git and GitHub basics.\nVisualizing Git\nThese visualizations can help to get some intuition for Git. Note that at the prompt, you can only type Git commands and since there are no actual files involved, you can’t use git add – just commit straight away.\nLearn Git branching\n\n\n\nUndoing\n\nSome slides on undoing that we did not get to in our Git week.\nHow to undo (almost) anything with Git – by the GitHub blog\nOh Shit, Git!?! – by Katie Sylor-Miller\nGit flight rules – by Kate Hudson.\nCovers much more than undoing.\n\n\n\n\n3.3 Further reading\n\nExcuse me, do you have a moment to talk about version control? (Bryan 2017, PeerJ)\nTen Simple Rules for Taking Advantage of Git and GitHub (Perez-Riverol et al. 2016, PLoS Comutational Biology)"
  },
  {
    "objectID": "ref/resources.html#r",
    "href": "ref/resources.html#r",
    "title": "Further learning resources",
    "section": "4 R",
    "text": "4 R\n\n4.1 Books\nTBA\n\n\n4.2 Courses\nTBA"
  },
  {
    "objectID": "ref/resources.html#nextflow",
    "href": "ref/resources.html#nextflow",
    "title": "Further learning resources",
    "section": "5 Nextflow",
    "text": "5 Nextflow\nTBA\n\nFurther reading\n\nWorkflow systems turn raw data into scientific knowledge (Perkel 2019, Nature “Toolbox” feature)."
  },
  {
    "objectID": "ref/resources.html#miscellaneous",
    "href": "ref/resources.html#miscellaneous",
    "title": "Further learning resources",
    "section": "6 Miscellaneous",
    "text": "6 Miscellaneous\n\n6.1 Books similar to CSB and Buffalo\n\nA Primer for Computational Biology\nPractical Computing for Biologists (Haddock & Dunn, 2011)\n\n\n\n6.2 Genomics and applied bioinformatics\n\nA few courses on genomic data analysis with lots of great online material available:\n\nIntroduction to Bioinformatics and Computational Biology by Shirley Liu\nApplied Genomics course by Michael Schatz\nApplied Computational Genomics course by Aaron Quinlan\n\nSites with online material for many computational biology workshops:\n\nUC Davis Bioinformatics Core\nHarvard Chan Bioinformatics Core\nBioinformatics.ca\nSummer Institute in Statistical Genetics\nGenomics Curriculum of Data Carpentry\n\nRosalind – A website with bioinformatics exercises"
  },
  {
    "objectID": "week01/w1_ua_osc-setup.html",
    "href": "week01/w1_ua_osc-setup.html",
    "title": "Pre-course assignment: OSC access",
    "section": "",
    "text": "Overview\nBefore Thursday’s session, you should make sure that you have access to the Ohio Supercomputer Center (OSC), and the OSC Project for this course (PAS2880).\n\n\nBackground\nYou will do much of your work during this course through your browser at the Ohio Supercomputer Center (OSC). To access OSC, you will need to have an account. This course has its own OSC “Project”, and membership of this specific project will allow you to access our shared files and to reserve computing resources.\n\n\nWhat you should do\nAfter completing the pre-course survey (LINK TBA), you will receive an invitation email from OSC referencing the course’s project number PAS2880:\n\nIf you don’t have an OSC account yet\nFollow the instructions in the email to sign up for OSC and accept the invitation.\nIf you already have an OSC account\nThe email will likely tell you that you have been added to the project and don’t have to do anything.\nIn either case, check whether you can log in\nGo to https://ondemand.osc.edu and log in by typing your username and password on the left-hand side of the page. If you just created your account, it may take up to half an hour or so before you can log in.\n\nIf you have any questions about this or run into problems, don’t hesitate to contact Jelmer.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "week01/w1_overview.html#content-overview",
    "href": "week01/w1_overview.html#content-overview",
    "title": "Week 1: Intro to the course, omics data, and OSC",
    "section": "1 Content overview",
    "text": "1 Content overview\nThis week, you will get an overview of the course, and introductions to omics data and the Ohio Supercomputer Center (OSC). More specifically, you will learn:\n\nCourse intro (Tuesday class)\n\nWhat to expect from this course\nWhich tools and languages we will use\nWhat is expected of you during this course\nGet up to speed on the infrastructure of the course\n\n\n\nOmics data (Tuesday/Thursday class)\n\nWhat omics data is\nAbout the main high-throughput sequencing technologies\nHow Illumina sequence libraries are structured\nWhat reference genomes are and are good for\nWhat the main sequence file types are and what FASTA and FASTQ files look like\n\n\n\nOSC Intro (Thursday class)\n\nWhat a supercomputer is and why they are useful\nWhat resources the Ohio Supercomputer Center (OSC) provides\nHow to access OSC resources through its OnDemand webportal"
  },
  {
    "objectID": "week01/w1_overview.html#assignments-and-exercises",
    "href": "week01/w1_overview.html#assignments-and-exercises",
    "title": "Week 1: Intro to the course, omics data, and OSC",
    "section": "2 Assignments and exercises",
    "text": "2 Assignments and exercises\n\nUngraded assignments\n\nPre-course survey - link TBA\nOhio Supercomputer Center account setup (This must be done by Thursday’s class)\n\nNo exercises this week."
  },
  {
    "objectID": "week01/w1_overview.html#readings",
    "href": "week01/w1_overview.html#readings",
    "title": "Week 1: Intro to the course, omics data, and OSC",
    "section": "3 Readings",
    "text": "3 Readings\n\nRequired readings\n\nPoinsignon et al. 2023 Working with Omics Data: An Interdisciplinary Challenge at the Crossroads of Biology and Computer Science\nLee 2023 The Principles and Applications of High-Throughput Sequencing Technologies\n\n\n\nOptional readings\nBook chapters\nThroughout the course, chapters from these two books are optional readings for this course. Both can be accessed online for free with OSU credentials via the links below.\n\nBioinformatics Data Skills (Buffalo 2015) (“Buffalo” for short)\nThis week: Preface and Chapter 1 - How to Learn Bioinformatics\nComputing Skills for Biologists (Allesina & Wilmes 2019) (“CSB” for short)\nThis week: Chapter 0 - Building a Computing Toolbox (Introduction & Section 0.1 only)\n\nPapers\nWhat a list 😳! Well, the idea is that you can browse through these titles and see if any of these spark your interest — many of these present omics and HTS data in a more field-specific light, e.g. plant pathology.\n\nvon der Heyden et al. 2025 Advancing species conservation and management through omics tools\nNizamani et al. 2023 High-throughput sequencing in plant disease management: a comprehensive review of benefits, challenges, and future perspectives\nAragona et al. 2022 New-generation sequencing technology in diagnosis of fungal plant pathogens: A dream comes true?\nMahmood et al. 2022 Multi-omics revolution to promote plant breeding efficiency\nRai et al. 2019 A new era in plant functional genomics TBA OSU LINK\nPinto & Bhatt 2024 Sequencing-based analysis of microbiomes\nManzoni et al. 2018 Genome, transcriptome and proteome: the rise of omics data and their integration in biomedical sciences\nvan Dijk et al. 2023 Genomics in the long-read sequencing era"
  },
  {
    "objectID": "week01/w1_03_osc.html#goals-for-this-session",
    "href": "week01/w1_03_osc.html#goals-for-this-session",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "1 Goals for this session",
    "text": "1 Goals for this session\nThis session will provide an introduction to high-performance computing in general and to the Ohio Supercomputer Center (OSC) more specifically.\nThis is only meant as a brief overview to give context about the working environment that we will start using next week: we will do all of our coding and computing at OSC during this course. During the course, you’ll learn a lot more about most topics touched on in this page — week 6 and 7 in particular focus on OSC."
  },
  {
    "objectID": "week01/w1_03_osc.html#high-performance-computing",
    "href": "week01/w1_03_osc.html#high-performance-computing",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "2 High-performance computing",
    "text": "2 High-performance computing\nA supercomputer (also known as a “compute cluster” or simply a “cluster”) consists of many computers that are connected by a high-speed network, and that can be accessed remotely by its users. In more general terms, supercomputers provide high-performance computing (HPC) resources.\nThis is what Cardinal, one of the OSC supercomputers, physically looks like:\n\n\n\nThe Cardinal OSC cluster\n\n\nHere are some possible reasons to use a supercomputer instead of your own laptop or desktop:\n\nYour analyses take a long time to run, need large numbers of processors, or a large amount of memory.\nYou need to run an analysis many times.\nYou need to store a lot of data.\nYour analyses require software available only for the Linux operating system, but you have Windows.\nYour analyses require specialized hardware, such as GPUs (Graphical Processing Units).\n\nWhen you’re working with omics data, many of these reasons typically apply. This can make it hard or sometimes simply impossible to run all your analyses on your personal workstation, and supercomputers provide a solution.\n\n\nThe Ohio Supercomputer Center (OSC)\nThe Ohio Supercomputer Center (OSC) is a facility provided by the state of Ohio. It has several supercomputers, lots of storage space, and an excellent infrastructure for accessing these resources.\n\n\n\n\n\n\n\nOSC websites and “Projects”\n\n\n\nOSC has three main websites — we will mostly or only use the first:\n\nhttps://ondemand.osc.edu: A web portal to use OSC resources through your browser (login needed).\nhttps://my.osc.edu: Account and project management (login needed).\nhttps://osc.edu: General website with information about the supercomputers, installed software, and usage.\n\n\nAccess to OSC’s computing power and storage space goes through OSC “Projects”:\n\nA project can be tied to a research project or lab, or be educational like this course’s project, PAS2880.\nEach project has a budget in terms of “compute hours” and storage space1.\nAs a user, it’s possible (and common) to be a member of multiple different projects."
  },
  {
    "objectID": "week01/w1_03_osc.html#the-structure-of-a-supercomputer-center",
    "href": "week01/w1_03_osc.html#the-structure-of-a-supercomputer-center",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "3 The structure of a supercomputer center",
    "text": "3 The structure of a supercomputer center\n\n3.1 Terminology\nLet’s start with some (super)computing terminology, going from smaller things to bigger things:\n\nNode\nA single computer that is a part of a supercomputer.\nSupercomputer / Cluster\nA collection of connected computers. OSC currently has three: “Ascend”, “Cardinal”, and “Pitzer”.\nSupercomputer Center\nA facility like OSC that has one or more supercomputers.\n\n\n\n3.2 Supercomputer components\nWe can think of a supercomputer as having three main parts:\n\nFile Systems: Where files are stored (these are shared between the OSC supercomputers!)\nLogin Nodes: The handful of computers everyone shares after logging in\nCompute Nodes: The many computers you can reserve to run your analyses\n\nWe wil briefly discuss these below, and come back to them in more detail later in the course.\n\n\n\nThe structure of a supercomputer with three main components: file systems, login nodes, and compute nodes\n\n\n\n\nFile systems\nOSC has several distinct file systems:\n\n\n\n\n\n\n\n\nFile system\nLocated within “path”\nMain purpose\n\n\n\n\nProject\n/fs/ess/\nMain storage location\n\n\nScratch\n/fs/scratch/\nAdditional, temporary storage\n\n\nHome\n/users/\nGeneral, personal files not tied to research projects or courses\n\n\n\nDuring the course, we’ll work in the project folder of the course’s OSC Project PAS2880: /fs/ess/PAS2880.\n\n\n\n\n\n\nPaths?\n\n\n\nPaths, like those shown in the table above, specify the locations of folders and files on a computer. You will learn more about them in the next few weeks.\n\n\n\n\n\nLogin Nodes\nLogin nodes are set aside as an initial landing spot for everyone who logs in to a supercomputer. There are only a handful of them on each supercomputer, they are shared among everyone, and they cannot be “reserved”.\nAs such, login nodes are meant only to do things like organizing your files and creating scripts for compute jobs, and are not meant for any serious computing, which should be done on the compute nodes.\n\n\n\nCompute Nodes\nData processing and analysis is done on compute nodes. You can only use compute nodes after putting in a request for resources (a “job”). The Slurm job scheduler, which we will learn to use later in this course, will then assign resources to your request.\n\n\n\n\n\n\nWhat works differently on a supercomputer like at OSC? (Click to expand)\n\n\n\n\n\nCompared to command-line computing on a laptop or desktop, a number of aspects are different when working on a supercomputer like at OSC. We’ll learn much more about these later on in the course, but here is an overview:\n\n“Non-interactive” computing is common\nIt is common to write and “submit” scripts to a queue instead of running programs interactively.\nSoftware\nYou generally can’t install “the regular way”, and a lot of installed software needs to be “loaded”.\nOperating system\nSupercomputers run on the Linux operating system.\nLogin versus compute nodes\nAs mentioned, the nodes you end up on after logging in are not meant for heavy computing and you have to request access to “compute nodes” to run most analyses."
  },
  {
    "objectID": "week01/w1_03_osc.html#osc-ondemand",
    "href": "week01/w1_03_osc.html#osc-ondemand",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "4 OSC OnDemand",
    "text": "4 OSC OnDemand\nThe OSC OnDemand web portal allows you to use a web browser to access OSC resources such as:\n\nA file browser where you can also create and rename folders and files, etc.\nA Unix shell\n“Interactive Apps”: programs such as RStudio and VS Code\n\n Go to https://ondemand.osc.edu and log in (use the boxes on the left-hand side).\nOnce logged in, you should see a landing page similar to the one below:\n\n\n\nThe OSC OnDemand landing page\n\n\nWe will now go through some of the dropdown menus in the blue bar along the top.\n\n\n4.1 Files menu\nHovering over the Files dropdown menu gives a list of directories that you have access to. If your account is brand new, and you were added to PAS2880, you should only have three directories listed:\n\nA Home directory (starts with /users/)\nThe PAS2880 project’s “project” directory (/fs/ess/PAS2880)\nThe PAS2880 project’s “scratch” directory (/fs/scratch/PAS2880)\n\nYou will only ever have one Home directory at OSC, but for every additional project you are a member of, you should usually see additional /fs/ess and /fs/scratch directories appear.\n Click on our focal directory /fs/ess/PAS2880.\nOnce there, you should see whichever directories and files are present at the selected location, and you can click on the directories to explore the contents further:\n\n\n\nThe OnDemand file browser\n\n\nThis interface is much like the file browser on your own computer, so you can also create, delete, move and copy files and folders, and even upload (from your computer to OSC) and download (from OSC your computer) files2 — see the buttons across the top.\n\n\n\n4.2 Interactive Apps menu\nWe can access programs with Graphical User Interfaces (GUIs; point-and-click interfaces) via the Interactive Apps dropdown menu:\n\n\n\nOptions in the OSC Ondemand Interactive Apps dropdown menu\n\n\nNext week, we will start using the VS Code text editor, which is listed here as Code Server. Later, we will also use RStudio Server to code in R.\n\n\n\n4.3 Clusters menu\n\nSystem Status\nIn the “Clusters” dropdown menu, click on the item at the bottom, “System Status”:\n\n\n\nThe Clusters dropdown menu\n\n\nThis page shows an overview of the live, current usage of the two clusters — that can be interesting to get a good idea of the scale of the supercomputer center, which cluster is being used more, what the size of the “queue” (which has jobs waiting to start) is, and so on.\n\n\n\nA screenshot of the “System Status” page showing live cluster usage.\n\n\n\n\nShell Acccess\nInteracting with a supercomputer is most commonly done using a so-called “Unix shell”, which is a command-line interface to a computer that we’ll use throughout the course. Still under the Clusters dropdown menu (see the screenshot above), you can access a Unix shell on Ascend, Cardinal, or Pitzer.\n Select a shell on the Cardinal cluster (Cardinal Shell Access item).\nThis will open a new browser tab, where the bottom of the page looks like this:\n\n\n\nA Unix shell on the Cardinal cluster as accessed through OSC OnDemand. OSC prints some information like file usage quota, and at the bottom of the screen is the prompt where you can type commands.\n\n\nWe’ll return to this Unix shell next week."
  },
  {
    "objectID": "week01/w1_03_osc.html#footnotes",
    "href": "week01/w1_03_osc.html#footnotes",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n But we don’t have to pay anything for educational projects like this one. Otherwise, for OSC’s rates for academic research, see this page.↩︎\nThough this is not meant for large (&gt;1 GB) transfers. Different methods are available — we’ll talk about those later on.↩︎"
  },
  {
    "objectID": "week06/w6_1_osc.html",
    "href": "week06/w6_1_osc.html",
    "title": "A closer look at OSC",
    "section": "",
    "text": "In this short session, we will touch on some aspects of the Ohio Supercomputer Center (OSC) that we did not talk about during our initial OSC introduction in week 1 of this course."
  },
  {
    "objectID": "week06/w6_1_osc.html#osc-projects",
    "href": "week06/w6_1_osc.html#osc-projects",
    "title": "A closer look at OSC",
    "section": "OSC Projects",
    "text": "OSC Projects\nIn this course, we are exclusively using the course’s OSC Project PAS2700. When you use OSC for your own research project, you would use a different OSC Project, one that’s likely either specific to a research project or grant, or that is your lab/PI’s general use OSC Project.\nGenerally, only PIs request OSC projects, and they typically manage them as well. OSC has this page with more information on how to do so. Whoever manages an OSC Project can add both existing OSC users and new users to the Project. Anyone added to an OSC Project will have access to the project’s directories, and will be able specify this Project when issuing compute node resource requests.\n\n\n\n\n\n\nBilling\n\n\n\nOSC will bill OSC Projects (not individual users), and only for the following two things:\n\nFile storage in the Project Storage file system\nCompute node usage per “core hour” (e.g. using 2 cores for 2 hours = 4 core hours)\n\nThe prices for academic usage are quite low (see this page for specifics), and at OSU, they are often covered at the department level so individual PIs often do not have to directly pay for this at all."
  },
  {
    "objectID": "week06/w6_1_osc.html#hierarchical-components",
    "href": "week06/w6_1_osc.html#hierarchical-components",
    "title": "A closer look at OSC",
    "section": "Hierarchical components",
    "text": "Hierarchical components\nIn week 1, you learned that a supercomputer center like OSC typically has multiple supercomputers, each of which in turn consists of many nodes. But I omitted a fourth “level” in this hierarchy:\n\nCore / Processor / CPU / Thread — Components of a computer/node that can each (semi-)independently be asked to perform a computing task like running a bioinformatics program. With most bioinformatics programs, you can also use multiple cores for a single run, which can speed things up considerably. While these four terms are not technically all synonyms, we can treat them as such for our purposes."
  },
  {
    "objectID": "week06/w6_1_osc.html#file-systems",
    "href": "week06/w6_1_osc.html#file-systems",
    "title": "A closer look at OSC",
    "section": "File systems",
    "text": "File systems\nLet’s expand our earlier list of file systems, and look at this table in a bit more detail:\n\n\n\n\n\n\n\n\n\n\n\nFile system\nLocated within\nQuota\nBacked up?\nAuto-purged?\nOne for each…\n\n\n\n\nHome\n/users/\n500 GB / 1 M files\nYes\nNo\nUser\n\n\nProject\n/fs/ess/\nFlexible\nYes\nNo\nOSC Project\n\n\nScratch\n/fs/scratch/\n100 TB\nNo\nAfter 90 days\nOSC Project\n\n\nCompute\n$TMPDIR\n1 TB\nNo\nAfter job completes\nCompute job\n\n\n\n\nWe’ve been working in the “Project dir” (/fs/ess/) for this course’s OSC project PAS2700. When you use OSC for your own research project with a different OSC Project, I would recommend that you also work mostly in the Project (/fs/ess/) dir, which offers backed up, permanent and flexible amounts of storage.\nOne other hand, Scratch dirs are temporary and not backed up, while the storage space of Home dirs is limited and cannot be expanded1. Compute storage space is linked to compute jobs and extremely fleeting: as soon as the compute “job” in question has stopped, these files will be deleted. So when are non-Project file systems useful?\n\nYour Home dir can e.g. be useful for files that you use across projects, like some software.\nScratch has the advantages of having effectively unlimited space and much faster data read and write (“I/O”) speed than Home and Project space. It therefore regularly makes sense to run analyses on Scratch, and copy over files that you will continue to need.\nCompute storage has even faster I/O so can be useful for very I/O-intensive jobs — but using it requires some extra code in your script2 and for better or worse, I personally end up using this very rarely.\n\n\n\n\n\n\n\nAccessing back-ups\n\n\n\nHome and Project directories are backed up daily. You don’t have direct access to the backups, but if you’ve accidentally deleted important files, you can email OSC to have them restore your files to the way they were on a specific date.\n\n\n\n\n\n\n\n\nAccessing files on different clusters\n\n\n\nFile systems are shared among OSC’s clusters, such that you can access your files in the exact same way regardless of which cluster you have connected to."
  },
  {
    "objectID": "week06/w6_1_osc.html#compute-nodes",
    "href": "week06/w6_1_osc.html#compute-nodes",
    "title": "A closer look at OSC",
    "section": "Compute nodes",
    "text": "Compute nodes\nCompute nodes come in different shapes and sizes:\n\n“Standard nodes” are by far the most numerous (e.g., Owens has 648 and Pitzer has 564) and even those vary in size, from 28 cores per node (Owens) to 48 cores per node (the “expansion” part of Pitzer).\nSome examples of other types of nodes are nodes with extra memory (largemem and hugemem) and nodes that provide access to GPUs (Graphical Processing Units) rather than CPUs.\n\nStandard nodes are used by default and these will serve you well for the majority of omics analysis. But you may occasionally need a different type of node, such as for genome or transcriptome assembly (you’ll need need nodes with a lot of memory) or Oxford Nanopore sequence data base-calling (you’ll need GPUs).\n\n\n\n\n\n\nMemory versus storage\n\n\n\nWhen we talk about “memory”, this refers to RAM: the data that your computer has actively “loaded” or in use. For example, if you play a computer game or have many browser tabs open, your computer’s memory will be heavily used. Genomics programs sometimes load all the input data from disk into memory to allow for fast access, or they will hold a huge assembly graph in memory, and therefore may need a lot of memory as well.\nDon’t confuse memory with file storage, the data that is on disk, some of which may have been unused for years.\n\n\n\n\nUsing compute nodes\nYou can use compute nodes by putting in a request for resources, such as the number of nodes, cores, and for how long you will need them. These requests result in “compute jobs” (also simply called “jobs”).\nBecause many different users are sending such requests all the time, there is software called a job scheduler (specifically, Slurm in case of OSC) that considers each request and assigns the necessary resources to the job as they become available. We’ve already been running compute jobs by running VS Code via OnDemand, and we will talk about submitting scripts as “batch jobs” this week."
  },
  {
    "objectID": "week06/w6_1_osc.html#putting-it-together",
    "href": "week06/w6_1_osc.html#putting-it-together",
    "title": "A closer look at OSC",
    "section": "Putting it together",
    "text": "Putting it together\nLet’s take a look at the specs for Owens now that we understand a supercomputer’s components a bit better:\n\n\n\n\n\n\n\n\n\n\n\n\nCiting and contacting OSC\n\n\n\n\nWhen you use OSC, it’s good practice to acknowledge and cite OSC in your papers, see their citation page.\nFor many questions such as if you have problems with your account, have problems installing or using specific software, or don’t understand why your jobs keep failing, you can email OSC at oschelp@osc.edu. They are usually very quick to respond!"
  },
  {
    "objectID": "week06/w6_1_osc.html#footnotes",
    "href": "week06/w6_1_osc.html#footnotes",
    "title": "A closer look at OSC",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n And file sharing / collaborating is also a bit more difficult with home dirs.↩︎\n Copying of files back-and-forth, and making sure your results are not lost upon some kind of failure.↩︎"
  },
  {
    "objectID": "week06/w6_2_data-management.html#overview",
    "href": "week06/w6_2_data-management.html#overview",
    "title": "Data management at OSC and beyond",
    "section": "Overview",
    "text": "Overview"
  },
  {
    "objectID": "week06/w6_2_data-management.html#managing-your-data-and-results",
    "href": "week06/w6_2_data-management.html#managing-your-data-and-results",
    "title": "Data management at OSC and beyond",
    "section": "Managing your data and results",
    "text": "Managing your data and results\n\nKeeping back-ups of raw data\nProject versus scratch dirs (and avoid your Home dir for research projects)"
  },
  {
    "objectID": "week06/w6_2_data-management.html#sharing-your-data-results-and-code",
    "href": "week06/w6_2_data-management.html#sharing-your-data-results-and-code",
    "title": "Data management at OSC and beyond",
    "section": "Sharing your data, results, and code",
    "text": "Sharing your data, results, and code\n\nRaw sequences to NCBI\nSome results can also go to NCBI, e.g. genome assemblies and annotations, GEO\nOther results to e.g. Dryad / Zenodo\nScripts via GitHub"
  },
  {
    "objectID": "week06/w6_2_data-management.html#file-transfer-to-and-from-osc",
    "href": "week06/w6_2_data-management.html#file-transfer-to-and-from-osc",
    "title": "Data management at OSC and beyond",
    "section": "File transfer to and from OSC",
    "text": "File transfer to and from OSC\n\nOverview of options\nIn week 1, you were shown that you can upload and download files from OSC in the OnDemand Files menu, but that’s only suitable for relatively small transfers. Here is a list of methods to transfer files between your computer and OSC:\n\n\n\n\n\n\n\n\n\n\nMethod\nTransfer size\nCLI or GUI\nEase of use\nFlexibility/options\n\n\n\n\nOnDemand Files menu\nsmaller (&lt;1GB)\nGUI\nEasy\nLimited\n\n\nRemote transfer commands\nsmaller (&lt;1GB)\nCLI\nModerate\nExtensive\n\n\nSFTP\nlarger (&gt;1GB)\nEither\nModerate\nExtensive\n\n\nGlobus\nlarger (&gt;1GB)\nGUI\nModerate\nExtensive\n\n\n\n\nFor more details, see the self-study section at the bottom of this page.\n\n\n\n\nFileZilla: a GUI-based SFTP client\nHere, we’ll go over what is probably overall the most convenient and versatile way to transfer files to and from OSC, since it works for transfers of any size, and is easy and quick: file transfer with a GUI-based SFTP client. There are a number of such programs, but I chose to show you FileZilla because it works on all operating systems1.\n\nGo to the FileZilla download page — it should automatically display a big green download button for your operating system: click that and install and open the program.\nTo connect to OSC, find the Site Manager: click File in the top menu bar and then Site Manager2. In the Site Manager:\n\nProtocol: select “SFTP - SSH File Transfer Protocol”.\nHost: type sftp.osc.edu (and you can leave the Port box empty).\nLogon Type: Make sure “Normal” is selected.\nUser: Type you OSC user name.\nPassword: Type or copy your OSC password.\nClick the “Connect” button at the bottom to connect to OSC.\n\n\n\n\n\n\n\n\nOnce connected, your main screen is split with a local file explorer on the left, and a remote file explorer on the right:\n\n\n\n\n\n\n\nTransferring dirs and files in either direction is as simple as dragging and dropping them to the other side!\nYour default location at OSC is your home dir in /users/, but you can type a path in the top bar to e.g. go to /fs/ess/PAS2700/users:"
  },
  {
    "objectID": "week06/w6_2_data-management.html#downloading-files-at-the-command-line",
    "href": "week06/w6_2_data-management.html#downloading-files-at-the-command-line",
    "title": "Data management at OSC and beyond",
    "section": "Downloading files at the command line",
    "text": "Downloading files at the command line\nWhen you need to get large files from public repositories like NCBI (e.g. reference genome files, raw reads from the SRA), I would recommend not to download these to your own computer and then transfer them to OSC, but to download them directly to OSC using commands.\n\nwget\nYou can download files from any URL that will allow you using the wget command. It has many options, but basic usage is quite simple:\n# This will download the file at the URL to your current working dir\nwget &lt;URL&gt;\nAs an example, say that we want to download the reference genome FASTA and GTF files for Culex pipiens, the focal species of the Garrigos et al. RNA-seq data set that we’ve been working with. The files for that genome can be found at this NCBI FTP webpage.\nYou can right-click on any of the files and then click “Copy Link Address” (or something similar, depending on your browser) — let’s copy the URL to the assembly FASTA (.fna.gz) file. Then type wget and Space in your terminal and paste the URL:\nwget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/016/801/865/GCA_016801865.2_TS_CPP_V2/GCA_016801865.2_TS_CPP_V2_genomic.fna.gz\n--2024-04-13 13:33:16--  https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/016/801/865/GCA_016801865.2_TS_CPP_V2/GCA_016801865.2_TS_CPP_V2_genomic.fna.gz\nResolving ftp.ncbi.nlm.nih.gov (ftp.ncbi.nlm.nih.gov)... 130.14.250.12, 130.14.250.11, 2607:f220:41e:250::12, ...\nConnecting to ftp.ncbi.nlm.nih.gov (ftp.ncbi.nlm.nih.gov)|130.14.250.12|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 172895917 (165M) [application/x-gzip]\nSaving to: ‘GCA_016801865.2_TS_CPP_V2_genomic.fna.gz’\n\n100%[=====================================================================================================================&gt;] 172,895,917 51.7MB/s   in 3.2s   \n\n2024-04-13 13:33:20 (51.7 MB/s) - ‘GCA_016801865.2_TS_CPP_V2_genomic.fna.gz’ saved [172895917/172895917]\nLet’s list our downloaded file:\nls -lh\n-rw-r--r--  1 jelmer PAS0471 165M Aug  5  2022 GCA_016801865.2_TS_CPP_V2_genomic.fna.gz\n\n\n\n\n\n\n\nAnother commonly used downloading command is curl\n\n\n\nwget and curl have similar functionality, so you don’t really need to learn to use both of them.\n\n\n\n\n\nOther genomic downloads\n\nFor reference genome downloads, NCBI also has a relatively new and very useful portal called “datasets” that also includes a CLI tools of the same name. This is especially useful if you want to download multiple or many genomes — for example, you can download all available genomes of a certain (say, bacterial) genus with a single command.\nFor downloads from the Sequence Read Archive (SRA), which contains high-throughput sequencing reads, you can use tools like fasterq-dump and dl-fastq. Also handy is the simple SRA explorer website, where you can enter an NCBI accession number and it will give you download links."
  },
  {
    "objectID": "week06/w6_2_data-management.html#file-permissions",
    "href": "week06/w6_2_data-management.html#file-permissions",
    "title": "Data management at OSC and beyond",
    "section": "File permissions",
    "text": "File permissions\nFile “permissions” are the types of things (e.g. reading, writing) that different groups of users (creator, group, anyone else) are permitted to do with files and dirs.\nThere are a couple of reasons you may occasionally need to view and modify file permissions:\n\nYou may want to make your data read-only\nYou may need to share files with other users at OSC\n\n\n\nViewing file permissions\nTo show file permissions, use ls with the -l (long format) option that we’ve seen before. The command below also uses the -a option to show all files, including hidden ones (and -h to show file sizes in human-readable format):\n\n\n\n\n\nHere is an overview of the file permission notation in ls -l output:\n\n\n\n\n\nIn the two lines above:\n\nrwxrwxr-x means:\nread + write + execute permissions for both the owner (first rwx) and the group (second rwx), and read + execute but not write permissions for others (r-x at the end).\nrw-rw-r-- means:\nread + write but not execute permissions for both the owner (first rw-) and the group (second rw-), and only read permissions for others (r-- at the end).\n\nLet’s create a file to play around with file permissions:\n# Create a test file\ntouch testfile.txt\n\n# Check the default permissions\nls -l testfile.txt\n-rw-rw----+ 1 jelmer PAS2700 0 Mar  7 13:36 testfile.txt\n\n\n\nChanging file permissions\nThis can be done in two different ways with the chmod command. Here, we’ll focus on the method with = (set permission to), + (add permission), and - (remove permission).\nFor example, to add read (r) permissions for all (a):\n# chmod &lt;who&gt;+&lt;permission-to-add&gt;:\nchmod a+r testfile.txt\n\nls -l testfile.txt\n-rw-rw-r--+ 1 jelmer PAS0471 0 Mar  7 13:40 testfile.txt\nTo set read + write + execute (rwx) permissions for all (a):\n# chmod &lt;who&gt;=&lt;permission-to-set&gt;`:\nchmod a=rwx testfile.txt\n\nls -l testfile.txt\n-rwxrwxrwx+ 1 jelmer PAS2700 0 Mar  7 13:36 testfile.txt\nTo remove write (w) permissions for others (o):\n# chmod &lt;who&gt;-&lt;permission-to-remove&gt;:\nchmod o-w testfile.txt\n\nls -l testfile.txt\n-rwxrwxr-x+ 1 jelmer PAS2700 0 Mar  7 13:36 testfile.txt\n\n\n\n\n\n\nAlternative: changing file permissions with numbers (Click to expand)\n\n\n\n\n\nYou can also use a series of 3 numbers (for user, group, and others) to set permissions, where each number can take on the following values:\n\n\n\nNr\nPermission\nNr\nPermission\n\n\n\n\n1\nx\n5\nr + x\n\n\n2\nw\n6\nr + w\n\n\n4\nr\n7\nr + w + x\n\n\n\nFor example, to set read + write + execute permissions for all:\nchmod 777 testfile.txt\nTo set read + write + execute permissions for yourself, and only read permission for the group and others:\nchmod 744 file.txt\n\n\n\n\n\n\nMaking your data read-only\nSo, if you want to make your raw data (here: the files in the data/fastq dir) read-only, you can use:\n\nSet only read permissions for everyone:\nchmod a=r data/fastq/*\nTake away write permissions for yourself (no-one else should have it by default):\nchmod u-w data/fastq/*\n\n\n\n\n\n\n\n\nRead/execute permissions for directories\n\n\n\nOne tricky and confusing aspect of file permissions is that to list a directory’s content, you need execute permissions for the dir! This is something to take into account when you want to grant others access to your project e.g. at OSC.\nTo set execute permissions for everyone but only for dirs throughout a dir hierarchy, use an X (uppercase x):\nchmod -R a+X my-dir\n\n\n\nAfter running one or both of the above commands, let’s check the permissions:\nls -l data/fastq\ntotal 0\n-r--r--r--+ 1 jelmer PAS0471 0 Mar  7 13:41 sample001_R1.fastq\n-r--r--r--+ 1 jelmer PAS0471 0 Mar  7 13:41 sample001_R2.fastq\n-r--r--r--+ 1 jelmer PAS0471 0 Mar  7 13:41 sample002_R1.fastq\n-r--r--r--+ 1 jelmer PAS0471 0 Mar  7 13:41 sample002_R2.fastq\n# [...output truncated...]\nWhat happens when we try to remove write-protected files?\nrm data/fastq/*fastq\nrm: remove write-protected regular empty file ‘data/fastq/sample001_R1.fastq’?\nYou’ll be prompted for every file! If you answer y (yes), you can still remove them. (But note that people other than the file’s owners cannot overried file permissions; only if they are system administrators.)"
  },
  {
    "objectID": "week06/w6_2_data-management.html#using-symbolic-links",
    "href": "week06/w6_2_data-management.html#using-symbolic-links",
    "title": "Data management at OSC and beyond",
    "section": "Using symbolic links",
    "text": "Using symbolic links\nFor example to use files across projects - recall week 2 - RBA\n\nSingle files\nA symbolic (or soft) links only links to the path of the original file, whereas a hard link directly links to the contents of the original file. Note that modifying a file via either a hard or soft link will modify the original file.\nCreate a symlink to a file using ln -s &lt;source-file&gt; [&lt;link-name&gt;]:\n# Only provide source =&gt; create link of the same name in the wd:\nln -s /fs/ess/PAS2700/share/garrigos/data/fastq/ERR10802863_R1.fastq.gz\n  \n# The link can also be given an arbitrary name/path:\nln -s /fs/ess/PAS2700/share/garrigos/data/fastq/ERR10802863_R1.fastq.gz shared-fastq.fastq.gz\n\n\n\n\n\n\nUse an absolute path to refer to the source file when creating links\n\n\n\nAt least at OSC, you have to use an absolute path for the source file(s), or the link will not work. The $PWD environment variable, which contains your current working directory can come in handy to do so:\n# (Fictional example, don't run this)\nln -s $PWD/shared-scripts/align.sh project1/scripts/\n\n\n\n\n\nMultiple files\nLink to multiple files in a directory at once:\n# (Fictional example, don't run this)\nln -s $PWD/shared_scripts/* project1/scripts/ \nLink to a directory:\n# (Fictional example, don't run this)\nln -s $PWD/shared_scripts/ project1/scripts/\nln -s $PWD/shared_scripts/ project1/scripts/ln-shared-scripts\n\n\n\n\n\n\nBe careful not to remove source files!\n\n\n\nBe careful when linking to directories: you are creating a point of entry to the original dir. Therefore, even if you enter via the symlink, you are interacting with the original files.\nThis means that a command like the following would remove the original directory!\nrm -r symlink-to-dir\nInstead, use rm symlink-to-dir (the link itself is a file, not a dir, so you don’t need -r!) or unlink symlink-to-dir to only remove the link.\n\n\n\n\n Exercise: Creating symbolic links\n\nCreate a symbolic link in your $HOME dir that points to your personal dir in the project dir (/fs/ess/PAS2700/users/$USER).\nIf you don’t provide a name for the link, it will be your username (why?), which is not particularly informative about its destination. Therefore, give it a name that makes sense to you, like PLNTPTH6193-SP24 or pracs-sp24.\n\n\n\nClick for the solution\n\nln -s /fs/ess/PAS1855/users/$USER ~/PLNTPTH6193-SP24\n\n\nWhat would happen if you do rm -rf ~/PLNTPTH8300-SP21? Don’t try this.\n\n\n\nClick for the solution\n\nThe content of the original dir will be removed."
  },
  {
    "objectID": "week06/w6_2_data-management.html#self-study-check-file-integrity-with-checksums",
    "href": "week06/w6_2_data-management.html#self-study-check-file-integrity-with-checksums",
    "title": "Data management at OSC and beyond",
    "section": "Self study: check file integrity with checksums",
    "text": "Self study: check file integrity with checksums\nTBA"
  },
  {
    "objectID": "week06/w6_2_data-management.html#footnotes",
    "href": "week06/w6_2_data-management.html#footnotes",
    "title": "Data management at OSC and beyond",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n And has a more intuitive user interface than CyberDuck, another very commonly used program that works both on Mac and Windows.↩︎\n There is a also an icon in the far top-left to access this menu. Additionally, there is a “Quickconnect” bar but I’ve not managed to connect to OSC with that.↩︎"
  },
  {
    "objectID": "week06/w6_exercises.html",
    "href": "week06/w6_exercises.html",
    "title": "Week 6 Exercises: OSC data and software",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "week08/w8_exercises.html",
    "href": "week08/w8_exercises.html",
    "title": "Week 8 Exercises",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "week09/w9_overview.html",
    "href": "week09/w9_overview.html",
    "title": "Week 9: Nextflow pipelines I",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "week09/w9_1_pipeline-intro.html#overview",
    "href": "week09/w9_1_pipeline-intro.html#overview",
    "title": "Workflow management systems",
    "section": "Overview",
    "text": "Overview\nTBA\n\n\n\nPerkel 2019 - https://www.nature.com/articles/d41586-019-02619-z\n\n\nTwo quotes from this article:\n\nTypically, researchers codify workflows using general scripting languages such as Python or Bash. But these often lack the necessary flexibility.\n\n\nWorkflows can involve hundreds to thousands of data files; a pipeline must be able to monitor their progress and exit gracefully if any step fails. And pipelines must be smart enough to work out which tasks need to be re-executed and which do not."
  },
  {
    "objectID": "week09/w9_1_pipeline-intro.html#workflow-management-systems",
    "href": "week09/w9_1_pipeline-intro.html#workflow-management-systems",
    "title": "Workflow management systems",
    "section": "1 Workflow management systems",
    "text": "1 Workflow management systems\nPipeline/workflow tools, often called “workflow management systems” in full, provide ways to formally describe and execute pipelines. Advantages of these tools are improved automation, flexibility, portability, and scalability1.\n\nAutomation\n\nDetect & rerun upon changes in input files and failed steps.\nAutomate Slurm job submissions.\nIntegration with software management.\nEasily run for other data sets.\n\n\n\n\nFlexibility, portability, and scalability\nThis is due to these tools separating generic pipeline nuts-and-bolts from the following two aspects:\n\nRun-specific configuration — samples, directories, settings/parameters.\nThings specific to the run-time environment (laptop vs. cluster vs. cloud).\n\n\nThe two most commonly used command-line based options in bioinformatics are Nextflow and Snakemake. Both have their pros and cons, but we’ll focus on Nextflow here.\n\nLearn to write pipelines?\nMost workflow tools are small “domain-specific” languages (DSLs), often a sort of extension of a more general language: for example, Python for Snakemake, and Groovy/Java for Nextflow.\nLearning one of these tools is harder than it should be, in my opinion — a truly excellent workflow tool does not yet exist, and may not appear in the near-future either because existing options have become entrenched. Therefore, learning to write your own pipelines with one of them is probably only worth it if you plan to regularly work on genomics/bioinformatics projects.\nIf you decide not to do this, I recommend that you instead use runner scripts as taught in this course. This week’s exercises and your final project can help you get some more practice with these.\nEither way, for many kinds of omics data, it is also possible (and a great idea) to use publicly available pipelines written with one of these workflow tools, and we’ll practice with that next."
  },
  {
    "objectID": "week09/w9_1_pipeline-intro.html#nf-core-pipelines",
    "href": "week09/w9_1_pipeline-intro.html#nf-core-pipelines",
    "title": "Workflow management systems",
    "section": "2 nf-core pipelines",
    "text": "2 nf-core pipelines\nAmong workflow tools, Nextflow has by far the best ecosystem of publicly available pipelines. The “nf-core” initiative (https://nf-co.re, Ewels et al. 2020) curates a set of best-practice, flexible, and well-documented pipelines written in Nextflow:\n\n\n\n\n\n\nFor many common omics analysis types, nf-core has a pipeline. It currently has 58 complete pipelines — these are the four most popular ones:\n\n\n\n\n\nLet’s take a closer look at the most widely used one, the rnaseq pipeline, which we’ll run in the next session:\n\n\n\n\n\nThere is often a bewildering array of bioinformatics programs for a given type of analysis, and it can be very hard and time-consuming to figure out what you should use. An additional advantage of using an nf-core (or similar) pipeline is that you can be confident that it uses a good if not optimal combination of tools and tool settings, since most of these pipelines have been developed over years by many experts in the field, and are also continuously updated."
  },
  {
    "objectID": "week09/w9_1_pipeline-intro.html#footnotes",
    "href": "week09/w9_1_pipeline-intro.html#footnotes",
    "title": "Workflow management systems",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThat’s a lot of big words!↩︎"
  },
  {
    "objectID": "week07/w7_1_slurm.html#basics-of-slurm-batch-jobs",
    "href": "week07/w7_1_slurm.html#basics-of-slurm-batch-jobs",
    "title": "Slurm batch jobs at OSC",
    "section": "1 Basics of Slurm batch jobs",
    "text": "1 Basics of Slurm batch jobs\nWhen you request a batch job, you ask the Slurm scheduler to run a script “out of sight” on a compute node. While that script will run on a compute node, you stay in your current shell at your current node regardless of whether that is on a login or compute node. After submitting a batch job, it will continue running even if you log off from OSC and shut down your computer.\n\n\n1.1 The sbatch command\nYou can use Slurm’s sbatch command to submit a batch job. But first, recall from last week that you can directly run a Bash script as follows:\nbash scripts/printname.sh Jane Doe\nThis script will print a first and a last name\nFirst name: Jane\nLast name: Doe\nThe above command ran the script on our current node. To instead submit the script to the Slurm queue, simply replace bash by sbatch:\nsbatch scripts/printname.sh Jane Doe\nsrun: error: ERROR: Job invalid: Must specify account for job  \nsrun: error: Unable to allocate resources: Unspecified error\nHowever, as the above error message “Must specify account for job” tells us, you need to indicate which OSC Project (or as Slurm puts it, “account”) you want to use for this compute job. Use the --account= option to sbatch to do this:\nsbatch --account=PAS2700 scripts/printname.sh Jane Doe\nSubmitted batch job 12431935\nThis output line means your job was successfully submitted (no further output will be printed to your screen — more about that below). The job has a unique identifier among all compute jobs by all users at OSC, and we can use this number to monitor and manage it. Each of us will therefore see a different job number pop up.\n\n\n\n\n\n\n\nsbatch options and script arguments\n\n\n\nAs you perhaps noticed in the command above, we can use sbatch options and script arguments in one command like so:\nsbatch [sbatch-options] myscript.sh [script-arguments]\nBut, depending on the details of the script itself, all combinations of using sbatch options and script arguments are possible:\nsbatch scripts/printname.sh                             # No options/arguments for either\nsbatch scripts/printname.sh Jane Doe                    # Script arguments but no sbatch option\nsbatch --account=PAS2700 scripts/printname.sh           # sbatch option but no script arguments\nsbatch --account=PAS2700 scripts/printname.sh Jane Doe  # Both sbatch option and script arguments\n(Omitting the --account option is possible when we specify this option inside the script, as we’ll see below.)\n\n\n\n\n\n1.2 Adding sbatch options in scripts\nThe --account= option is just one of many options you can use when reserving a compute job, but is the only required one. Defaults exist for all other options, such as the amount of time (1 hour) and the number of cores (1 core).\nInstead of specifying sbatch options on the command-line when submitting the script, you can also add these options inside the script. This is a useful alternative because:\n\nYou’ll often want to specify several options, which can lead to very long sbatch commands.\nIt allows you to store a script’s typical Slurm options as part of the script, so you don’t have to remember them.\n\nThese options are added in the script using another type of special comment line akin to the shebang (#!/bin/bash) line, marked by #SBATCH. Just like the shebang line, the #SBATCH line(s) should be at the top of the script. Let’s add one such line to the printname.sh script, such that the first few lines read:\n#!/bin/bash\n#SBATCH --account=PAS2700\n\nset -euo pipefail\nSo, the equivalent of adding --account=PAS2700 after sbatch on the command line is a line in your script that reads #SBATCH --account=PAS2700.\nAfter adding this to the script, you are now able to run the sbatch command without options (which failed earlier):\nsbatch scripts/printname.sh Jane Doe\nSubmitted batch job 12431942\nAfter submitting a batch job, you immediately get your prompt back. The job will run outside of your immediate view, and you can continue doing other things in the shell while it does (or log off). This behavior allows you to submit many jobs at the same time, because you don’t have to wait for other jobs to finish, or even to start.\n\n\n\n\n\n\nsbatch option precedence!\n\n\n\nAny sbatch option provided on the command line will override the equivalent option provided inside the script. This is sensible because it allows you to provide “defaults” inside the script, and change one or more of those when needed “on the go” on the command line.\n\n\n\n\n\n\n\n\nRunning a script with #SBATCH lines in non-Slurm contexts (Click to expand)\n\n\n\n\n\nBecause #SBATCH lines are special comment lines, they will simply be ignored (and not throw any errors) when you run a script with such lines in other contexts: for example, when not running it as a batch job at OSC, or even when running it on a computer without Slurm installed.\n\n\n\n\n\n\n1.3 Where does the script’s output go?\nAbove, we saw that when you ran printname.sh directly with bash, its output was printed to the screen, whereas when you submitted it as a batch job, only Submitted batch job &lt;job-number&gt; was printed to screen. Where did your output go?\nThe output ended up in a file called slurm-&lt;job-number&gt;.out (e.g., slurm-12431942.out; since each job number is unique to a given job, each file has a different number). We will call this type of file a Slurm log file.\n\n\nAny idea why we may not want batch job output printed to screen, even if it was possible? (Click for the answer)\n\nThe power of submitting batch jobs is that you can submit many at once — e.g. one per sample, running the same script. If the output from all those scripts ends up on your screen, things become a big mess, and you have no lasting record of what happened.\n\n\nYou should already have two of these Slurm log files if you ran all the above code:\nls\nscripts slurm-12431935.out slurm-12431942.out\nLet’s take a look at the contents of one of these:\n# (Replace the number in the file name with whatever you got! - check with 'ls')\ncat slurm-12431935.out\nThis script will print a first and a last name\nFirst name: Jane  \nLast name: Doe\nThis file contains the script’s output that was printed to screen when we ran it with bash — nothing more or less.\n\nTwo types of output files\nIt’s important to realize the distinction between two broad types of output a script may have:\n\nOutput that is printed to screen when you directly run a script (bash myscript.sh), and that ends up in the Slurm log file when you submit the script as a batch job. This includes output produced by echo statements, by any errors that may occur, and logging output by any program that we run in the script1.\nOutput of commands inside the script that is redirected to a file or that a program writes to an output file. This type of output will end up in the exact same files regardless of whether we run the script directly (with bash) or as a batch job (with sbatch).\n\nOur script above only had the first type of output, but typical scripts have both, and we’ll see examples of this below.\n\n\nCleaning up the Slurm logs\nWhen using batch jobs, your working dir can easily become a confusing mess of anonymous-looking Slurm log files. Two strategies help to prevent this:\n\nChanging the default Slurm log file name to include a one- or two-word description of the job/script (see below).\nCleaning up your Slurm log files, by:\n\nRemoving them when no longer needed — as is e.g. appropriate for our current Slurm log file.\nMoving them into a Results dir, which is often appropriate after you’ve run a bioinformatics tool, since the Slurm log file may contain some info you’d like to keep. For example, we may move any Slurm log files for jobs that ran FastQC to a dir results/fastqc/logs.\n\n\n# In this case, we'll simply remove the Slurm log files\nrm slurm*out\n\n\n\n\n\n\n\nThe working directory stays the same\n\n\n\nBatch jobs start in the directory that they were submitted from: that is, your working directory remains the same."
  },
  {
    "objectID": "week07/w7_1_slurm.html#monitoring-batch-jobs",
    "href": "week07/w7_1_slurm.html#monitoring-batch-jobs",
    "title": "Slurm batch jobs at OSC",
    "section": "2 Monitoring batch jobs",
    "text": "2 Monitoring batch jobs\nWhen submitting batch jobs for your research, you’ll often have jobs that run for a while, and/or you’ll submit many jobs at once. In addition, longer-running jobs and that ask for many cores sometimes remain queued for a while before they start. It’s therefore important to know how you can monitor your batch jobs.\n\n2.1 A sleepy script for practice\nWe’ll use another short shell script to practice monitoring and managing batch jobs. First create a new file:\ntouch scripts/sleep.sh\nOpen the file in the VS Code editor and copy the following into it:\n#!/bin/bash\n#SBATCH --account=PAS2700\n\necho \"I will sleep for 30 seconds\" &gt; sleep.txt\nsleep 30s\necho \"I'm awake! Done with script sleep.sh\"\n\n\n Exercise: Batch job output recap\nPredict what would happen if you submit the sleep.sh script as a batch job using sbatch scripts/sleep.sh:\n\nHow many output files will this batch job produce?\nWhat will be in each of those files?\nIn which directory will the file(s) appear?\nIn terms of output, what would have been different if we had run the script directly, using the command bash scripts/sleep.sh?\n\nThen, test your predictions by running the script.\n\n\nClick for the solutions\n\n\nThe job will produce 2 files:\n\nslurm-&lt;job-number&gt;.out: The Slurm log file, containing output normally printed to screen.\nsleep.txt: Containing output that was redirected to this file in the script.\n\nThe those files will contain the following:\n\nslurm-&lt;job-number&gt;.out: I’m awake! Done with script sleep.sh\nsleep.txt: “I will sleep for 30 seconds”\n\nBoth files will end up in your current working directory. Slurm log files always go to the directory from which you submitted the job. Slurm jobs also run from the directory from which you submitted your job, and since we redirected the output simply to sleep.txt, that file was created in our working directory.\nIf we had run the script directly, sleep.txt would have also been created with the same content, but “All done!” would have been printed to screen.\n\nRun the script and check the outputs:\nsbatch scripts/sleep.sh\nSubmitted batch job 27935840\ncat sleep.txt\nI will sleep for 30 seconds\ncat slurm-27935840.out\nI'm awake! Done with script sleep.sh\n\n\n\n\n\n2.2 Checking the job’s status\nAfter you submit a job, it may be initially be waiting to be allocated resources: i.e., it may be queued (“pending”). Then, the job will start running — you’ve seen all of this with the VS Code Interactive App job as well.\nWhereas Interactive App jobs will keep running until they’ve reached the end of the allocated time2, batch jobs will stop as soon as the script has finished. And if the script is still running when the job runs out of its allocated time, it will be killed (stopped) right away.\n\nThe squeue command\nYou can check the status of your batch job using the squeue Slurm command:\nsqueue -u $USER -l\nThu Apr 4 15:47:51 2023\n        JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n     23640814 condo-osu ondemand   jelmer  RUNNING       6:34   2:00:00      1 p0133\nIn the command above:\n\nYou specify your username with the -u option (without this, you’d see everyone’s jobs!). In this example, I used the environment variable $USER to get your user name, just so that the very same code will work for everyone (you can also simply type your username if that’s shorter or easier).\nThe option -l (lowercase L, not the number 1) will produce more verbose (“long”) output.\n\nIn the output, after a line with the date and time, and a header line, you should see information about a single compute job, as shown above: this is the Interactive App job that runs VS Code. That’s not a “batch” job, but it is a compute job, and all compute jobs are listed.\nThe following pieces of information about each job are listed:\n\nJOBID — The job ID number\nPARTITION — The type of queue\nNAME — The name of the job\nUSER — The user name of the user who submitted the job\nSTATE — The job’s state, usually PENDING (queued) or RUNNING. Finished jobs do not appear on the list.\nTIME — For how long the job has been running (here as minutes:seconds)\nTIME_LIMIT — the amount of time you reserved for the job (here as hours:minutes:seconds)\nNODES — The number of nodes reserved for the job\nNODELIST(REASON) — When running: the ID of the node on which it is running. When pending: why it is pending.\n\n\n\n\nsqueue example\nNow, let’s see a batch job in the squeue listing. Start by submitting the sleep.sh script as a batch job:\nsbatch scripts/sleep.sh\nSubmitted batch job 12431945\nIf you’re quick enough, you may be able to catch the STATE as PENDING before the job starts:\nsqueue -u $USER -l\nThu Apr 4 15:48:26 2023\n         JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n      12520046 serial-40 sleep.sh   jelmer  PENDING       0:00   1:00:00      1 (None)\n      23640814 condo-osu ondemand   jelmer  RUNNING       7:12   2:00:00      1 p0133\nBut soon enough it should say RUNNING in the STATE column:\nsqueue -u $USER -l\nThu Apr 4 15:48:39 2023\n         JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n      12520046 condo-osu sleep.sh   jelmer  RUNNING       0:12   1:00:00      1 p0133\n      23640814 condo-osu ondemand   jelmer  RUNNING       8:15   2:00:00      1 p0133\nThe script should finish after 30 seconds (because your command was sleep 30s), after which the job will immediately disappear from the squeue listing, because only pending and running jobs are shown:\nsqueue -u $USER -l\nMon Aug 21 15:49:26 2023\n         JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n      23640814 condo-osu ondemand   jelmer  RUNNING       9:02   2:00:00      1 p0133\n\n\n\nChecking the output files\nWhenever you’re running a script as a batch job, even if you’ve been monitoring it with squeue, you should also make sure it ran successfully. You typically do so by checking the expected output file(s). As mentioned above, you’ll usually have two types of output from a batch job:\n\nFile(s) directly created by the command inside the script (here, sleep.sh).\nA Slurm log file with the script’s standard output and standard error (i.e. output that is normally printed to screen).\n\nAnd you saw in the exercise above that this was also the case for the output of our sleepy script:\ncat sleep.txt\nI will sleep for 30 seconds\ncat slurm-12520046.out\nI'm awake! Done with script sleep.sh\n\nLet’s keep things tidy and remove the sleepy script outputs:\n# (Replace the number in the file name with whatever you got! - check with 'ls')\nrm slurm*.out sleep.txt\n\n\n\n\n\n\n\nSee output added to the Slurm log file in real time\n\n\n\nText will be added to the Slurm log file in real time as the running script (or the program ran by the script) outputs it. However, the output that commands like cat and less print are static.\nTherefore, if you find yourself opening/printing the contents of the Slurm log file again and again to keep track of progress, then instead use tail -f, which will “follow” the file and will print new text as it’s added to the Slurm log file:\n# See the last lines of the file, with new contents added in real time\ntail -f slurm-12520046.out\nTo exit the tail -f livestream, press Ctrl+C.\n\n\n\n\n\n\n2.3 Cancelling jobs\nSometimes, you want to cancel one or more jobs, because you realize you made a mistake in the script, or because you used the wrong input files as arguments. You can do so using scancel:\n# [Example - DON'T run this: the second line would cancel your VS Code job]\nscancel 2979968        # Cancel job number 2979968\nscancel -u $USER       # Cancel all your running and queued jobs (careful with this!)\n\n\n\n\n\n\n\nAdditional job management commands and options (Click to expand)\n\n\n\n\n\n\nUse squeue’s -t option to restrict the type of jobs you want to show. For example, to only show running and not pending jobs:\nsqueue -u $USER -t RUNNING\nYou can see more details about any running or finished job, including the amount of time it ran for:\nscontrol show job &lt;jobID&gt;\nUserId=jelmer(33227) GroupId=PAS0471(3773) MCS_label=N/A\nPriority=200005206 Nice=0 Account=pas2700 QOS=pitzer-default\nJobState=RUNNING Reason=None Dependency=(null)\nRequeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0\nRunTime=00:02:00 TimeLimit=01:00:00 TimeMin=N/A\nSubmitTime=2020-12-14T14:32:44 EligibleTime=2020-12-14T14:32:44\nAccrueTime=2020-12-14T14:32:44\nStartTime=2020-12-14T14:32:47 EndTime=2020-12-14T15:32:47 Deadline=N/A\nSuspendTime=None SecsPreSuspend=0 LastSchedEval=2020-12-14T14:32:47\nPartition=serial-40core AllocNode:Sid=pitzer-login01:57954\n[...]\nUpdate directives for a job that has already been submitted (this can only be done before the job has started running):\nscontrol update job=&lt;jobID&gt; timeLimit=5:00:00\nHold and release a pending (queued) job, e.g. when needing to update input file before it starts running:\nscontrol hold &lt;jobID&gt;       # Job won't start running until released\nscontrol release &lt;jobID&gt;    # Job is free to start"
  },
  {
    "objectID": "week07/w7_1_slurm.html#common-slurm-options",
    "href": "week07/w7_1_slurm.html#common-slurm-options",
    "title": "Slurm batch jobs at OSC",
    "section": "3 Common Slurm options",
    "text": "3 Common Slurm options\nHere, we’ll go through the most commonly used Slurm options. As pointed out above, each of these can either be:\n\nPassed on the command line: sbatch --account=PAS2700 myscript.sh (has precedence over the next)\nAdded at the top of the script you’re submitting: #SBATCH --account=PAS2700.\n\nAlso, note that many Slurm options have a corresponding long (--account=PAS2700) and short format (-A PAS2700). For clarity, we’ll stick to long format options here.\n\n3.1 --account: The OSC project\nAs seen above. When submitting a batch job, always specify the OSC project (“account”).\n\n\n3.2 --time: Time limit (“wall time”)\nUse the --time option to specify the maximum amount of time your job will run for:\n\nYour job will be killed (stopped) as soon as it hits the specified time limit!\nCompare “Wall time” with “core hours”: if a job runs for 2 hour and used 8 cores, the wall time was 2 hours and the number of core hours was 2 x 8 = 16.\nThe default time limit is 1 hour. Acceptable time formats include:\n\nminutes (e.g. 60 =&gt; 60 minutes)\nhours:minutes:seconds (e.g. 1:00:00 =&gt; 60 minutes)\ndays-hours (e.g. 2-12 =&gt; two-and-a-half days)\n\nFor single-node jobs, up to 168 hours (7 days) can be requested. If that’s not enough, you can request access to the longserial queue for jobs of up to 336 hours (14 days).\nOSC bills you for the time your job actually used, not what you reserved. But jobs asking for more time may be queued longer before they start.\n\nAn example, asking for 2 hours in the “minute-format”:\n#!/bin/bash\n#SBATCH --time=120\nOr for 12 hours in the “hour-format”:\n#!/bin/bash\n#SBATCH --time=12:00:00\n\n\n\n\n\n\nWhen in doubt, reserve more time\n\n\n\nIt is common to be uncertain about how much time your job will take (i.e., how long it will take for your script to finish). Whenever this happens, ask for more, perhaps much more, time than what you think/guesstimate you will need. It is really annoying to have a job run out of time after several hours, while the increase in queueing time for jobs asking for more time is often quite minimal at OSC.\n\n\n\n\n Exercise: exceed the time limit\nModify the sleep.sh script to reserve only 1 minute for the job while making the script run for longer than that.\nIf you succeed in exceeding the time limit, an error message will be printed. Where do you think this error message will be printed: to the screen, in the Slurm log file, or in sleep.txt? After waiting for the job to be killed after 60 seconds, check if you were correct and what the error message is.\n\n\nClick for the solution\n\nThis script would do the trick, where we request 1 minute of wall-time while we let the script sleep for 80 seconds:\n#!/bin/bash\n#SBATCH --account=PAS2700\n#SBATCH --time=1\n\necho \"I will sleep for 80 seconds\" &gt; sleep.txt\nsleep 80s\necho \"I'm awake! Done with script sleep.sh\"\nSubmit it as usual:\nsbatch scripts/sleep.sh\nSubmitted batch job 23641567\nThis would result in the following type of error, which will be printed in the Slurm log file:\nslurmstepd: error: *** JOB 23641567 ON p0133 CANCELLED AT 2024-04-04T14:55:24 DUE TO TIME LIMIT ***\n\n\n\n\n\n3.3 Cores (& nodes and tasks)\nThere are several options to specify the number of nodes (≈ computers), cores, or “tasks” (processes). These are separate but related options, and this is where things can get confusing! Some background:\n\nNote that Slurm mostly uses the terms “core” and “CPU” interchangeably3. More generally with bioinformatics tools, “thread” is also commonly used interchangeably with core/CPU4. Therefore, as also mentioned in the session on OSC, for our purposes, you can think of core, CPU, and thread as synonyms that refer to the sub-parts/components of a node that you can reserve and use separately.\n\n\nRunning a program with multiple threads/cores/CPUs (“multi-threading”) is very common, and this can make the running time of such programs much shorter. While the specifics depend on the program, using 8-12 cores is often a sweet spot, whereas asking for even more cores can lead to rapidly diminishing returns.\nRunning multiple processes (tasks) or needing multiple nodes in a single batch job is not common.\n\nIn practice, my recommendations are to basically always:\n\nSpecify the number of threads/cores/CPUs to Slurm with --cpus-per-task=n (the short notation is -c).\nKeep the number of tasks and nodes to their defaults of 1 (in which case the above -c option specifies the number of cores, period).\nTell the program that you’re running about the number of available cores — most bioinformatics tools have an option like --cores or --threads. You should set this to the same value n as the --cpus-per-task.\n\nAn example, where we ask for 8 CPUs/cores/threads:\n#!/bin/bash\n#SBATCH --cpus-per-task=8\n\n# And we tell a fictional program about that number of cores:\ncool_program.py --cores 8 sampleA_R1.fastq.gz\n\n\n\n\n\n\nRare cases: multiple nodes or tasks (Click to expand)\n\n\n\n\n\n\nYou can specify the number of nodes with --nodes and the number of tasks with --ntasks and/or --ntasks-per-node; all have defaults of 1 (see the table below).\nOnly ask for more than one node when a program is parallelized with e.g. “MPI”, which is rare in bioinformatics.\nFor jobs with multiple processes (tasks), you can use --ntasks=n or --ntasks-per-node=n — this is also quite rare! However, note in practice, specifying the number of tasks n with one of these options is equivalent to using --cpus-per-task=n, in the sense that both ask for n cores that can subsequently be used by a program in your script. Therefore, some people use tasks as opposed to cpus for multi-threading, and you can see this usage in the OSC documentation too. Yes, this is confusing!\n\nHere is an overview of the options related to cores, tasks, and nodes:\n\n\n\n\n\n\n\n\n\nResource/use\nshort\nlong\ndefault\n\n\n\n\nNr. of cores/CPUs/threads (per task)\n-c 1\n--cpus-per-task=1\n1\n\n\nNr. of “tasks” (processes)\n-n 1\n--ntasks=1\n1\n\n\nNr. of tasks per node\n-\n--ntasks-per-node=1\n1\n\n\nNr. of nodes\n-N 1\n--nodes=1\n1\n\n\n\n\n\n\n\n\n\n3.4 --mem: RAM memory\nUse the --mem option to specify the maximum amount of RAM (Random Access Memory) that your job can use:\n\nEach core on a node has 4 GB of memory “on it”, and therefore, the default amount of memory you will get is 4 GB is per reserved core. For example, if you specify --cpus-per-task=4, you will have 16 GB of memory. And the default number of cores is 1, so the default amount of memory is 4 GB.\nBecause it is common to ask for multiple cores and due to the above-mentioned adjustment of the memory based on the number of cores, you will usually end up having enough memory automatically — therefore, it is common to omit the --mem option.\nThe default --mem unit is MB (MegaBytes); append G for GB (i.e. 100 means 100 MB, 10G means 10 GB).\nLike with the time limit, your job gets killed by Slurm when it hits the memory limit.\nThe maximum amount of memory you can request on regular Pitzer compute nodes is 177 GB (and 117 GB on Owens). If you need more than that, you will need one of the specialized largemem or hugemem nodes — switching to such a node can happen automatically based on your requested amount, though with caveats: see this OSC page for details on Pitzer, and this page for details on Owens.\n\nFor example, to request 20 GB of RAM:\n#!/bin/bash\n#SBATCH --mem=20G\n\n\n\n\n\n\nIt is not always clear what happened when your job ran out of memory (Click to expand)\n\n\n\n\n\nWhereas you get a very clear Slurm error message when you hit the time limit (as seen in the exercise above), hitting the memory limit can result in a variety of errors.\nBut look for keywords such as “Killed”, “Out of Memory” / “OOM”, and “Core Dumped”, as well as actual “dumped cores” in your working dir (large files with names like core.&lt;number&gt;, these can be deleted).\n\n\n\n\n\n Exercise: Adjusting cores and memory\nThink about submitting a shell script that runs a bioinformatics tool like FastQC as a batch job, in the following two scenarios:\n\nThe program has an option --threads, and you want to set that to 8. The program also says you’ll need 25 GB of memory. What #SBATCH options related to this will you use?\n\n\n\nClick for the solution\n\nYou should only need the following, since this will give you 8 * 4 = 32 GB of memory. There is no point in “downgrading” the amount of memory.\n#SBATCH --cpus-per-task=8\n\n\nThe program has an option --cores, and you want to set that to 12. The program also says you’ll need 60 GB of memory. What #SBATCH options will you use?\n\n\n\nClick for the solution\n\nHere, it will make sense to ask for --mem separately.\n#SBATCH --cpus-per-task=12\n#SBATCH --mem=60G\nAlternatively, you could ask for 15 cores, but then instruct the program to use only 12. Or you could reason that since you’ll need 15 cores anyway due to the amount of memory you’ll need, you might as well instruct the program to use all 15, since this may well speed things up a little more.\n\n\n\n\n\n3.5 --output: Slurm log files\nAs we saw above, by default, all output from a script that would normally be printed to screen will end up in a Slurm log file when we submit the script as a batch job. This file will be created in the directory from which you submitted the script, and will be called slurm-&lt;job-number&gt;.out, e.g. slurm-12431942.out.\nBut it is possible to change the name of this file. For instance, it can be useful to include the name of the bioinformatics program that the script runs, so that it’s easier to recognize this file later. We can do this with the --output option, e.g. --output=slurm-fastqc.out if we were running FastQC.\nBut you’ll generally want to keep the batch job number in the file name too5. Since we won’t know the batch job number in advance, we need a trick here — and that is to use %j, which represents the batch job number:\n#!/bin/bash\n#SBATCH --output=slurm-fastqc-%j.out\n\n\n\n\n\n\nThe output streams stdout and stderr, and separating them (Click to expand)\n\n\n\n\n\nBy default, two output streams from commands and programs called “standard output” (stdout) and “standard error” (stderr) are printed to screen. Without discussing this in detail, we have seen this several times: any regular output by a command is stdout and any error messages we’ve seen were stderr. Both of these streams by default also end up in the same Slurm log file, but it is possible to separate them into different files.\nBecause stderr, as you might have guessed, often contains error messages, it could be useful to have those in a separate file. You can make that happen with the --error option, e.g. --error=slurm-fastqc-%j.err.\nHowever, reality is more messy: some programs print their main output not to a file but to standard out, and their logging output, errors and regular messages alike, to standard error. Yet other programs use stdout or stderr for all messages.\nI therefore usually only specify --output, such that both streams end up in that file.\n\n\n\n\n\n\n3.6 --mail-type: Receive emails\nYou can use the --mail-type option to have Slurm email you for example when a job begins, completes or fails. You don’t have to specify your email address: you’ll be automatically emailed on the email address that is linked to your OSC account. I tend to use:\n\nFAIL for shorter-running jobs (roughly up to a few hours)\nFAIL will email you upon job failure, e.g. when the scripts exits with an error or times out. This is especially useful when submitting many jobs with a loop: this way you know immediately whether any of the jobs failed.\nEND and FAIL for longer-running jobs\nThis is helpful because you don’t want to have to keep checking in on jobs that run for many hours.\n\nI would avoid having Slurm send you emails upon regular completion for shorter jobs, because you may get inundated with emails and then quickly start ignoring the emails altogether.\n#!/bin/bash\n#SBATCH --mail-type=END,FAIL\n#!/bin/bash\n#SBATCH --mail-type=FAIL\n\n\n\n\n\n\nGet warned when your job is close to its time limit (Click to expand)\n\n\n\n\n\nYou may also find the values TIME_LIMIT_90, TIME_LIMIT_80, and TIME_LIMIT_50 useful for very long-running jobs, which will warn you when the job is at 90/80/50% of the time limit. For example, it is possible to email OSC to ask for an extension on individual jobs. You shouldn’t do this often, but if you have a job that ran for 6 days and it looks like it may time out, this may well be worth it.\n\n\n\n\n\n Exercise: Submit your FastQC script as a batch job\nLast week, we created a shell script to run FastQC, and ran it as follows:\nfastq_file=../../garrigos_data/fastq/ERR10802863_R1.fastq.gz\nbash scripts/fastqc.sh \"$fastq_file\" results/fastqc\n\nAdd an #SBATCH lines to the script to specify the course’s OSC project PAS2700, and submit the modified script as a batch job with the same arguments as above.\n\n\n\nSolution (click here)\n\n\nThe top of your script should read as follows:\n#!/bin/bash\n#SBATCH --account=PAS2700\nSubmit the script as follows:\nfastq_file=../../garrigos_data/fastq/ERR10802863_R1.fastq.gz\nsbatch scripts/fastqc.sh \"$fastq_file\" results/fastqc\nSubmitted batch job 12431988\n\n\n\nMonitor your job with squeue.\nWhen it has finished, check the Slurm log file in your working dir and the main FastQC output files in results/fastqc.\nBonus — add these #SBATCH options, then resubmit:\n\nLet the Slurm log file include ‘fastqc’ in the file name as well as the job ID number.\nLet Slurm email you both when the job completes normally and when it fails. Check that you received the email.\n\n\n\n\nSolution (click here)\n\n\nThe top of your script should read as follows:\n#!/bin/bash\n#SBATCH --account=PAS2700\n#SBATCH --output=slurm-fastqc-%j.out\n#SBATCH --mail-type=END,FAIL"
  },
  {
    "objectID": "week07/w7_1_slurm.html#in-closing-making-sure-your-jobs-ran-successfully",
    "href": "week07/w7_1_slurm.html#in-closing-making-sure-your-jobs-ran-successfully",
    "title": "Slurm batch jobs at OSC",
    "section": "4 In closing: making sure your jobs ran successfully",
    "text": "4 In closing: making sure your jobs ran successfully\nHere are some summarizing notes on the overall strategy to monitor your batch jobs:\n\nTo see whether your job(s) have started, check the queue (with squeue) or check for Slurm log files (with ls).\nOnce the jobs are no longer listed in the queue, they will have finished: either successfully or because of an error.\nWhen you’ve submitted many jobs that run the same script for different samples/files:\n\nCarefully read the full Slurm log file, and check other output files, for at least 1 one of the jobs.\nCheck whether no jobs have failed: via email when using --mail-type=END, or by checking the tail of each log for “Done with script” messages6.\nCheck that you have the expected number of output files and that no files have size zero (run ls -lh)."
  },
  {
    "objectID": "week07/w7_1_slurm.html#self-study-material",
    "href": "week07/w7_1_slurm.html#self-study-material",
    "title": "Slurm batch jobs at OSC",
    "section": "5 Self-study material",
    "text": "5 Self-study material\n\nSlurm environment variables\nInside a shell script that will be submitted as a batch job, you can use a number of Slurm environment variables that will automatically be available, such as:\n\n\n\n\n\n\n\n\nVariable\nCorresponding option\nDescription\n\n\n\n\n$SLURM_JOB_ID\nN/A\nJob ID assigned by Slurm\n\n\n$SLURM_JOB_NAME\n--job-name\nJob name\n\n\n$SLURM_CPUS_PER_TASK\n-c / --cpus-per-task\nNumber of CPUs (~ cores/threads) available\n\n\n$SLURM_MEM_PER_NODE\n--mem\nAmount of memory available (per node)\n\n\n$TMPDIR\nN/A\nPath to the Compute storage available during the job\n\n\n$SLURM_SUBMIT_DIR\nN/A\nPath to dir from which job was submitted.\n\n\n\n\nAs an example of how these environment variables can be useful, the command below uses $SLURM_CPUS_PER_TASK in its call to the program STAR inside the script:\nSTAR --runThreadN \"$SLURM_CPUS_PER_TASK\" --genomeDir ...\nWith this strategy, you will automatically use the correct (requested) number of cores, and don’t risk having a mismatch. Also, if you need to change the number of cores, you’ll only have to modify it in one place: in the resource request to Slurm.\n\n\n\n5.1 Interactive shell jobs\nInteractive shell jobs will grant you interactive shell access on a compute node. We’ve been working in a shell in VS Code Server, which means that we already have interactive shell access on a compute node!\nHowever, we only have access to 1 core and 4 GB of memory in this VS Code shell, and there is no way of changing this. If you want an interactive shell job with more resources, you’ll have to start one with Slurm commands.\nA couple of different commands can be used to start an interactive shell job. I prefer the general srun command7, which we can use with --pty /bin/bash added to get an interactive Bash shell.\nsrun --account=PAS2700 --pty /bin/bash\nsrun: job 12431932 queued and waiting for resources  \nsrun: job 12431932 has been allocated resources\n\n[...regular login info, such as quota, not shown...]\n\n[jelmer@p0133 PAS2700]$\nThere we go! First some Slurm scheduling info was printed to screen: initially, the job was queued, and then it was “allocated resources”: that is, computing resources such as a compute node were reserved for the job. After that:\n\nThe job starts and because we’ve reserved an interactive shell job, a new Bash shell is initiated: for that reason, we get to see our regular login info once again.\nWe have now moved to the compute node at which our interactive job is running, so you should have a different p number in your prompt.\n\n\n\n\n5.2 Table with sbatch options\nFirst, here are the options we discussed above:\n\n\n\n\n\n\n\n\n\nResource/use\nshort\nlong\ndefault\n\n\n\n\nProject to be billed\n-A PAS2700\n--account=PAS2700\nN/A\n\n\nTime limit\n-t 4:00:00\n--time=4:00:00\n1:00:00\n\n\nNr of nodes\n-N 1\n--nodes=1\n1\n\n\nNr of cores\n-c 1\n--cpus-per-task=1\n1\n\n\nNr of “tasks” (processes)\n-n 1\n--ntasks=1\n1\n\n\nNr of tasks per node\n-\n--ntasks-per-node\n1\n\n\nMemory limit per node\n-\n--mem=4G\n(4G)\n\n\nLog output file\n-o\n--output=slurm-fastqc-%j.out\n\n\n\nError output (stderr)\n-e\n--error=slurm-fastqc-%j.err\n\n\n\nGet email when job starts, ends,fails, or all of the above\n-\n--mail-type=START  --mail-type=END  --mail-type=FAIL  --mail-type=ALL\n\n\n\n\n\nAnd a couple of additional ones:\n\n\n\n\n\n\n\nResource/use\noption\n\n\n\n\nJob name (displayed in the queue)\n--job-name=fastqc\n\n\nPartition (=queue type)\n--partition=longserial  --partition=hugemem\n\n\nLet job begin only after a specific time\n--begin=2024-04-05T12:00:00\n\n\nLet job begin only after another job is done\n--dependency=afterany:123456"
  },
  {
    "objectID": "week07/w7_1_slurm.html#footnotes",
    "href": "week07/w7_1_slurm.html#footnotes",
    "title": "Slurm batch jobs at OSC",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n This type of output is referred standard out (non-error output) and standard error — see the box in the section on Slurm log files for more↩︎\nUnless you actively “Delete” to job on the Ondemand website.↩︎\nEven though technically, one CPU often contains multiple cores.↩︎\nEven though technically, one core often contains multiple threads.↩︎\nFor instance, we might be running the FastQC script multiple times, and otherwise those would all have the same name and be overwritten.↩︎\nThe combination of using strict Bash settings (set -euo pipefail) and printing a line that marks the end of the script (echo \"Done with script\") makes it easy to spot scripts that failed, because they won’t have that marker line at the end of the Slurm log file.↩︎\nOther options: salloc works almost identically to srun, whereas sinteractive is an OSC convenience wrapper but with more limited options.↩︎"
  },
  {
    "objectID": "week07/w7_2_slurm.html",
    "href": "week07/w7_2_slurm.html",
    "title": "Managing batch jobs in multi-step analyses",
    "section": "",
    "text": "While thinking about different ways to organize and run your analyses, let’s have an example in mind with a simple RNA-seq analysis that:"
  },
  {
    "objectID": "week07/w7_2_slurm.html#more-on-runner-scripts",
    "href": "week07/w7_2_slurm.html#more-on-runner-scripts",
    "title": "Managing batch jobs in multi-step analyses",
    "section": "1 More on runner scripts",
    "text": "1 More on runner scripts\nIn the past two weeks of this course, we have created a few “runner scripts” (usually named run/run.sh): digital notebook-like scripts that we ran line-by-line in order to run (week 4) or submit as batch jobs (week 5) our “primary scripts” (e.g. scripts/fastqc.sh).\nThese runner scripts were useful to store that code instead of typing it directly in the terminal, especially because we were passing arguments to our script and running loops — not only would this be tedious to type, but it would also reduce reproducibility if we didn’t store exactly how we ran our scripts.\nHowever, our runner scripts so far have been very short. In the context of a research project that would include running a series of steps with different bioinformatics tools (and perhaps custom scripts), the idea would be to include all these steps in such a runner script. For example, for the above-mentioned RNA-seq analysis, such a runner script could look like so:\n# [hypothetical example - don't run this]\n# Define the inputs\nfastq_dir=data/fastq\nref_assembly=data/ref/assembly.fna\nref_annotation=data/ref/annotation.gtf\n\n# Trim the reads:\nfor R1 in \"$fastq_dir\"/*_R1.fastq.gz; do\n    # (The trim.sh script takes 2 arguments: R1 FASTQ and output dir)\n    sbatch scripts/trim.sh \"$R1\" results/trim\ndone\n\n# Align (map) the reads to a reference genome assembly:\nfor R1 in results/trim/*_R1.fastq.gz; do\n    # (The map.sh script takes 3 arguments: R1 FASTQ, ref. assembly, and output dir)\n    sbatch scripts/map.sh \"$R1\" \"$ref_assembly\" results/map\ndone\n\n# Count alignments per sample per gene using the annotation:\n# (The count.sh script takes 3 arguments: input dir, ref. annotation, and output dir)\nsbatch scripts/count.sh results/map \"$ref_annotation\" results/count_table.txt\nThe code above runs a primary shell script for each of the three steps (trim.sh, map.sh, andcount.sh): each of these takes arguments and runs a bioinformatics tool to perform that step (for example, TrimGalore for trimming).\nHere are some advantages of using such a script structure with flexible (i.e., argument-accepting) primary scripts, and an overarching runner script:\n\nRerunning everything, including with a modified sample set, or tool settings, is relatively straightforward — both for yourself and others, improving reproducibility.\nRe-applying the same set of analyses in a different project is straightforward.\nThe runner script is a form of documentation of all steps taken.\nIt (more or less) ensures you are including all necessary steps."
  },
  {
    "objectID": "week07/w7_2_slurm.html#pipelines",
    "href": "week07/w7_2_slurm.html#pipelines",
    "title": "Managing batch jobs in multi-step analyses",
    "section": "2 Pipelines",
    "text": "2 Pipelines\nWhat exactly do we mean by a “pipeline”? We may informally refer to any consecutive series of analysis steps as a pipeline. The above runner script, in particular, can informally be called a pipeline. But here, I am using pipeline in a stricter sense to mean a series of steps that can be executed from start to finish with a single command.\nAn advantage of a true pipeline is increased automation, as well as “supercharging” all the above-mentioned advantages of runner script. For example, a pipeline truly ensures that you are including all necessary steps.\n\n\n\nBut wait, can we not just run our runner script with a single command: bash run/run.sh? (Click to expand)\n\nThis doesn’t work because we are submitting batch jobs in each step. Because the script would continue to the next line/submission immediately after the previous lines, all jobs would effectively be submitted at the same time, and e.g. the mapping script would fail because the trimmed reads it needs are not yet there. (Below, we’ll briefly talk about ways around this problem.)\n\n\nTo turn our runner script into a pipeline, we would need to overcome the problem of simultaneous batch job submission. Additionally, a pipeline worth its salt should also be able to detect and stop upon failure, and to rerun parts of the pipeline flexibly. The latter may be necessary after, e.g.:\n\nSome scripts failed for all or some samples\nYou added or removed a sample\nYou had to modify a script or settings somewhere halfway the pipeline.\n\nSo how could we implement all of that?\n\nPush the limits of the Bash and Slurm tool set\nUse if statements, many script arguments, and Slurm “job dependencies” (see the box below) — but this is hard to manage for more complex workflows. Alternatively, if you only want to solve the simultaneous batch job problem, you can put all steps in a single script, but this would make for a very inefficient pipeline.\nUse a formal workflow management system.\nWe’ll talk about these some more below.\n\n\n\n\n\n\n\n\nPushing the limits of the Bash and Slurm tool set (Click to expand)\n\n\n\n\n\nFirst, here are some low-tech, ad-hoc solutions to rerunning parts of the workflow:\n\nComment out part of the workflow — e.g., to skip a step:\n# Trim:\n#for R1 in data/fastq/*_R1.fastq.gz; do\n#    sbatch scripts/trim.sh \"$R1\" results/trim\n#done\n\n# Align (map):\nfor R1 in results/trim/*_R1.fastq.gz; do\n    sbatch scripts/map.sh \"$R1\" results/map\ndone\n\n# Count alignments per sample per gene:\nsbatch scripts/count.sh results/map results/count_table.txt\nMake temporary changes — e.g., to only run a single added sample:\n# Trim:\n#for R1 in data/fastq/*_R1.fastq.gz; do\n    R1=data/fastq/newsample_R1.fastq.gz\n    sbatch scripts/trim.sh \"$R1\" results/trim\n#done\n\n# Align (map):\n#for R1 in results/trim/*_R1.fastq.gz; do\n    R1=results/trim/newsample_R1.fastq.gz\n    sbatch scripts/map.sh \"$R1\" results/map\n#done\n\n# Count - NOTE, this steps should be rerun as a whole:\nsbatch scripts/count.sh results/map results/count_table.txt\n\n\nSecond, here are some more bespoke code-based solutions:\n\nCommand-line options and if-statements to flexibly run part of the pipeline (and perhaps change settings):\ntrim=$1   # true or false\nmap=$2    # true or false\ncount=$3  # true or false\n\nif [[ \"$trim\" == true ]]; then\n    for R1 in data/fastq/*_R1.fastq.gz; do\n        bash scripts/trim.sh \"$R1\" results/trim\n    done\nfi\n\nif [[ \"$map\" == true ]]; then\n    for R1 in results/trim/*_R1.fastq.gz; do\n        bash scripts/map.sh \"$R1\" results/map\n    done\nfi\n\nif [[ \"$count\" == true ]]; then\n    bash scripts/count.sh results/map results/count_table.txt\nfi\nSlurm job dependencies — in the example below, jobs will only start after their “dependencies” (jobs whose outputs they need) have finished:\nfor R1 in data/fastq/*_R1.fastq.gz; do\n    # Submit the trimming job and store its job number:\n    submit_line=$(sbatch scripts/trim.sh \"$R1\" results/trim)\n    trim_id=$(echo \"$submit_line\" | sed 's/Submitted batch job //')\n\n    # Submit the mapping job with the condition that it only starts when the\n    # trimming job is done, using '--dependency=afterok:':\n    R1_trimmed=results/trim/$(basename \"$R1\")\n    sbatch --dependency=afterok:$trim_id scripts/map.sh \"$R1_trimmed\" results/map\ndone\n\n# If you give the mapping and counting jobs the same name with `#SBATCH --job-name=`,\n# then you can use '--dependency=singleton': the counting job will only start\n# when ALL the mapping jobs are done:\nsbatch --dependency=singleton scripts/count.sh results/map results/count_table.txt"
  },
  {
    "objectID": "week07/w7_ga_slurm.html",
    "href": "week07/w7_ga_slurm.html",
    "title": "Graded Assignment IV: Slurm batch jobs",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "week13/w13_exercises.html",
    "href": "week13/w13_exercises.html",
    "title": "Week 13 Exercises: R Data visualization",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "week14/w14_ga_R.html",
    "href": "week14/w14_ga_R.html",
    "title": "Graded Assignment VI: R",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "week15/w15_overview.html",
    "href": "week15/w15_overview.html",
    "title": "Week 15: Running nf-core pipelines",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "week12/w12_overview.html",
    "href": "week12/w12_overview.html",
    "title": "Week 12: Data visualization in R",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PRACS-AU25",
    "section": "",
    "text": "Practical Computing Skills for Omics Data  Ohio State PLNTPTH 5004 - Autumn 2025\n\n\n\n\nWeek\nTue session\nThu session\nOv. Ex. UA GA\n\n\n\n\n01\n08/26: Course intro & omics data\n08/28: Intro to OSC\n           \n\n\n02\n09/02: Unix shell basics\n09/04: Unix shell: working with files\n              \n\n\n03\n09/09: Project organization & VS Code + Markdown\n09/11: Advanced file management in the shell\n   \n\n\n04\n09/16: Version control with Git\n09/18: Git remotes on GitHub\n              \n\n\n05\n09/23: Shell scripting\n09/25: Running CLI tools with shell scripts\n                   \n\n\n06\n09/30: More on OSC & data management\n10/02: Software at OSC\n   \n\n\n07\n10/07: Slurm batch jobs\n10/09: Managing batch jobs in multi-step analyses\n                   \n\n\n08\n10/14: Using generative AI when coding\nFall break\n   \n\n\n09\n10/21: Workflow management systems\n10/23: Building Nextflow pipelines I\n   \n\n\n10\n10/28: Building Nextflow pipelines II\n10/30: Building Nextflow pipelines III\n                   \n\n\n11\n11/04: R language basics\n11/06: Data wrangling with R\n   \n\n\n12\nVeteran’s day\n11/13: Data visualization with R\n   \n\n\n13\n11/18: R for omics data I\n11/20: R for omics data II\n   \n\n\n14\n11/25: R + Markdown with Quarto\nThanksgiving\n                   \n\n\n15\n12/02: Running nf-core pipelines I\n12/04: Running nf-core pipelines II\n   \n\n\n16\n12/09: Student presentations\n\n\n\n\n\n\nOv. /  = Week overview       Ex. /  = Exercises       UA /  = Ungraded assignment       GA /  = Graded assignment\n\n\n\n\n Back to top"
  },
  {
    "objectID": "week05/w5_overview.html#links",
    "href": "week05/w5_overview.html#links",
    "title": "Week 5: Shell scripting & CLI tools",
    "section": "1 Links",
    "text": "1 Links\n\nLecture pages\n\nTuesday – Shell scripts.\nThursday – Running CLI tools with shell scripts.\nBonus (optional self-study) content: While loops, arrays, and more."
  },
  {
    "objectID": "week05/w5_overview.html#content-overview",
    "href": "week05/w5_overview.html#content-overview",
    "title": "Week 5: Shell scripting & CLI tools",
    "section": "2 Content overview",
    "text": "2 Content overview\nThis week, we will talk about shell scripting. So far, we have been running commands interactively in the shell, one line at a time. When you need to repeat a certain sequence of commands regularly, or run a bioinformatics program that may take a while, it becomes useful to put your shell commands in a script. Such a script can be easily and quickly (re-)executed, or submitted to a queue on a cluster (the latter is next week’s topic).\nWe will also start practicing with running programs/tools with a command-line interface (CLI), focusing on bioinformatics/genomics tools, and doing so inside shell scripts.\nSome of the things you will learn this week:\n\nWhy it is useful to collect your commands into shell scripts that can be rerun easily.\nThe basics of shell scripts including hell script header lines.\nWhy and how to adorn scripts with tests and echo statements.\nMore on shell variables and how to use them.\nUsing command-line arguments with your own scripts.\nif statements and true/false tests.\nRunning command-line programs (we focus on bioinformatics tools), and running them using shell scripts."
  },
  {
    "objectID": "week05/w5_overview.html#exercises-assignments",
    "href": "week05/w5_overview.html#exercises-assignments",
    "title": "Week 5: Shell scripting & CLI tools",
    "section": "3 Exercises & assignments",
    "text": "3 Exercises & assignments\n\nGraded assignment: Shell scripts (Due 9/29)\nExercises for this week"
  },
  {
    "objectID": "week05/w5_overview.html#readings",
    "href": "week05/w5_overview.html#readings",
    "title": "Week 5: Shell scripting & CLI tools",
    "section": "4 Readings",
    "text": "4 Readings\n\nOptional readings\n\nBuffalo Chapter 12: “Bioinformatics Shell Scripting, Writing Pipelines, and Parallelizing Tasks”\nThe latter part of this chapter is about using find, xargs, and Makefiles. These are somewhat tangential to the week’s topic of scripts, and we will not talk about them in class. As for Makefiles specifically, we will instead learn Nextflow later in this course, an alternative approach to workflow management,"
  },
  {
    "objectID": "week05/w5_exercises.html#exercise-1-a-shell-script-that-prints-a-specific-line",
    "href": "week05/w5_exercises.html#exercise-1-a-shell-script-that-prints-a-specific-line",
    "title": "Week 4 exercises",
    "section": "Exercise 1: A shell script that prints a specific line",
    "text": "Exercise 1: A shell script that prints a specific line\nWrite a shell script scripts/printline.sh that accepts two arguments, a file name and a line number, and prints the requested line from the file to screen. Additional notes:\n\nSince this is a simple utility script, I suggest to make the script not print anything other than the requested line from the file (i.e., no echo statements).\nDon’t forget the best-practice shell script header lines we discussed.\nWith an if statement, let the script check whether the correct number of arguments were passed to it, and if not, exit the script.\nTest your script by printing a couple of different lines from garrigos_data/meta/metadata.tsv. Also test that your argument-number-check works. (Add the lines of code to run the script various ways in your run_exercises.sh runner script.)\n\n\n\nHint 1: an overview of the steps to take (Click to expand)\n\n\nOpen a new text file and save it as scripts/printline.sh.\nIn the script, start with the shebang and set lines.\nYour script takes two arguments: a file name ($1) and a line number ($2) .\nCheck the number of arguments in an if statement like we did in class.\nCopy the $1 and $2 placeholder variables to descriptively named variables.\nTo print a specific line, think how you might combine head and tail to do this. If you’re at a loss, feel free to check out the next hint.\n\n\n\n\nHint 2: how to print a specific line number (Click to expand)\n\nFor example, to print line 4 of the Garrigos et al. metadata file:\nhead -n 4 garrigos_data/meta/metadata.tsv | tail -n 1\nERR10802879     10dpi   cathemerium\nHow this command works:\n\nhead -n 4 garrigos_data/meta/metadata.tsv prints the first 4 lines of that file.\nThose 4 lines are then piped into the tail command.\nWith -n 1, tail will only print the last line of its input: this will be line 4 of the original input file."
  },
  {
    "objectID": "week05/w5_exercises.html#exercise-2-a-script-to-run-trimgalore",
    "href": "week05/w5_exercises.html#exercise-2-a-script-to-run-trimgalore",
    "title": "Week 4 exercises",
    "section": "Exercise 2: A script to run TrimGalore",
    "text": "Exercise 2: A script to run TrimGalore\n\nIntroduction to TrimGalore\nTrimGalore is a tool that can trim and filter FASTQ files, removing:\n\nAny adapter sequences that are present in the reads1.\nPoor-quality bases at the start and end of the reads.\nReads that have become very short after the prior two steps.\n\nTrimGalore takes FASTQ files as input, and outputs filtered FASTQ files. When you have paired-end reads (as you do here), a single TrimGalore run should include both the R1 and R2 file for 1 sample.\n\n\n\n\n\n\nMore about TrimGalore (Click to expand)\n\n\n\n\n\nSeveral largely equivalent tools exist for this kind of FASTQ preprocessing — Trimmomatic and fastp are two other commonly used ones. TrimGalore itself is “just” a wrapper around yet another tool called Cutadapt, but it is simpler to use. Two advantages of TrimGalore are:\n\nIt will auto-detect the adapters that are present in your reads.\nIt can automatically run FastQC on the trimmed sequences.\n\n\n\n\nTrimGalore isn’t installed at OSC, but you can use a so-called “Conda environment”2 that I have created for it:\n# First load OSC's (mini)Conda module\nmodule load miniconda3/23.3.1-py310\n# Then load ('activate') the Conda environment with TrimGalore\nconda activate /fs/ess/PAS0471/jelmer/conda/trimgalore\nAfter running those two lines, check that you can run TrimGalore and which version you’re working with, as follows:\n# Note: the command is 'trim_galore' with an underscore\ntrim_galore --version\n            Quality-/Adapter-/RRBS-/Speciality-Trimming\n                    [powered by Cutadapt]\n                        version 0.6.10\nThen print the help info — you’ll be using this below!\ntrim_galore --help\n[output not shown]\n\n\n\nYour TrimGalore shell script\nWrite a shell script scripts/trimgalore.sh that runs TrimGalore on paired end-FASTQ files as follows:\n\nThe script should only run TrimGalore once, i.e. for one sample (two FASTQ files).\nYour script should accept and process arguments that specify the input FASTQ files and the output dir. (For the FASTQ files, I suggest you follow the strategy used in class where the script takes only the R1 file name as an argument3 and infers the name of the corresponding R2 file.)\nFor each of the items below, figure out the relevant TrimGalore option, and use that option:\n\nTell the program that the reads are paired-end4.\nSet the output dir.\nMake the program run FastQC on the trimmed FASTQ files.\n\nCheck what the default values are for the Phred quality score and read length thresholds. Do you understand what these do? You don’t have to change them here, the defaults will work fine for us.\n\n\n\nHint: See the relevant parts of the TrimGalore help pages (Click to expand)\n\n\ntrim_galore --help\n USAGE:\ntrim_galore [options] &lt;filename(s)&gt;\n\n--paired                This option performs length trimming of quality/adapter/RRBS trimmed reads for\n                        paired-end files.\n\n-o/--output_dir &lt;DIR&gt;   If specified all output will be written to this directory instead of the current\n                        directory. If the directory doesn't exist it will be created for you.\n\n-j/--cores INT          Number of cores to be used for trimming [default: 1].\n\n--fastqc                Run FastQC in the default mode on the FastQ file once trimming is complete.\n\n--fastqc_args \"&lt;ARGS&gt;\"  Passes extra arguments to FastQC.\n\n-a/--adapter &lt;STRING&gt;   Adapter sequence to be trimmed. If not specified explicitly, Trim Galore will\n                        try to auto-detect whether the Illumina universal, Nextera transposase or Illumina\n                        small RNA adapter sequence was used.\n\n-q/--quality &lt;INT&gt;      Trim low-quality ends from reads in addition to adapter removal. [...]\n                        Default Phred score: 20.\n\n--length &lt;INT&gt;          Discard reads that became shorter than length INT because of either\n                        quality or adapter trimming. A value of '0' effectively disables\n                        this behaviour. Default: 20 bp.\n\n\n\n\n\n\n\nWarning: Don’t use/adjust TrimGalore’s --cores option here (Click to expand)\n\n\n\n\n\nSince you’re running the program interactively on our single-core VS Code compute job, you only have 1 core at your disposal — and that’s TrimGalore’s default. Next week we’ll be submitting our scripts as batch jobs: that will give us the opportunity to use multiple cores with programs like this."
  },
  {
    "objectID": "week05/w5_exercises.html#exercise-3-run-your-trimgalore-script-once",
    "href": "week05/w5_exercises.html#exercise-3-run-your-trimgalore-script-once",
    "title": "Week 4 exercises",
    "section": "Exercise 3: Run your TrimGalore script once",
    "text": "Exercise 3: Run your TrimGalore script once\n\nRun your TrimGalore script for just the ERR10802863 sample in garrigos_data/fastq (remember to add the code to run the script in the run_exercises.sh runner script).\nCheck if everything went well; also take a look at TrimGalore’s output files, which should include new (filtered) FASTQ files as well as FastQC outputs and a text file for each sample. If something went wrong, go back to your trimgalore.sh script and try to fix it.\nTake a closer look at the output that TrimGalore printed to screen (note that most of that is also saved in the *_trimming_report.txt file in the output dir):\n\nIn each file (R1 and R2), in what percentage of reads were adapters detected?\nIn each file (R1 and R2), what percentage of bases were quality trimmed?\nWhat percentage of reads were removed because they were too short?5"
  },
  {
    "objectID": "week05/w5_exercises.html#exercise-4-run-your-trimgalore-script-for-all-samples",
    "href": "week05/w5_exercises.html#exercise-4-run-your-trimgalore-script-for-all-samples",
    "title": "Week 4 exercises",
    "section": "Exercise 4: Run your TrimGalore script for all samples",
    "text": "Exercise 4: Run your TrimGalore script for all samples\nOnce your single-sample run works, write a for loop in your runner script to run your TrimGalore script on all samples in the garrigos_data/fastq dir, and run that. (Don’t forget to take into account that you should loop over samples or R1 FASTQ files, not over all FASTQ files.)\n\n\n\n\n\n\nThis will take some time (~20 minutes) to run!\n\n\n\nAnd note that these FASTQ files are much smaller than regular ones. You’ll hopefully agree that we need to start using more computing power, and have the script run simultaneously rather than consecutively for each sample — we can do all of that by submitting the script as batch jobs, as we’ll see next week."
  },
  {
    "objectID": "week05/w5_exercises.html#solutions",
    "href": "week05/w5_exercises.html#solutions",
    "title": "Week 4 exercises",
    "section": "Solutions",
    "text": "Solutions\n\nExercise 1\n\n\nThe script (Click to expand)\n\n#!/bin/bash\nset -euo pipefail\n\n# Check the number of command-line arguments\nif [[ ! \"$#\" -eq 2 ]]; then\n    echo \"Error: wrong number of arguments\"\n    echo \"You provided $# arguments, while 2 are required.\"\n    echo \"Usage: printline.sh &lt;file&gt; &lt;line-number&gt;\"\n    exit 1\nfi\n\n# Copy the command-line arguments\ninput_file=$1\nline_nr=$2\n\n# Only print the requested line\nhead -n \"$line_nr\" \"$input_file\" | tail -n 1\n\n\n\nRunning and testing the script (Click to expand)\n\n\nTo run the script and make it print the 4th line of the Garrigos et al. metadata file:\nbash scripts/printline.sh garrigos_data/meta/metadata.tsv 4\nERR10802879     10dpi   cathemerium\nOr the 7th line:\nbash scripts/printline.sh garrigos_data/meta/metadata.tsv 7\nERR10802884     10dpi   control\nTo test that the argument-number-check works:\n# Only pass 1 argument:\nbash scripts/printline.sh garrigos_data/meta/metadata.tsv\nError: wrong number of arguments\nYou provided 1 arguments, while 2 are required\nUsage printline.sh &lt;file&gt; &lt;line-number&gt;\n# Pass 3 arguments:\nbash scripts/printline.sh garrigos_data/meta/metadata.tsv 4 7\nError: wrong number of arguments\nYou provided 3 arguments, while 2 are required\nUsage printline.sh &lt;file&gt; &lt;line-number&gt;\n\n\n\n\n\nExercise 2\n\n\nThe TrimGalore options you were looking for (Click to expand)\n\n\n--paired to indicate that the FASTQ files are paired-end.\n--output_dir (or equivalently, -o) to specify the output directory.\n--fastqc to run FastQC on the trimmed FASTQ files.\n\n\n\n\nThe TrimGalore shell script (Click to expand)\n\nSave this as e.g. scripts/trimgalore.sh:\n#!/bin/bash\nset -euo pipefail\n\n# Load TrimGalore\nmodule load miniconda3/23.3.1-py310\nconda activate /fs/ess/PAS0471/jelmer/conda/trimgalore\n\n# Copy the placeholder variables\nR1_in=$1\noutdir=$2\n\n# Infer the R2 FASTQ file name\nR2_in=${R1_in/_R1/_R2}\n\n# Report\necho \"# Starting script trimgalore.sh\"\ndate\necho \"# Input R1 FASTQ file:      $R1_in\"\necho \"# Input R2 FASTQ file:      $R2_in\"\necho \"# Output dir:               $outdir\"\necho\n\n# Create the output dir\nmkdir -p \"$outdir\"\n\n# Run TrimGalore\ntrim_galore \\\n    --paired \\\n    --fastqc \\\n    --output_dir \"$outdir\" \\\n    \"$R1_in\" \\\n    \"$R2_in\"\n\n# Report\necho\necho \"# Done with script trimgalore.sh\"\ndate\n\n\n\nQuality and length threshold options & interpretation (Click to expand)\n\n\nThe option -q (short notation) or --quality (long notation) can be used to adjust the base quality score threshold, which has the “Phred” unit. The default threshold is 20, which corresponds to an estimated 0.01 error rate. TrimGalore will trim bases with a lower quality score than this threshold from the ends of the reads. It will not remove or mask bases with lower quality scores elsewhere in the read.\nThe option -l (short notation) or --length (long notation) can be used to adjust the read length threshold: any reads that are shorter than this after adapter and quality trimming will be removed. The default threshold is 20 bp. (When the input is paired-end, the corresponding second read will also be removed, regardless of its length: paired-end FASTQ files always need to contain both the R1 and R2 for each pair; orphan reads would need to be stored in a separate file.)\n\n\n\n\n\nExercise 3\n\n\n1 - Run the script (Click to expand)\n\n# Exercise 2\nR1=garrigos_data/fastq/ERR10802863_R1.fastq.gz\nbash scripts/trimgalore.sh \"$R1\" results/trimgalore\n# Starting script trimgalore.sh\nThu Mar 28 10:34:24 EDT 2024\n# Input R1 FASTQ file:      garrigos_data/fastq/ERR10802863_R1.fastq.gz\n# Input R2 FASTQ file:      garrigos_data/fastq/ERR10802863_R2.fastq.gz\n# Output dir:               results/trimgalore\n\nMulticore support not enabled. Proceeding with single-core trimming.\nPath to Cutadapt set as: 'cutadapt' (default)\nCutadapt seems to be working fine (tested command 'cutadapt --version')\nCutadapt version: 4.4\n\n# [...output truncated...]\n\n\n\n2 - Check the output files (Click to expand)\n\nls -lh results/trimgalore\ntotal 42M\n-rw-rw----+ 1 jelmer PAS0471 2.4K Mar 28 10:49 ERR10802863_R1.fastq.gz_trimming_report.txt\n-rw-rw----+ 1 jelmer PAS0471 674K Mar 28 10:49 ERR10802863_R1_val_1_fastqc.html\n-rw-rw----+ 1 jelmer PAS0471 349K Mar 28 10:49 ERR10802863_R1_val_1_fastqc.zip\n-rw-rw----+ 1 jelmer PAS0471  20M Mar 28 10:49 ERR10802863_R1_val_1.fq.gz\n-rw-rw----+ 1 jelmer PAS0471 2.3K Mar 28 10:49 ERR10802863_R2.fastq.gz_trimming_report.txt\n-rw-rw----+ 1 jelmer PAS0471 676K Mar 28 10:50 ERR10802863_R2_val_2_fastqc.html\n-rw-rw----+ 1 jelmer PAS0471 341K Mar 28 10:50 ERR10802863_R2_val_2_fastqc.zip\n-rw-rw----+ 1 jelmer PAS0471  21M Mar 28 10:49 ERR10802863_R2_val_2.fq.gz\n\n\n\n3 - Check the trimming results (Click to expand)\n\n\nNote that the adapter- and quality trimming results summary for the R1 and R2 files are separated widely among all the output that TrimGalore prints — they are printed below (first for the R1, then for the R2 file), and the answers are:\n\n13.0% (R1) and 10.1% (R2) of reads had adapters\n3.6% (R1) and 3.5% (R2) of bases were quality-trimmed\n\n=== Summary ===\n\nTotal reads processed:                 500,000\nReads with adapters:                    65,070 (13.0%)\nReads written (passing filters):       500,000 (100.0%)\n\nTotal basepairs processed:    35,498,138 bp\nQuality-trimmed:               1,274,150 bp (3.6%)\nTotal written (filtered):     34,138,461 bp (96.2%)\n=== Summary ===\n\nTotal reads processed:                 500,000\nReads with adapters:                    50,357 (10.1%)\nReads written (passing filters):       500,000 (100.0%)\n\nTotal basepairs processed:    36,784,563 bp\nQuality-trimmed:               1,283,793 bp (3.5%)\nTotal written (filtered):     35,440,230 bp (96.3%)\nJust before the FastQC logs is a line that reports how many reads were removed due to the length filter — the answer is 6.79%.\nNumber of sequence pairs removed because at least one read was shorter than the length cutoff (20 bp): 33955 (6.79%)\n\n\n\n\n\nExercise 4\n\n\nRun the script for all samples (Click to expand)\n\nAdd this loop code to your runner script (run/run_exercises.sh) and run it:\nfor R1 in garrigos_data/fastq/*_R1.fastq.gz; do\n    bash scripts/trimgalore.sh \"$R1\" results/trimgalore\ndone\n# (Output not shown, same as earlier but should repeat for each sample)\n\n\n\nCheck the output files (Click to expand)\n\nls -lh results/trimgalore\ntotal 949M\n-rw-rw----+ 1 jelmer PAS0471  20M Mar 28 11:18 ERR10802863_R1.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 2.4K Mar 28 11:17 ERR10802863_R1.fastq.gz_trimming_report.txt\n-rw-rw----+ 1 jelmer PAS0471 674K Mar 28 11:18 ERR10802863_R1_val_1_fastqc.html\n-rw-rw----+ 1 jelmer PAS0471 349K Mar 28 11:18 ERR10802863_R1_val_1_fastqc.zip\n-rw-rw----+ 1 jelmer PAS0471  21M Mar 28 11:18 ERR10802863_R2.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 2.3K Mar 28 11:18 ERR10802863_R2.fastq.gz_trimming_report.txt\n-rw-rw----+ 1 jelmer PAS0471 676K Mar 28 11:18 ERR10802863_R2_val_2_fastqc.html\n-rw-rw----+ 1 jelmer PAS0471 341K Mar 28 11:18 ERR10802863_R2_val_2_fastqc.zip\n-rw-rw----+ 1 jelmer PAS0471  21M Mar 28 11:19 ERR10802864_R1.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 2.7K Mar 28 11:18 ERR10802864_R1.fastq.gz_trimming_report.txt\n-rw-rw----+ 1 jelmer PAS0471 675K Mar 28 11:19 ERR10802864_R1_val_1_fastqc.html\n-rw-rw----+ 1 jelmer PAS0471 349K Mar 28 11:19 ERR10802864_R1_val_1_fastqc.zip\n-rw-rw----+ 1 jelmer PAS0471  22M Mar 28 11:19 ERR10802864_R2.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 2.6K Mar 28 11:19 ERR10802864_R2.fastq.gz_trimming_report.txt\n-rw-rw----+ 1 jelmer PAS0471 671K Mar 28 11:19 ERR10802864_R2_val_2_fastqc.html\n-rw-rw----+ 1 jelmer PAS0471 336K Mar 28 11:19 ERR10802864_R2_val_2_fastqc.zip\n-rw-rw----+ 1 jelmer PAS0471  21M Mar 28 11:20 ERR10802865_R1.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 2.6K Mar 28 11:19 ERR10802865_R1.fastq.gz_trimming_report.txt\n-rw-rw----+ 1 jelmer PAS0471 682K Mar 28 11:20 ERR10802865_R1_val_1_fastqc.html\n-rw-rw----+ 1 jelmer PAS0471 361K Mar 28 11:20 ERR10802865_R1_val_1_fastqc.zip\n[...output truncated...]"
  },
  {
    "objectID": "week05/w5_exercises.html#footnotes",
    "href": "week05/w5_exercises.html#footnotes",
    "title": "Week 4 exercises",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n Adapters are always added to DNA/cDNA fragments of interest prior to Illumina sequencing, and the last bases of a read can “read-through” into the adapter if the fragment is shorter than the read length.↩︎\n We’ll talk more about Conda environments next week.↩︎\n Alternatively, it could take both R1 and R2 files as arguments.↩︎\n If you don’t do this, TrimGalore will process the two FASTQ files independently.↩︎\n Hint: this info is printed at the very end, and for both files at once.↩︎"
  },
  {
    "objectID": "week05/w5_1_scripts.html#overview-and-setting-up",
    "href": "week05/w5_1_scripts.html#overview-and-setting-up",
    "title": "Shell scripting",
    "section": "Overview and setting up",
    "text": "Overview and setting up\n\nThis and next week\nThis week, you will learn how to write shell scripts and then how to use shell scripts to run programs, like various bioinformatics tools, with command-line interfaces (CLIs).\nOrganizing your code into scripts that do a single task is generally a good idea, and it is particularly important if you want to be able to analyze your data at OSC: this is best done by submitting shell scrits as “batch jobs”, as we’ll learn in the next two weeks.\n\n\nThis session\nIn this session, we will talk about:\n\nThe basics of shell scripts\nBoilerplate shell script header lines: shebang and safe settings\nCommand-line arguments to scripts\nShell variables once again\n\n\n\n\n\n\n\n\nVS Code improvements\n\n\n\nThese two settings will make life easier when writing shell scripts in VS Code.\nFirst, we’ll add a keyboard shortcut to send code from your editor to the terminal. This is the same type of behavior that you may be familiar with from RStudio, and will mean that won’t have to copy-and-paste code into the terminal:\n\nClick the  (bottom-left) =&gt; Keyboard Shortcuts.\nFind Terminal: Run Selected Text in Active Terminal, click on it, then add a shortcut, e.g. Ctrl+Enter1.\n\nIn VS Code’s editor pane, the entire line that your cursor is on is selected by default. As such, your keyboard shortcut will send the line that your cursor is in to the terminal; you can also send multiple lines to the terminal after selecting them.\n\nSecond, we’ll add the ShellCheck VS Code extension. This extension checks shell scripts for errors like referencing variables that have not been assigned. Potential problems show up as colored squiggly lines. It also provided links with more information about the error and how to improve your code. This extension is incredibly useful!\n\nClick on the Extensions icon in the far left (narrow) sidebar in VS Code.\nType “shellcheck” and click the small purple “Install” button next to the entry of this name (the description should include “Timon Wong”, who is the author)."
  },
  {
    "objectID": "week05/w5_1_scripts.html#introduction-to-shell-scripts",
    "href": "week05/w5_1_scripts.html#introduction-to-shell-scripts",
    "title": "Shell scripting",
    "section": "1 Introduction to shell scripts",
    "text": "1 Introduction to shell scripts\nMany bioinformatics tools (programs/software) that are used to analyze omics data are run from the command line. We can run them using command line expressions that are structurally very similar to how we’ve been using basic Unix shell commands.\nHowever, we’ve been running shell commands in a manner that we may call “interactive”, by typing or pasting them into the shell, and then pressing Enter. But when you run bioinformatics tools, it is in most cases a much better idea to run them via shell scripts, which are plain-text files that contain shell code.\n\n“Most Bash scripts in bioinformatics are simply commands organized into a rerunnable script with some added bells and whistles to check that files exist and ensuring any error causes the script to abort.” — Buffalo Ch. 12\n\nTherefore, shell scripts are relatively straightforward to write given what you already know! In this session, we will learn about those bells and whistles from the quote above.\n\n\n\n\n\n\nBash vs. shell\n\n\n\nSo far, we’ve mostly used talked about the Unix shell and shell scripts. The quote above uses the word “Bash”, and we’ll see that term more often this week. The difference is this: there are multiple Unix shell (language) variants and the specific one we have been using, which is also by far the most common, is the Bash shell. Our shell scripts are therefore in the Bash language and can be specifically called Bash scripts.\n\n\n\n\nRunning commands interactively vs. via scripts\nBefore we see why it’s often a better idea to use scripts than to run code interactively, let’s go through a minimal example of both approaches with the tool FastQC, which performs FASTQ file quality control (QC; more on FastQC in the next session).\n\nHere’s how you can run FastQC on one FASTQ file — the command fastqc followed by a file name:\nfastqc data/fastq/A_R1.fastq.gz\nThis is what a minimal shell script to do the same thing would look like:\n#!/bin/bash\nfastqc data/fastq/A_R1.fastq.gz\nIf the above shell script is saved as fastqc.sh in our working dir, it can be executed as follows:\nbash fastqc.sh\n\n\n\n\nWhy use shell scripts\nThere are several general reasons why it can be beneficial to use shell scripts instead of running code interactively line-by-line:\n\nIt is a good way to save and organize your code.\nYou can easily rerun scripts and re-use them in similar contexts.\nRelated to the point above, they provide a first step towards automating the set of analyses in your project.\nWhen your code is tucked away in a shell script, you only have to call the script to run what is potentially a large set of commands.\n\nAnd very importantly for our purposes at OSC, we can submit scripts as “batch jobs” to the compute job scheduling program (which is called Slurm), and this allows us to:\n\nRun scripts remotely without needing to stay connected to the running process, or even to be connected at all to it: we can submit a script, log out from OSC and shut down our computer, and it will still run.\nEasily run analyses that take many hours or even multiple days.\nRun a script many times simultaneously, such as for different files/samples.\n\n\n\n\nSummary of what we need to learn about\n\nWriting shell scripts (this week)\nMaking software available at OSC (next week)\nSubmitting scripts to the Slurm job scheduler (in two weeks)"
  },
  {
    "objectID": "week05/w5_1_scripts.html#a-basic-shell-script",
    "href": "week05/w5_1_scripts.html#a-basic-shell-script",
    "title": "Shell scripting",
    "section": "2 A basic shell script",
    "text": "2 A basic shell script\n\n2.1 A one-line script to start\nCreate your first script, printname.sh (note that shell scripts usually have the extension .sh) as follows:\n# First, let's create and move into a new dir\nmkdir -p week04/scripts\ncd week04\n# Create an empty file\ntouch scripts/printname.sh\nA nice VS Code trick is that is if you hold Ctrl (Cmd on Mac) while hovering over a file path in the terminal, the path should become underlined and you can click on it to open the file. Try that with the printname.sh script2.\nOnce the file is open in your editor pane, type or paste the following inside the script:\necho \"This script will print a first and a last name\"\nShell scripts mostly contain the same regular Unix shell code that we have gotten familiar with, but have so far directly typed in the terminal. As such, our single line with an echo command constitutes a functional shell script!\nOne way of running the script is by typing bash followed by the path to the script:\nbash scripts/printname.sh\nThis script will print a first and a last name\nThat worked! The script doesn’t yet print any names like it “promises” to do, but we will add that functionality in a little bit. But first, we’ll learn about two header lines that are good practice to add to every shell script.\n\n\n\n\n\n\n\nAuto Save in VS Code (Click to expand)\n\n\n\n\n\nAny changes you make to this and other files in the editor pane should be immediately, automatically saved by VS Code. If that’s not happening for some reason, you should see an indication of unsaved changes like a large black dot next to the script’s file name in the editor pane tab header.\nIf the file is not auto-saving, you can always save it manually (including with Ctrl/Cmd+S) like you would do in other programs. However, it may be convenient to turn Auto Save on: press Ctrl/Cmd+Shift+S to open the Command Palette and type “Auto Save”. You should see an option “Toggle Auto Save”: click on that.\n\n\n\n\n\n\n2.2 Shebang line\nWe use a so-called “shebang” line as the first line of a script to indicate which computer language our script uses. More specifically, this line tell the computer where to find the binary (executable) that will run our script.\n#!/bin/bash\nSuch a line starts with #! (hash-bang), basically marking it as a special type of comment. After those two characters, we give the file path of the relevant program: in our case Bash, which itself is just a program with an executable file that is located at /bin/bash on Linux and Mac computers.\nWhile not always strictly necessary, adding a shebang line to every shell script is good practice, especially when you submit your script to OSC’s Slurm queue, as we’ll do later.\n\n\n\n2.3 Shell script settings\nAnother best-practice line you should add to your shell scripts will change some default settings to safer alternatives.\n\nBad default shell settings\nThe following two default settings of the Bash shell are bad ideas inside scripts:\n\nWhen you reference a non-existent (“unset”) variable, the shell replaces that with nothing without complaint:\necho \"Hello, my name is $myname. What is yours?\"\nHello, my name is . What is yours?\nIn scripts, this can lead to all sorts of downstream problems, because you very likely tried and failed to do something with an existing variable (e.g. you misspelled its name, or forgot to assign it altogether). Even more problematically, this can lead to potentially very destructive file removal, as the box below illustrates.\nA Bash script keeps running after encountering errors. That is, if an error is encountered when running, say, line 2 of a script, any remaining lines in the script will nevertheless be executed.\nIn the best case, this is a waste of computer resources, and in worse cases, it can lead to all kinds of unintended consequences. Additionally, if your script prints a lot of output, you might not notice an error somewhere in the middle if it doesn’t produce more errors downstream. But the downstream results from what we at that point might call a “zombie script” can still be completely wrong.\n\n\n\n\n\n\n\n\nAccidental file removal with unset variables\n\n\n\nThe shell’s default behavior of ignoring the referencing of unset variables can lead to accidental file removal as follows:\n\nUsing a variable, we try to remove some temporary files whose names start with tmp_:\n# NOTE: DO NOT run this!\ntemp_prefix=\"temp_\"\nrm \"$tmp_prefix\"*\nUsing a variable, we try to remove a temporary directory:\n# NOTE: DO NOT run this!\ntempdir=output/tmp\nrm -r $tmpdir/*\n\n\n\nAbove, the text specified the intent of the commands. What would have actually happened? (Click to expand)\n\nIn both examples, there is a similar typo: temp vs. tmp, which means that we are referencing a (likely) non-existent variable.\n\nIn the first example, rm \"$tmp_prefix\"* would have been interpreted as rm *, because the non-existent variable is simply ignored. Therefore, we would have removed all files in the current working directory.\nIn the second example, along similar lines, rm -rf $tmpdir/* would have been interpreted as rm -rf /*. Horrifyingly, this would attempt to remove the entire filesystem: recall that a leading / in a path is a computer’s root directory. (-r makes the removal recursive and -f makes forces removal).\n\n\nThese kinds of accidents are especially likely to happen inside scripts, where it is common to use variables and to work non-interactively.\nBefore you get too scared of creating terrible damage, note that at OSC, you would not be able to remove any essential files3, since you don’t have the permissions to do so. On your own computer, this could be more genuinely dangerous, though even there, you would not be able to remove operating system files without specifically requesting “admin” rights.\n\n\n\n\n\nSafer settings\nThe following three settings will make your shell scripts more robust and safer. With these settings, the script terminates with an appropriate error message if:\n\nset -u — an “unset” (non-existent) variable is referenced.\nset -e — almost any error occurs.\nset -o pipefail — an error occurs in a shell “pipeline” (e.g., sort | uniq).\n\nWe can change all of these settings in one line in a script:\nset -e -u -o pipefail\nOr even more concisely:\nset -euo pipefail\n\n\n\n\n2.4 Adding the header lines to our script\nAdd the discussed header lines to your printname.sh script, so it will now contain the following:\n#!/bin/bash\nset -euo pipefail\n\necho \"This script will print a first and a last name\"\nAnd run the script again:\nbash scripts/printname.sh\nThis script will print a first and a last name\nThat didn’t change anything to the output, but at least we confirmed that the script still works.\n\n\n\n\n\n\nCan I run scripts without the bash command? (Click to expand)\n\n\n\n\n\nIt’s possible to execute scripts “as a command”, i.e. by only typing their path — for example:\n# If the script is in a different dir:\nsandbox/printname.sh\n\n# If the script is in your working dir:\n./printname.sh\nTo be able to do this, the script:\n\nNeeds to have a shebang line so the computer knows which language to execute the script with (which is what the bash prefix also does).\nThe script needs to be “executable”. You can do that by changing the file permissions — we will talk about file permissions next week.\n\n\nWhy do I need ./ in the example above if the script is in my working dir? The ./ is necessary to make it explicit that we are referring to a file name: otherwise, when running just printname.sh, the shell would look for a command or program of that name, and wouldn’t be able to find it. With yet another step, it is possible to basically add your script to the computer’s registry of commands/programs, but we won’t cover that here (Google $PATH if you’re curious)."
  },
  {
    "objectID": "week05/w5_1_scripts.html#command-line-arguments-for-scripts",
    "href": "week05/w5_1_scripts.html#command-line-arguments-for-scripts",
    "title": "Shell scripting",
    "section": "3 Command-line arguments for scripts",
    "text": "3 Command-line arguments for scripts\n\n3.1 Executing a script with arguments\nWhen you execute a script, you can pass arguments to it, such as a file to operate on. This is much like when you provide a command like ls with arguments:\n# [Don't run any of this, these are just syntax examples]\n\n# Run ls without arguments:\nls\n\n# Pass 1 filename as an argument to ls:\nls data/sampleA.fastq.gz\n\n# Pass 2 filenames as arguments to ls, separated by spaces:\nls data/sampleA.fastq.gz data/sampleB.fastq.gz\nAnd here is what it looks like to pass arguments to scripts:\n# [Don't run any of this, these are just syntax examples]\n\n# Run scripts without arguments:\nbash scripts/fastqc.sh\nbash scripts/printname.sh\n\n# Run scripts with 1 or 2 arguments:\nbash scripts/fastqc.sh data/sampleA.fastq.gz  # 1 argument: a filename\nbash scripts/printname.sh John Doe            # 2 arguments: strings representing names\nIn the next section, we’ll see how we can work with these arguments inside our scripts.\n\n\n\n3.2 Placeholder variables\nInside the script, any command-line arguments that you pass to it are automatically available in “placeholder” variables. Specifically:\n\nAny first argument will be assigned to the variable $1\nAny second argument will be assigned to $2\nAny third argument will be assigned to $3, and so on.\n\n\n\n\n In the calls to fastqc.sh and printname.sh above, what are the placeholder variables and their values? (Click for the solution)\n\n\n\nIn bash scripts/fastqc.sh data/sampleA.fastq.gz, a single argument, data/sampleA.fastq.gz, is passed to the script, and will be assigned to $1.\nIn bash scripts/printname.sh John Doe, two arguments are passed to the script: the first one (John) will be stored in $1, and the second one (Doe) in $2.\n\n\n\nHowever, while they are made available, these placeholder variables are not “automagically” used. So, unless we explicitly include code in the script to do something with these variables, nothing extra really happens.\nTherefore, let’s add some code to our printname.sh script to “process” any first and last name that are passed to the script. For now, our script will simply echo the placeholder variables, so that we can see what happens:\n#!/bin/bash\nset -euo pipefail\n\necho \"This script will print a first and a last name\"\necho \"First name: $1\"\necho \"Last name: $2\"\n\n# [Paste this into you script - don't enter this directly in your terminal.]\nNext, we’ll run the script, passing the arguments John and Doe:\nbash scripts/printname.sh John Doe\nThis script will print a first and a last name\nFirst name: John\nLast name: Doe\n\n\n Exercise: Command-line arguments\nIn each scenario that is described below, think about what might happen. Then, run the script as instructed in the scenario to test your prediction.\n\nRunning the script printname.sh without passing arguments to it.\n\n\n\nClick here for the solution\n\nThe script will error out because we are referencing variables that don’t exist: since we didn’t pass command-line arguments to the script, the $1 and $2 have not been set.\nbash scripts/printname.sh\nprintname.sh: line 5: $1: unbound variable\n\n\nAfter commenting out the line with set settings, running the script again without passing arguments to it.\n\n\n\nClick here to learn what “commenting out” means\n\nYou can deactivate a line of code without removing it (because perhaps you’re not sure you may need this line in the end) by inserting a # as the first character of that line. This is often referred to as “commenting out” code.\nFor example, below I’ve commented out the ls command, and nothing will happen if I run this line:\n#ls\n\n\n\nClick here for the solution\n\nThe script will run in its entirety and not throw any errors, because we are now using default Bash settings such that referencing non-existent variables does not throw an error. Of course, no names are printed either, since we didn’t specify any:\nbash scripts/printname.sh\necho \"First name:\"\necho \"Last name:\"\nBeing “commented out”, the set line should read:\n#set -euo pipefail\n\n\nDouble-quoting the entire name when you run the script, e.g.: bash scripts/printname.sh \"John Doe\".\n\n\n\nClick here for the solution\n\nBecause we are quoting \"John Doe\", both names are passed as a single argument and both names end up in $1, the “first name”:\nbash scripts/printname.sh \"John Doe\"\necho \"First name: John Doe\"\necho \"Last name:\"\n\nTo get back to where you were, remove the # you inserted in the script in step 2 above to reactive the set line.\n\n\n\n\n3.3 Copying placeholders to variables with descriptive names\nWhile you can use the $1-style placeholder variables throughout your script, I find it very useful to copy them to more descriptively named variables — for example:\n#!/bin/bash\nset -euo pipefail\n\nfirst_name=$1\nlast_name=$2\n\necho \"This script will print a first and a last name\"\necho \"First name: $first_name\"\necho \"Last name: $last_name\"\nUsing descriptively named variables in your scripts has several advantages, such as:\n\nIt will make your script easier to understand for others and for your future self.\nIt will make it less likely that you make errors in your script in which you use the wrong variable in the wrong place.\n\n\n\n\n\n\n\nOther variables that are automatically available inside scripts\n\n\n\n\n$0 contains the script’s file name.\n$# contains the number of command-line arguments passed to the script."
  },
  {
    "objectID": "week05/w5_1_scripts.html#more-on-shell-variables",
    "href": "week05/w5_1_scripts.html#more-on-shell-variables",
    "title": "Shell scripting",
    "section": "4 More on shell variables",
    "text": "4 More on shell variables\n\n4.1 Why use variables\nAbove, we saw that variables are useful to be able to pass arguments to a script, so you can easily rerun a script with a different input file / settings / etc. Let’s take a step back and think about variables and their uses once again.\n\n“Processing pipelines having numerous settings that should be stored in variables (e.g., which directories to store results in, parameter values for commands, input files, etc.).\nStoring these settings in a variable defined at the top of the file makes adjusting settings and rerunning your pipelines much easier.\nRather than having to change numerous hardcoded values in your scripts, using variables to store settings means you only have to change one value—the value you’ve assigned to the variable.”\n— Buffalo ch. 12\n\nIn brief, recall that variables are especially useful for things that you refer to repeatedly and/or are subject to change.\n\n\n\n4.2 Quoting variables\nI have mentioned that it is good practice to quote variables (i.e. to use \"$myvar\" instead of $myvar). So what can happen if you don’t do this?\n# Start by making and moving into a dir to create some messy files\nmkdir sandbox\ncd sandbox\nIf a variable’s value contains spaces:\n# Assign a string with spaces to variable 'today', and print its value:\ntoday=\"Tue, Mar 26\"\necho $today\nTue, Mar 26\n# Try to create a file with a name that includes this variable: \ntouch README_$today.txt\n\n# (Using the -1 option to ls will print each entry on its own line)\nls -1\n26.txt\nMar\nREADME_Tue,\nOops! The shell performed “field splitting” to split the value into three separate units — as a result, three files were created. This can be avoided by quoting the variable:\ntouch README_\"$today\".txt\nls -1\nREADME_Tue, Mar 26.txt\nAdditionally, without quoting, we can’t explicitly indicate where a variable name ends:\n# We intend to create a file named 'README_Tue, Mar 26_final.txt'\ntouch README_$today_final.txt\nls -1\nREADME_.txt\n\n\nDo you understand what happened here? (Click for the solution)\n\nWe have assigned a variable called $today, but the shell will instead look for a variable called $today_final. This is because we have not explicitly indicated where the variable name ends, so the shell will include all characters until it hits a character that cannot be part of a shell variable name: in this case a period, ..\n\nQuoting solves this, too:\ntouch README_\"$today\"_final.txt\nls -1\nREADME_Tue, Mar 26_final.txt\n\n\n\n\n\n\n\nCurly braces notation: ${myvar} (Click to expand)\n\n\n\n\n\nThe $var notation to refer to a variable in the shell is actually an abbreviation of the full notation, which includes curly braces:\necho ${today}\nTue, Mar 26\nPutting variable names between curly braces will also make it clear where the variable name begins and ends, although it does not prevent field splitting:\ntouch README_${today}_final.txt\n\nls\n26_final.txt  Mar  README_Tue,\nBut you can combine curly braces and quoting:\ntouch README_\"${today}\"_final.txt\n\nls\n'README_Tue, Mar 26_final.txt'\n\n\n\n\n\n\n\n\n\nQuoting as “escaping” special meaning & double vs. single quotes (Click to expand)\n\n\n\n\n\nBy double-quoting a variable, we are essentially escaping (or “turning off”) the default special meaning of the space as a separator, and are asking the shell to interpret it as a literal space.\nSimilarly, double quotes will escape other “special characters”, such as shell wildcards. Compare:\n# Due to shell expansion, this will echo/list all files in the current working dir\necho *\n18.txt Aug README_Thu, README_Thu, Aug 18.txt\n# This will simply print the literal \"*\" character \necho \"*\"\n*\nHowever, double quotes not turn off the special meaning of $ (which is to denote a string as a variable):\necho \"$today\"\nThu, Aug 18\n…but single quotes will:\necho '$today'\n$today\n\n\n\n\n\n\n\n4.3 Variable names\nIn the shell, variable names:\n\nCan contain letters, numbers, and underscores\nCannot contain spaces, periods (.), dashes (-), or other special symbols4.\nCannot start with a number\n\nTry to make your variable names descriptive, like $input_file above, as opposed to say $x and $myvar.\nThere are multiple ways of distinguishing words in the absence of spaces, such as $inputFile and $input_file: I prefer the latter, which is called “snake case”.\n\n\n\n\n\n\nCase and environment variables\n\n\n\nAll-uppercase variable names are pretty commonly used — and recall that so-called environment variables are always in uppercase (we’ve seen $USER and $HOME). Alternatively, you can use lowercase for variables and uppercase for “constants”, like when you include certain file paths or settings in a script without allowing them to be set from outside of the script.\n\n\n# Move out of the 'sandbox' dir (back to /fs/ess/PAS2700/users/$SUER/week04)\ncd .."
  },
  {
    "objectID": "week05/w5_1_scripts.html#bonus-conditionals",
    "href": "week05/w5_1_scripts.html#bonus-conditionals",
    "title": "Shell scripting",
    "section": "5 Bonus: Conditionals",
    "text": "5 Bonus: Conditionals\nWith conditionals like if statements, we can run one or more commands only if some condition is true. Also, we can run a different set of commands if the condition is not true. This can be useful in shell scripts because we may, for instance, want to process a file differently depending on its file type.\n\n5.1 Basic syntax\nThis is the basic syntax of an if statement in Bash (note that similarities with for loop syntax):\nif &lt;test&gt;; then\n    # Command(s) to run if the condition is true\nfi\nWe’ll have to add an else clause to run alternative command(s) if the condition is false:\nif &lt;test&gt;; then\n    # Command(s) to run if the condition is true\nelse\n    # Commands(s) to run if the condition is false\nfi\n\n\n\n5.2 String comparisons\nFirst, an if statement that tests the file type of say an input file, and runs different code depending on the result:\n# [Hypothetical example - don't run this]\n# Say we have a variable $filetype that contains a file's type\n\nif [[ \"$filetype\" == \"fastq\" ]]; then\n    echo \"Processing FASTQ file...\"\n    # Commands to process the FASTQ file\nelse\n    echo \"Unknown filetype!\"\n    exit 1\nfi\nIn the code above, note that:\n\nThe double square brackets [[ ]] represent a test statement5.\nThe spaces bordering the brackets on the inside are necessary: [[\"$filetype\" == \"fastq\"]] would fail!\nDouble equals signs (==) are common in programming to test for equality — this is to contrast it with a single =, which is used for variable assignment.\nWhen used inside a script, the exit command will stop the execution of the script. With exit 1, the exit status of our script is 1: in bash, an exit status of 0 means success — any other integer, including 1, means failure.\n\n\n\n\n\n\n\n\n\nString comparison\nEvaluates to true if\n\n\n\n\nstr1 == str2\nStrings str1 and str2 are identical6\n\n\nstr1 != str2        \nStrings str1 and str2 are different                \n\n\n-z str\nString str is null/empty (useful with variables)\n\n\n\n\n\n\n5.3 File tests\nThe code below tests whether an input file exists using the file test -f and if it does not (hence the !), it will stop the execution of the script:\n# [Hypothetical example - don't run this]\n\n# '-f' is true if the file exists,\n# and '! -f' is true if the file doesn't exist\nif [[ ! -f \"$fastq_file\" ]]; then\n    echo \"Error: Input file $fastq_file not found!\"\n    exit 1\nfi\n\n\n\n\n\n\n\n\nFile/dir test\nEvaluates to true if\n\n\n\n\n-f file\nfile exists and is a regular file (not a dir or link)\n\n\n-d dir\ndir exists and is a directory          \n\n\n-e file/dir\nfile/dir exists\n\n\n\n\n\n\n5.4 Integer (number) comparisons\nTo avoid unexpected or hard-to-understand errors later on in a shell script, we may choose to test at the beginning whether the correct number of arguments was passed to the script, and abort the script if this is not the case:\n# [Hypothetical example - don't run this]\n\nif [[ ! \"$#\" -eq 2 ]]; then\n    echo \"Error: wrong number of arguments\"\n    echo \"You provided $# arguments, while 2 are required.\"\n    echo \"Usage: printname.sh &lt;first-name&gt; &lt;last-name&gt;\"\n    exit 1\nfi\n\n\n\n\n\n\n\n\nInteger comparisons\nEvaluates to true if\n\n\n\n\nint1 -eq int2\nIntegers int1 and int2 are equal\n\n\nint1 -ne int2\nIntegers int1 and int2 are not equal\n\n\nint1 -lt int2\nInteger int1 is less than int2 (-le for less than or equal to)\n\n\nint1 -gt int2\nInteger int1 is greater than int2 (-ge for greater than or equal to)\n\n\n\n\n\n\n\n\n\n\nAnother integer comparison example (Click to expand)\n\n\n\n\n\nSay that we want to run a program with options that depend on our number of samples. With the number of samples determined from the number of lines in a hypothetical file samples.txt and stored in a variable $n_samples, we can test if the number is greater than 9 with \"$n_samples\" -gt 9, where gt stands for “greater than”:\n# [Hypothetical example - don't run this]\n\n# Store the number of samples in variable $n_samples:\nn_samples=$(cat samples.txt | wc -l)\n\n# With '-gt 9', the if statement tests whether the number of samples is greater than 9:\nif [[ \"$n_samples\" -gt 9 ]]; then\n    # Commands to run if nr of samples &gt;9:\n    echo \"Processing files with algorithm A\"\nelse\n    # Commands to run if nr of samples is &lt;=9:\n    echo \"Processing files with algorithm B...\"\nfi\n\n\n\n\n\n\n\n\n\nCombining multiple expressions with && and || (Click to expand)\n\n\n\n\n\nTo test for multiple conditions at once, use the && (“and”) and || (“or”) shell operators — for example:\n\nIf the number of samples is less than 100 and at least 50 (i.e. 50-99):\nif [[ \"$n_samples\" -lt 100 && \"$n_samples\" -ge 50 ]]; then\n    # Commands to run if the number of samples is 50-99\nfi\nIf either one of two FASTQ files don’t exist:\nif [[ ! -f \"$fastq_R1\" || ! -f \"$fastq_R2\" ]]; then\n    # Commands to run if either file doesn't exist - probably report error & exit\nfi\n\n\n\n\n\n\n Exercise: No middle names allowed!\nIn your printname.sh script, add the if statement from above that tests whether the correct number of arguments were passed to the script. Then, try running the script consecutively with 1, 2, or 3 arguments.\n\n\nStart with this printname.sh script we wrote above.\n\n#!/bin/bash\nset -euo pipefail\n\nfirst_name=$1\nlast_name=$2\n\necho \"This script will print a first and a last name\"\necho \"First name: $first_name\"\necho \"Last name: $last_name\"\n\n\n\nClick for the solution\n\nNote that the if statement should come before you copy the variables to first_name and last_name, otherwise you get the “unbound variable error” before your descriptive custom error, when you pass 0 or 1 arguments to the script.\nThe final script:\n#!/bin/bash\nset -euo pipefail\n\nif [[ ! \"$#\" -eq 2 ]]; then\n    echo \"Error: wrong number of arguments\"\n    echo \"You provided $# arguments, while 2 are required.\"\n    echo \"Usage: printname.sh &lt;first-name&gt; &lt;last-name&gt;\"\n    exit 1\nfi\n\nfirst_name=$1\nlast_name=$2\n\necho \"This script will print a first and a last name\"\necho \"First name: $first_name\"\necho \"Last name: $last_name\"\nRun it with different numbers of arguments:\nbash scripts/printname.sh Jelmer\nError: wrong number of arguments\nYou provided 1 arguments, while 2 are required.\nUsage: printname.sh &lt;first-name&gt; &lt;last-name&gt;\nbash scripts/printname.sh Jelmer Poelstra\nFirst name: Jelmer\nLast name: Poelstra\nbash scripts/printname.sh Jelmer Wijtze Poelstra\nError: wrong number of arguments\nYou provided 3 arguments, while 2 are required.\nUsage: printname.sh &lt;first-name&gt; &lt;last-name&gt;\n\n\n\n Exercise: Conditionals II\nOpen a new script sandbox.sh and in it, write an if statement that tests whether the script scripts/printname.sh exists and is a regular file, and:\n\nIf it is (then block), report the outcome with echo (e.g. “The file is found”).\nIf it is not (else block), also report that outcome with echo (e.g. “The file is not found”).\n\nThen:\n\nRun your if statement by pasting the code into the terminal — it should report that the file is found.\nIntroduce a typo in the file name in the if statement, and run it again, to check that the file is not indeed not found.\n\n(Note that your new script isn’t meant to be run per se, but it is much easier to write multi-line statements in a text file than directly in the terminal.)\n\n\nClick for the solution\n\n# Note: you need single quotes when using exclamation marks with echo!\nif [[ -f scripts/printname.sh ]]; then\n    echo 'Phew! The file is found.'\nelse\n    echo 'Oh no! The file is not found!'\nfi\nPhew! The file is found.\nAfter introducing a typo:\nif [[ -f scripts/printnames.sh ]]; then\n    echo 'Phew! The file is found.'\nelse\n    echo 'Oh no! The file is not found!'\nfi\nOh no! The file is not found!"
  },
  {
    "objectID": "week05/w5_1_scripts.html#footnotes",
    "href": "week05/w5_1_scripts.html#footnotes",
    "title": "Shell scripting",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n Don’t worry about the warning that other keybindings exist for this shortcut.↩︎\n Alternatively, find the script in the file explorer in the side bar and click on it there.↩︎\n And more generally, you can’t remove or edit files that are not yours unless you’ve explicitly been given permission for this.↩︎\n Compare this with the situation for file names, which ideally do not contain spaces and special characters either, but in which - and . are recommended.↩︎\n You can also use single square brackets [ ] but the double brackets have more functionality and I would recommend to always use these.↩︎\nA single = also works but == is clearer.↩︎"
  },
  {
    "objectID": "week02/w2_01_shell.html#introduction-to-unix-and-the-unix-shell",
    "href": "week02/w2_01_shell.html#introduction-to-unix-and-the-unix-shell",
    "title": "Unix shell basics",
    "section": "1 Introduction to Unix and the Unix Shell",
    "text": "1 Introduction to Unix and the Unix Shell\n\n1.1 Some terminology and concepts\n\nUnix (vs. Linux)\nWe can conceptualize Unix (or “Unix-like” / “Unix-based”)1 as a family of operating systems, which includes Linux and Mac but not Windows.\nFor scientific computing, Unix-based operating systems are generally preferable. Supercomputers, like the Ohio Supercomputer Center (OSC), use Linux. In this course, though, your laptop/desktop can run on any operating system, precisely because we will connect to OSC and do our work there.\n\n\nUnix shell-related terms\n\nCommand Line and Command Line Interface (CLI) — A user interface where you type commands\nShell — a more specific and technical term for a command line interface to your computer\nUnix Shell — the type of shell on Unix-based (Linux + Mac) computers\nBash — the specific Unix shell language that is most common\nTerminal — the program/app/window that can run a shell\n\nWhile these are clearly not synonyms, in day-to-day computing, they are often used interchangeably.\n\n\n\n\n\n\nCLI vs. GUI\n\n\n\nInstead of a Command-line Interface (CLI), most programs that we use on a day-to-day basis have a Graphical User Interface (GUI), which are operated mostly by pointing and clicking rather than typing.\n\n\n\n\n\n\n1.2 Why use the Unix shell?\nThe Unix shell has been around for a long time and can sometimes seem a bit archaic. But astonishingly, a system largely built decades ago in an era with very different computers and datasets has stood the test of time, and the ability to use it is a crucial skill in applied bioinformatics.\nSpecifically, you may want to or need to use the Unix shell as opposed to programs with Graphical User Interfaces (GUIs) because of:\n\nSoftware needs\nWhen working with omics data, we don’t code our entire analysis from scratch – for example, we don’t build our own genome assembly or read alignment algorithm. Many of the external tools that we use for this are run in the shell with a Command-Line Interface (CLI).\nAutomation and fewer errors\nThe shell allows you to repeat and automate tasks easily and without introducing errors.\nReproducibility\nIt’s more straightforward to keep a detailed record of what you have done.\nWorking with large files\nShell commands are really good at processing large, plain-text files, which are very common in omics data.\nRemote computing – especially supercomputers\nIt is often only possible to work in a shell when doing remote computing.\n\nBut what about versus other coding languages? Why do we need to know the Unix shell on top of R and/or Python?\n\nEfficiency\nFor many simpler tasks, built-in shell tools are faster — in terms of coding time, processing time, and the ease of working with very large files.\nRunning external software\nAs mentioned above, we need to use many external programs with a CLI in omics data analysis. While those can be called from within other languages, the shell has a direct interface to them and is therefore often preferred.\n\n\n\n\n1.3 The Unix directory structure\n\n“Directory” (or “dir” for short) is the term for folder that is commonly used in Unix contexts.\nThe Unix directory structure is hierarchical, with a single starting point: the root, depicted as /.\nA “path” gives the location of a file or directory, in which directories are separated by forward slashes /.\nSo: a leading / in a path is the root dir, and any subsequent / are used to separate dirs. For example, the path to our OSC project’s dir is /fs/ess/PAS2880. This means: the dir PAS2880 is located inside the dir ess, which in turn is inside the dir fs, which in turn is in the computer’s root directory.\nThe OSC dir structure is somewhat different from that of a personal computer. Our Home dir is not /home/&lt;username&gt; like in the book and the schematic on the left, but /users/PASXXXX/&lt;username&gt;.\n\n\n\n\n\n\nGeneric computer dir structure, from O’Neil 2017\n\n\n\n\n\n\nKey dirs in the OSC dir structure\n\n\n\n\n\n\n Exercise: Figure out the path\nIn the above schematic on the left, what is the path to the file todo_list.txt?\n\n\nClick to see the solution\n\nThe path to the file todo_list.txt is: /home/oneils/documents/todo_list.txt"
  },
  {
    "objectID": "week02/w2_01_shell.html#first-steps-in-the-shell",
    "href": "week02/w2_01_shell.html#first-steps-in-the-shell",
    "title": "Unix shell basics",
    "section": "2 First steps in the shell",
    "text": "2 First steps in the shell\n\n2.1 Accessing a shell at OSC\n\nGo to https://ondemand.osc.edu in your browser, and log in.\nClick on the Clusters dropdown menu in the top bar and select Cardinal Shell Access.\n\n\n\n2.2 The shell’s prompt\nInside your terminal, the “prompt” indicates that the shell is ready for a command. What is shown exactly varies across shells and can also be customized, but our prompts at OSC should show the following information:\n&lt;username&gt;@&lt;node-name&gt; &lt;working-dir&gt;]$\nFor example (and note that ~ means your Home dir):\n[jelmer@cardinal-login02 ~]$ \nWe type our commands after the dollar sign $, and then press Enter to execute the command. When the command has finished executing, we’ll get our prompt back and are able to type and execute a new command.\n\n\n\n\n\n\nClear the screen with Ctrl+L\n\n\n\nOSC prints welcome messages and storage quota information when you open a shell. To reduce the amount of text on the screen, I will clear the screen now and regularly throughout. This can be done with the keyboard shortcut Ctrl+L.\nThis is the first of a number of keyboard shortcuts you will learn, as these are very useful when working in the shell.\n\n\n\n\n\n2.3 A few simple commands: date and pwd\nThe Unix shell comes with hundreds of “commands”: small programs that perform specific actions. (If you’re familiar with R or Python, a Unix command is like an R/Python function.)\nLet’s start with a few simple commands:\n\nThe date command prints the current date and time:\ndate\nThu Feb 29 14:58:19 EST 2024\nThe pwd (Print Working Directory) command prints the path to the directory you are currently located in:\npwd\n/users/PAS0471/jelmer\n\nBoth of those commands provided us with some output. That output was printed to screen, which is the default behavior for nearly every Unix command.\n\n\n\n\n\n\n\nCopying and pasting in this shell\n\n\n\nYou can’t right-click in this shell, so to copy-and-paste:\n\nCopy simply by selecting text (you should see a copy () icon appear).\nPaste using Ctrl+V.\n\n Try copying and pasting a random word into your shell. This may just work, you may get a permission pop-up, or it may silently fail — if the latter, click on the clipboard icon in your browser’s address bar (see red circle in screenshot below):"
  },
  {
    "objectID": "week02/w2_01_shell.html#options-arguments-and-help",
    "href": "week02/w2_01_shell.html#options-arguments-and-help",
    "title": "Unix shell basics",
    "section": "3 Options, arguments, and help",
    "text": "3 Options, arguments, and help\n\n3.1 The cal command — and options & arguments\nThe cal command is another example of a command that simply prints some information to the screen, in this case a calendar. We’ll use it to learn about command options and arguments.\nInvoking cal without options or arguments will show a calendar for the current month:\ncal\n    February 2024   \nSu Mo Tu We Th Fr Sa\n             1  2  3\n 4  5  6  7  8  9 10\n11 12 13 14 15 16 17\n18 19 20 21 22 23 24\n25 26 27 28 29\n\nOption examples\nUse the option -j (a dash - and then j) to instead get a Julian calendar, in which day numbering is continuous instead of restarting each month:\n# Make sure to leave a space between `cal` and `-j`!\ncal -j\n       February 2024       \nSun Mon Tue Wed Thu Fri Sat\n                 32  33  34\n 35  36  37  38  39  40  41\n 42  43  44  45  46  47  48\n 49  50  51  52  53  54  55\n 56  57  58  59  60\nUse the -3 option to show 3 months (adding the previous and next month):\ncal -3\n    January 2024          February 2024          March 2024     \nSu Mo Tu We Th Fr Sa  Su Mo Tu We Th Fr Sa  Su Mo Tu We Th Fr Sa\n    1  2  3  4  5  6               1  2  3                  1  2\n 7  8  9 10 11 12 13   4  5  6  7  8  9 10   3  4  5  6  7  8  9\n14 15 16 17 18 19 20  11 12 13 14 15 16 17  10 11 12 13 14 15 16\n21 22 23 24 25 26 27  18 19 20 21 22 23 24  17 18 19 20 21 22 23\n28 29 30 31           25 26 27 28 29        24 25 26 27 28 29 30\n                                            31   \nWe can combine multiple options – for example:\ncal -j -3\n        January 2024                February 2024                  March 2024        \nSun Mon Tue Wed Thu Fri Sat  Sun Mon Tue Wed Thu Fri Sat  Sun Mon Tue Wed Thu Fri Sat\n      1   2   3   4   5   6                   32  33  34                       61  62\n  7   8   9  10  11  12  13   35  36  37  38  39  40  41   63  64  65  66  67  68  69\n 14  15  16  17  18  19  20   42  43  44  45  46  47  48   70  71  72  73  74  75  76\n 21  22  23  24  25  26  27   49  50  51  52  53  54  55   77  78  79  80  81  82  83\n 28  29  30  31               56  57  58  59  60           84  85  86  87  88  89  90\n                                                           91 \nHandily, options can be “pasted together” like so (output not shown - same as above):\ncal -j3\n\n\nOptions summary\nAs we’ve seen, options are specified with a dash - (or to dashes --, as you’ll see later). So far, we’ve only worked with the type of options that are also called “flags”, which change some functionality in an ON/OFF type way:\n\nTurning a Julian calender display ON with -j\nTurning a 3-month display ON with -3.\n\nGenerally speaking, we can say that options change the behavior of a command.\n\n\nArguments\nWhereas options change the behavior of a command, arguments typically tell the command what to operate on. Most commonly, these are file or directory paths.\nAdmittedly, the cal command is not the best illustration of this pattern — when you give it one argument, this should be the year to show a calendar for:\ncal 2020\n                              2020                               \n\n       January               February                 March       \nSu Mo Tu We Th Fr Sa   Su Mo Tu We Th Fr Sa   Su Mo Tu We Th Fr Sa\n          1  2  3  4                      1    1  2  3  4  5  6  7\n 5  6  7  8  9 10 11    2  3  4  5  6  7  8    8  9 10 11 12 13 14\n12 13 14 15 16 17 18    9 10 11 12 13 14 15   15 16 17 18 19 20 21\n19 20 21 22 23 24 25   16 17 18 19 20 21 22   22 23 24 25 26 27 28\n26 27 28 29 30 31      23 24 25 26 27 28 29   29 30 31\n\n# [...output truncated, entire year is shown...]\nWe’ll see some examples of commands operating on files or dirs later. Finally, we can also combine options and arguments:\ncal -j 2020\n                            2020                          \n\n          January                       February         \nSun Mon Tue Wed Thu Fri Sat   Sun Mon Tue Wed Thu Fri Sat\n              1   2   3   4                            32\n  5   6   7   8   9  10  11    33  34  35  36  37  38  39\n 12  13  14  15  16  17  18    40  41  42  43  44  45  46\n 19  20  21  22  23  24  25    47  48  49  50  51  52  53\n 26  27  28  29  30  31        54  55  56  57  58  59  60\n \n# [...output truncated, entire year is shown...]\nTo summarize, different from options, arguments to a command:\n\nAre not preceded by a - (or --)\nIf options and arguments are combined, arguments come after options2.\n\n\n\n\n\n\n\nTBA: Command structure visualization\n\n\n\n\n\n\n\n\n\n\n3.2 Getting help\nMany basic Unix commands – and other command-line programs! – have a -h option for help, which usually gives a concise summary of the command’s “syntax”, i.e. its available options and arguments:\ncal -h\n\nUsage:\n cal [options] [[[day] month] year]\n\nOptions:\n -1, --one        show only current month (default)\n -3, --three      show previous, current and next month\n -s, --sunday     Sunday as first day of week\n -m, --monday     Monday as first day of week\n -j, --julian     output Julian dates\n -y, --year       show whole current year\n -V, --version    display version information and exit\n -h, --help       display this help text and exit\nIn this case, each option can be referred to in two ways:\n\nA short-form notation: a single dash followed by a single character (e.g. -s)\nA long-form notation: two dashes followed by a word (e.g. --sunday)\n\n\n\n\n\n\n\nAnother way to see documentation: the man command (Click to expand)\n\n\n\n\n\nAn alternative way of getting help for Unix commands is with the man command:\nman cal\nThis manual page often includes a lot more details than the --help output, and it is opened inside a “pager” rather than printed to screen: type q to exit the pager that man launches.\n\n\n\n\n Exercise: Interpreting the help output\n\nLook through the options listed when you ran cal -h, and try an option we haven’t used yet. (You can also combine this new option with other options, if you want.)\n\n\n\nClick to see the solution\n\n# Print a calendar with Monday as the first day of th week (instead of the default, Sunday) \ncal -m\n    February 2024   \nMo Tu We Th Fr Sa Su\n          1  2  3  4\n 5  6  7  8  9 10 11\n12 13 14 15 16 17 18\n19 20 21 22 23 24 25\n26 27 28 29\n\n\nTry using one or more options in their “long form” (with --). Why would those be useful?\n\n\n\nClick to see the solution\n\nFor example:\ncal --julian --monday\n       February 2024       \nMon Tue Wed Thu Fri Sat Sun\n             32  33  34  35\n 36  37  38  39  40  41  42\n 43  44  45  46  47  48  49\n 50  51  52  53  54  55  56\n 57  58  59  60\nThe advantage of using long options is that they are less cryptic and more descriptive. Therefore, it is much more likely that any reader of the code (including yourself next week) will immediately understand what these options are doing.\nNote that long options cannot be “pasted together” like short options.\n\n\n\nBonus: Try to figure out / guess what the meaning is of the top of the help text: Usage: cal [options] [[[day] month] year]. Based on that, can you print a calendar for April 2017?\n\n\n\nClick to see the solution\n\nFirst of all, the square brackets around options and all of the possible arguments means that none of these are required — as we’ve seen, just cal (with no options or arguments) is a valid command.\nThe structure of the multiple square brackets around the day-month-year arguments indicate that:\n\nIf you provide only one argument, it should be a year\nIf you provide a second argument, it should be a month\nIf you provide a third argument, it should be a day\n\nTherefore, to print a calendar for April 2017:\ncal 4 2017\n     April 2017     \nSu Mo Tu We Th Fr Sa\n                   1\n 2  3  4  5  6  7  8\n 9 10 11 12 13 14 15\n16 17 18 19 20 21 22\n23 24 25 26 27 28 29\n30"
  },
  {
    "objectID": "week02/w2_01_shell.html#cd-paths-and-shortcuts",
    "href": "week02/w2_01_shell.html#cd-paths-and-shortcuts",
    "title": "Unix shell basics",
    "section": "4 cd, paths, and shortcuts",
    "text": "4 cd, paths, and shortcuts\n\n4.1 cd and command “actions”\nAll the commands so far “merely” provided some information, which was printed to the screen.\nBut many commands perform another kind of action. For example, the command cd will change your working directory. And like many commands that perform a potentially invisible action, cd normally has no output at all.\nFirst, let’s check again where we are — we should be in our Home directory:\n# (Note: you will have a different PAS project listed in your Home dir. This is not important.)\npwd\n/users/PAS0471/jelmer\nNow, let’s use cd to move to another directory by specifying the path to that directory as an argument:\ncd /fs/ess/PAS2880\n# Double-check that we moved:\npwd\n/fs/ess/PAS2880\nIn summary:\n\nLike cal, cd accepts an argument. Unlike cal, this argument takes the form of a path that the command should operate on, which is much more typical.\ncd gives no output when it successfully changed the working directory. This is very common behavior for Unix commands that perform operations: when they succeed, they are silent.\n\nLet’s also see what happens when cd does not succeed — it gives an error:\ncd /fs/ess/PAs2880\n-bash: cd: /fs/ess/PAs2880: No such file or directory\n\n\n\n\n\n\n\nCase-sensitivity and comments\n\n\n\n\nEverything in the shell (and in coding in general) is case-sensitive, including commands and file names — hence the error above.\nAny text that comes after a # is considered a comment instead of code! Comments are not executed but are ignored by the shell. For example:\n# This entire line is a comment - you can run it and nothing will happen\npwd    # 'pwd' will be executed but everything after the '#' is ignored\n/users/PAS0471/jelmer\n\n\n\n\n\n\n\n4.2 Keyboard shortcuts\nUsing keyboard shortcuts help you work much more efficiently in the shell. And some are invaluable:\n\nCommand history — Up / Down arrow keys to cycle through your command history.\nTab completion — The shell will auto-complete partial commands or file paths when you press Tab.\nCancel/stop/abort — If your prompt is “missing”, the shell is still busy executing your command, or you typed an incomplete command. To abort, press Ctrl+C and you will get your prompt back.\n\n\n Practice with Tab completion & command history\n\nType /f and press Tab (will autocomplete to /fs/)\nAdd e (/fs/e) and press Tab (will autocomplete to /fs/ess/).\nAdd PAS (/fs/ess/PAS) and press Tab. Nothing should happen: there are multiple (many!) options.\nPress Tab Tab (i.e., twice in quick succession) and it should say:\nDisplay all 619 possibilities? (y or n)\nType n to answer no: we don’t need to see all the dirs starting with PAS.\nAdd 288 (/fs/ess/PAS27) and press Tab twice in quick succession (a single Tab won’t do anything): you should see at least four dirs that start with PAS288.\nAdd 0 so your line reads /fs/ess/PAS2880. Press Enter. You will get the following output, which is an error. Can you think of any reason why this you may have gotten an error here?\nbash: /fs/ess/PAS2880/: Is a directory\n\n\n\nClick to see the solution\n\nBasically, everything you type in the shell should start with a command. Just typing a path will not make the shell print some info about, or the contents of, this dir or file. Instead, since a path is not a command, you will get an error.\n\n\nPress ⇧ to get the previous “command” back on the prompt.\nPress Ctrl+A to move to the beginning of the line at once.\nAdd cd and a space in front of the dir, and press Enter again.\ncd /fs/ess/PAS2880/\n\n\n\n\n\n\n\nYour cursor can be anywhere on a line (not just at the end) when you press Enter to execute a command!\n\n\n\n\n\n\n\n\n\n Practice with canceling\nTo simulate a long-running command that we may want to abort, we can use the sleep command, which will make the computer wait for a specified amount of time until giving your prompt back. Run the below command and instead of waiting for the full 60 seconds, press Ctrl + C to get your prompt back sooner!\nsleep 60s\nAs another example of a situation where you might have to use Ctrl + C, simply type an opening parenthesis ( and press Enter:\n(\nWhen you do this, nothing is executed and you are not getting your prompt back: you should see a &gt; on the next line. This is the shell wanting you to “complete” you command. Why would that be?\n\n\nClick to see the solution\n\nThis is an incomplete command by definition because any opening parenthesis should have a matching closing parenthesis.\n\nPress Ctrl + C to get your regular prompt back.\n\n\n\n\n\n\n\nTable with useful keyboard shortcuts (Click to expand)\n\n\n\n\n\n(Note that even on Macs, you should use Ctrl instead of switching them out for Cmd as you may be used to doing – though in some cases, like copy/paste, both keys work).\n\n\n\n\n\n\n\nShortcut\nFunction\n\n\n\n\nTab\nTab completion\n\n\n⇧ / ⇩\nCycle through previously issued commands\n\n\nCtrl(+Shift)+C\nCopy selected text\n\n\nCtrl(+Shift)+V\nPaste text from clipboard\n\n\nCtrl+A / Ctrl+E\nGo to beginning/end of line\n\n\nCtrl+U /Ctrl+K\nCut from cursor to beginning / end of line\n\n\nCtrl+W\nCut word before before cursor (Only works on Macs in our shell in the browser!)\n\n\nCtrl+Y\nPaste (“yank”) text that was cut with one of the shortcuts above\n\n\nAlt+. / Esc+.\nRetrieve last argument of previous command (very useful!) (Esc+. for Mac)\n\n\nCtrl+R\nSearch history: press Ctrl+R again to cycle through matches, Enter to put command in prompt.\n\n\nCtrl+C\nCancel (kill/stop/abort) currently active command\n\n\nCtrl+D\nExit (a program or the shell, depending on the context) (same as exit command)\n\n\nCtrl+L\nClear the screen (same as clear command)\n\n\n\n\n\n\n\n\n\n4.3 Environment variables\nYou may be familiar with the concept of variables from previous experience with perhaps R or another language. Variables can hold values and other pieces of data and are essential in programming.\n\nAssigning and printing the value of a variable in R:\n\n# (Don't run this)\nx &lt;- 5\nx\n\n[1] 5\n\n\nAssigning and printing the value of a variable in the Unix shell:\nx=5\necho $x\n5\n\n\n\n\n\n\n\nIn the Unix shell code above, note that:\n\n\n\n\nThere cannot be any spaces around the = in x=5.\nYou need a $ prefix to reference (but not to assign) variables in the shell3.\nYou need the echo command, a general command to print text, to print the value of $x (cf. in R).\n\nBy the way, echo can also print literal text (as shown below) or combinations of literals and variables (next exercise):\necho \"Welcome to PLNTPTH 5004\"\nWelcome to PLNTPTH 5004\n\n\n\nEnvironment variables are pre-existing variables that have been automatically assigned values. Two examples:\n# $HOME contains the path to your Home dir:\necho $HOME\n/users/PAS0471/jelmer\n# $USER contains your user name:\necho $USER\njelmer\n\n Exercise: environment variables\nUsing an environment variable, print “Hello, my name is &lt;your username&gt;” (e.g. “Hello, my name is natalie”).\n\n\nClick for the solution\n\n# (This would also work without the \" \" quotes)\necho \"Hello, my name is $USER\"\nHello, my name is jelmer\n\n\n\n\n\n4.4 Create your own dir & download some data\nOur base OSC directory for the course is the /fs/ess/PAS2880 dir we are currently in. Now, you will create our own directory within here, and download a repository with practice data from the CSB book.\n\nCreate a directory for yourself using the mkdir (make dir) command:\nmkdir users/$USER\nMove there using cd:\n# (Instead of $USER, you can also start typing your username and press Tab)\ncd users/$USER\nGet the files associated with the CSB book by “cloning” (downloading) its GitHub repository:\ngit clone https://github.com/CSB-book/CSB.git\nMove into the sandbox dir for the Unix chapter (remember to use tab completion):\ncd CSB/unix/sandbox\n\n\n\n\n4.5 Paths and path shortcuts\n\nAbsolute (full) paths versus relative paths\nRecall from above that “paths” specify the location of a file or dir. Any file or dir can be referred to in two different ways, using either:\n\nAn absolute (full) path (e.g. /fs/ess/PAS2880)\nA path that begin with a / starts from the computer’s root directory, and is called an “absolute path”.\n(It is equivalent to GPS coordinates for a geographical location, and works regardless of where you are).\nA Relative path (e.g. CSB/unix/sandbox or todo_list.txt)\nA Path that starts from your current working directory is a “relative path”.\n(It works like directions along the lines of “take the second left:” it depends on your current location.)\n\nNext week, we’ll talk more about the distinction between absolute and relative paths, and their respective merits.\n\n\nIn the example above, how can the file name todo_list.txt represent a path? (Click for the solution)\n\nA file name like todo_list.txt, with no directories included, can be seen (and is often used!) as a relative path that implies that the file is in our current working dir.\nAlternatively, and often equivalently, you can be explicit about that by using a ./ preface, e.g.: ./todo_list.txt (see the Path shortcuts section below).\n\n\n\n\nPath shortcuts\n\n~ (a tilde) — represents your Home directory. For example, cd ~ moves you to your Home dir.\n. (a single period) — represents the current working directory (we’ll see soon why that can be useful).\n.. (two periods) — Represents the directory “one level up”, i.e. towards the computer’s root dir.\n\nSome examples of ..:\n\nUse .. to go up one level in the dir hierarchy:\n# Start by reminding yourself of where you are:\npwd\n/fs/ess/PAS2880/users/jelmer/CSB/unix/sandbox\n# Move on level up:\ncd ..\n\n# Check where you are now:\npwd\n/fs/ess/PAS2880/users/jelmer/CSB/unix\nThis pattern can be continued all the way to the root of the computer, so ../.. means two levels up:\ncd ../..\npwd\n/fs/ess/PAS2880/users/jelmer\n\n\n\n\n\n\n\n\nThese path shortcuts work with many commands\n\n\n\nThese are general shell shortcuts that work with any command that accepts a path/file name.\n\n\n\n\n\n Exercise: navigation\n\nA) Use a relative path to move back to the /fs/ess/PAS2880/users/$USER/CSB/unix/sandbox dir. (You should be in /fs/ess/PAS2880/users/$USER.)\n\n\n\nClick to see the solution\n\ncd CSB/unix/sandbox\n\n\nB) Use a relative path (with ..) to move into the /fs/ess/PAS2880/users/$USER/CSB/unix/data dir.\n\n\n\nClick to see the solution\n\ncd ../data\nYou may have done this in two steps, because you may not have realized that you can “add to” a path after .. like we did above. So you may have done this:\ncd ..\ncd data\nThat’s OK, but is obviously more typing.\n\n\nC) The ls command lists files and dirs, and accepts one or more paths as arguments. Use ls to list the files in your Home dir with a shortcut and without moving there.\n\n\n\nClick to see the solution\n\n\nWith the ~ shortcut:\n\nls ~\n# (Output not shown)\n\nWith the $HOME environment variable:\n\nls $HOME\n# (Output not shown)"
  },
  {
    "objectID": "week02/w2_01_shell.html#footnotes",
    "href": "week02/w2_01_shell.html#footnotes",
    "title": "Unix shell basics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n Technically, the latter terms are more correct, as Unix formally does refer to a specific operating system.↩︎\n Though some commands are flexible and accept either order.↩︎\nBecause of this, anytime you see a word/string that starts with a $ in the shell, you can safely assume that it is a variable.↩︎"
  },
  {
    "objectID": "week02/w2_02_shell.html#the-shell-as-a-file-browser",
    "href": "week02/w2_02_shell.html#the-shell-as-a-file-browser",
    "title": "Unix shell: working with files",
    "section": "1 The shell as a file browser",
    "text": "1 The shell as a file browser\n\n1.1 Introduction\nFile browser GUIs, such as “Finder” on Mac, “File Explorer” on Windows, and the one we saw in OSC OnDemand’s File menu, can perform operations like listing, creating, moving, renaming, copying and removing files and folders.\nHere, we will learn to use Unix shell commands that can perform these actions.\n\n\n\n\n\n\nWhy do I need to learn to do these relatively trivial tasks in the shell?\n\n\n\n\nThis will help you to get more comfortable with working in the shell.\nThese are good commands to learn how the shell works more generally, and how commands are structured.\nYou will eventually use the shell for data processing, and it is more efficient to stick with the shell for file browser operations, rather than going back and forth all the time.\nYou may run into situations where a GUI file browser is not at all available.\nWith practice, using the shell is faster and more powerful than a GUI file browser.\n\n\n\nIn the previous session, we already learned some commands that also fall in this category – to recap:\n\ncd will change your working directory, i.e. it can move you around\nmkdir will create a new directory (folder)\n\n\n\n\n1.2 ls to list files\nThe ls command, short for “list”, will list files and directories — by default those in your current working dir:\n# (You should be in /fs/ess/PAS2880/users/$USER/CSB/unix/data)\nls\nBuzzard2015_about.txt  Gesquiere2011_about.txt  Marra2014_about.txt   miRNA                   Pacifici2013_data.csv  Saavedra2013_about.txt\nBuzzard2015_data.csv   Gesquiere2011_data.csv   Marra2014_data.fasta  Pacifici2013_about.txt  Saavedra2013\n\n\n\n\n\n\nls output colors (click to expand)\n\n\n\n\n\nThe ls output above does not show the different colors you should see in your shell — the most common ones are:\n\nEntries in blue are directories (like miRNA and Saavedra2013 above)\nEntries in black are regular files (like all other entries above)\nEntries in red are compressed files (we’ll see examples of this later).\n\n\n\n\nYou can use an argument to change the dir (or file) that ls operates on, and you can use options to change how it shows the output.\nLet’s start with options. For example, we can run ls with the option -l (lowercase L):\nls -l \ntotal 1793\n-rw-rw----+ 1 jelmer PAS0471     562 Feb 24 20:30 Buzzard2015_about.txt\n-rw-rw----+ 1 jelmer PAS0471   39058 Feb 24 20:30 Buzzard2015_data.csv\n-rw-rw----+ 1 jelmer PAS0471     447 Feb 24 20:30 Gesquiere2011_about.txt\n-rw-rw----+ 1 jelmer PAS0471   38025 Feb 24 20:30 Gesquiere2011_data.csv\n-rw-rw----+ 1 jelmer PAS0471     756 Feb 24 20:30 Marra2014_about.txt\n-rw-rw----+ 1 jelmer PAS0471  566026 Feb 24 20:30 Marra2014_data.fasta\ndrwxrwx---+ 2 jelmer PAS0471    4096 Feb 24 20:30 miRNA\n-rw-rw----+ 1 jelmer PAS0471     520 Feb 24 20:30 Pacifici2013_about.txt\n-rw-rw----+ 1 jelmer PAS0471 1076150 Feb 24 20:30 Pacifici2013_data.csv\ndrwxrwx---+ 2 jelmer PAS0471    4096 Feb 24 20:30 Saavedra2013\n-rw-rw----+ 1 jelmer PAS0471     322 Feb 24 20:30 Saavedra2013_about.txt\nIt lists the same items as earlier, but printed in a different format: one item per line, with additional information such as the date and time each file was last modified, and file sizes in bytes (to the left of the date).\nLet’s add another option, -h:\nls -lh\ntotal 1.8M\n-rw-rw----+ 1 jelmer PAS0471  562 Feb 24 20:30 Buzzard2015_about.txt\n-rw-rw----+ 1 jelmer PAS0471  39K Feb 24 20:30 Buzzard2015_data.csv\n-rw-rw----+ 1 jelmer PAS0471  447 Feb 24 20:30 Gesquiere2011_about.txt\n-rw-rw----+ 1 jelmer PAS0471  38K Feb 24 20:30 Gesquiere2011_data.csv\n-rw-rw----+ 1 jelmer PAS0471  756 Feb 24 20:30 Marra2014_about.txt\n-rw-rw----+ 1 jelmer PAS0471 553K Feb 24 20:30 Marra2014_data.fasta\ndrwxrwx---+ 2 jelmer PAS0471 4.0K Feb 24 20:30 miRNA\n-rw-rw----+ 1 jelmer PAS0471  520 Feb 24 20:30 Pacifici2013_about.txt\n-rw-rw----+ 1 jelmer PAS0471 1.1M Feb 24 20:30 Pacifici2013_data.csv\ndrwxrwx---+ 2 jelmer PAS0471 4.0K Feb 24 20:30 Saavedra2013\n-rw-rw----+ 1 jelmer PAS0471  322 Feb 24 20:30 Saavedra2013_about.txt\n\n\nWhat is different about the output, and what do you think that means? (Click to see the answer)\n\nThe only difference is in the format of the column reporting the sizes of the items listed.\nWe now have “Human-readable filesizes” (hence -h), where sizes on the scale of kilobytes will be shown with Ks, of megabytes with Ms, and of gigabytes with Gs. That can be useful especially for large files.\n\n\nMoving on to the argument(s) to ls – we can list files in directories other than the one we are in, by specifying that dir as an argument:\nls miRNA\nggo_miR.fasta  hsa_miR.fasta  miR_about.txt  miRNA_about.txt  ppa_miR.fasta  ppy_miR.fasta  ptr_miR.fasta  ssy_miR.fasta\nAnd like we saw with cal, we can combine options and arguments:\nls -lh miRNA\ntotal 320K\n-rw-rw----+ 1 jelmer PAS0471  18K Feb 24 20:30 ggo_miR.fasta\n-rw-rw----+ 1 jelmer PAS0471 131K Feb 24 20:30 hsa_miR.fasta\n-rw-rw----+ 1 jelmer PAS0471  104 Feb 24 20:30 miR_about.txt\n-rw-rw----+ 1 jelmer PAS0471  104 Feb 24 20:30 miRNA_about.txt\n-rw-rw----+ 1 jelmer PAS0471 4.0K Feb 24 20:30 ppa_miR.fasta\n-rw-rw----+ 1 jelmer PAS0471  33K Feb 24 20:30 ppy_miR.fasta\n-rw-rw----+ 1 jelmer PAS0471  29K Feb 24 20:30 ptr_miR.fasta\n-rw-rw----+ 1 jelmer PAS0471  495 Feb 24 20:30 ssy_miR.fasta\n\n Exercise: Listing a single file\nSay you wanted to check the size of a single file. Then, it is not always convenient to list an entire dir’s contents, as it could contain many files.\nTry to use ls to see the file size only for the miRNA_about.txt file we saw listed above.\n\n\nClick to see the solution\n\nThe argument to ls can be a path of any kind, including to a file rather than to a dir:\nls -lh miRNA/miRNA_about.txt\n-rw-rw----+ 1 jelmer PAS0471  104 Feb 24 20:30 miRNA_about.txt\n\n\n\nIn preparation for the next sections, let’s move into the sandbox dir:\ncd ../sandbox\n\nls\nPapers and reviews\n\n\n\n1.3 touch to create a new file\nTo create new, empty files, we can use the touch command, specifiying the file name as the argument(s):\n# Create a single new file called newfile1.txt\ntouch newfile1.txt\n\n# Create two additional files:\ntouch newfile2.txt newfile3.txt\n\n\n\n\n\n\nOperating on multiple files\n\n\n\nIt is important to realize that almost all Unix commands that operate on files and/or dirs can operate on multiple (many!) files at a time, which is one reason using them can be highly efficient. In week 6, you’ll learn to select many files at once using shortcuts known as wildcards.\n\n\n\n Exercise: file locations\n\nUsing the code above, where were these files created? Can you check their sizes?\nCan you create a create a new file test.txt inside a new dir testdir?\n\n\n\nClick to see the solutions\n\n\nThe initial files were created in your working dir.\n\nls -lh\nTBA\n\nTo make a new dir and create a file in there, you can’t just use the touch command, as it cannot create the directory for you (pay attention to the error, which may seem a bit confusing):\n\ntouch testdir/test.txt\ntouch: testdir/test.txt: No such file or directory\nInstead, we should first create the new dir with mkdir, and then create the new file in there:\nmkdir testdir\ntouch testdir/test.txt\n\n\n\n\n\n1.4 cp to copy files and dirs\nThe cp command copies files and/or dirs from one location to another. Just like when copying files in a GUI file browser, the copy can have a different name than the original file – or the same name, as long as it’s copied to a different dir.\ncp has two required arguments in the following order:\n\nWhat you want to copy (source path)\nWhere you want to copy it to (destination path).\n\nThat is, its basic syntax is cp &lt;source path&gt; &lt;destination path&gt;1.\n\nBasic examples\nCreate a copy of one of the files we created above:\ncp newfile3.txt newfile3_copy.txt\nImportantly, like with any Unix command, you can always refer to files and dirs that aren’t in your current working dir – for example:\ncp ../data/Buzzard2015_about.txt buzz2.txt\nThe above copied the file Buzzard2015_about.txt, which was not in our working dir, to a new file with the name buzz2.txt in our working dir.\n\n\n\n\n\n\nCopying into your working dir without changing the name\n\n\n\nWhen copying something into our working dir, like above, we may not actually want to change the file name. To accomplish this, we could of course just repeat the original file name in the destination path:\ncp ../data/Buzzard2015_about.txt Buzzard2015_about.txt\nBut do we really need to repeat the filename (which is not just more typing, but can be error-prone as well)? No. If the destination path is simply the . shortcut that indicates our current working dir, the file name of the copy will not be changed:\ncp ../data/Buzzard2015_data.csv .\n\n\n\n\n\n1.5 Copying dirs and their contents\nFinally, cp will by default refuse to copy directories and their contents — that is, it is not “recursive” by default2. The -r option is needed for recursive copying:\ncp -r ../data . \nCheck the contents of the sandbox dir again now that we’ve copied several items there:\nls\nbuzz2.txt              data           newfile2.txt        newfile3.txt          test\nBuzzard2015_data.csv   newfile1.txt   newfile3_copy.txt  'Papers and reviews'\n\n\n\n1.6 mv to move and rename files and dirs\nMoving files to a different dir and renaming files is fundamentally the same operation: you are changing the path.\nIt is important to realize that when talking about files, both its file name (e.g. buzz2.txt) and the dir it is inside (/fs/ess/PAS2880/users/jelmer/CSB/sandbox) are part of the path (/fs/ess/PAS2880/users/jelmer/CSB/sandbox/buzz2.txt) — and, as pointed out before, that a mere file name like buzz2.txt also represents a path.\nThe mv command, then, can be used to move files, rename them, or do both at the same time:\n# Same directory, different file name (\"renaming\"):\nmv buzz2.txt buzz_copy.txt\n\n# Different directory, same file name (\"moving\"):\n# (You don't need the trailing slash to data/, but this can be clearer.)\nmv buzz_copy.txt data/\n\n# Different directory, different file name (\"moving + renaming\"):\nmv Buzzard2015_data.csv data/Buzzard.txt\nFinally, unlike cp, the mv command is recursive by default!\n\n\n\n\n\n\nBoth the mv and cp commands will by default:\n\n\n\n\nNot report what they do: no output = success (use the -v option for verbose to make them report what they do).\nOverwrite existing files without reporting this (use the -i option for interactive to make them ask before overwriting).\n\n\n\n\n\n\n1.7 rm to remove files and dirs\nThe rm command removes files and optionally dirs — here, we’ll remove the file copy we made above:\nrm newfile1.txt\nLike with cp, the -r option is needed to make the command work recursively:\nrm testdir\nrm: cannot remove ‘testdir’: Is a directory\nBut it does work (silently!) with the ‘-r’ option:\nrm -r testdir\n\n\n\n\n\n\nThere is no thrash bin when deleting files in the shell, so use rm with caution! (Click to expand)\n\n\n\n\n\nrm -r can be very dangerous — for example, the command rm -r / would attempt to remove the entire contents of the computer, including the operating system.\nA couple ways to take precautions:\n\nYou can add the -i option, which will have you confirm each individual removal (can be tedious)\nWhen you intend to remove an empty dir, you can use the rmdir command which will do just (and only) that — that way, if the dir isn’t empty after all, you’ll get an error.\n\n\n\n\n\n\n Bonus exercise: cp and mv behavior\nFor both cp and mv, when operating on files (and this works equivalently for dirs):\n\nIf the destination path is an existing dir, the file will go into that dir and keep its original name.\nIf the destination path is not an existing dir, the (last bit of the) destination specifies the new file name.\nA trailing slash in the destination path makes explicit that you are referring to a dir and not a file.\n\n\nWith that in mind, try to answer the following questions about this command:\ncp Buzzard2015_about.txt more_data/\n\nWhat do you think the command would do or attempt to do?\nDo you think the command will succeed?\nWhat would the command have done if we had omitted the trailing forward slash?\n\n\n\nClick to see the solution\n\n\n\nBecause we put a trailing forward slash in more_data/, we are making clear that we are referring to a directory. So the file should be copied into a dir more_data, and keep the same file name.\nHowever, the more_data/ dir does not exist, and cp will not create a dir on the fly, so this will fail:\ncp newfile2.txt more_data/\ncp: cannot create regular file ‘more_data/’: Not a directory\nIf we had omitted the trailing forward slash, we would have created a copy of the file with file name more_data (note that no file extension is needed, per se).\nP.S: To make the original intention work, first create the destination dir:\nmkdir more_data\ncp newfile2.txt more_data/\nNote also that once the more_data dir exists, it does not make a difference whether or not you using a trailing slash (!)."
  },
  {
    "objectID": "week02/w2_02_shell.html#viewing-the-contents-of-text-files",
    "href": "week02/w2_02_shell.html#viewing-the-contents-of-text-files",
    "title": "Unix shell: working with files",
    "section": "2 Viewing the contents of text files",
    "text": "2 Viewing the contents of text files\n\n2.1 Plain-text files\nIn this course, we’ll be working almost exclusively with so-called “plain-text” files (files with .txt and related extensions). These are simple files that can be opened by any text editor on any computer, and by Unix commands.\nIn contrast, so-called “binary” formats like Excel or Word files can only be opened in their respective apps and cannot be operated on with Unix commands. That can pose many problems: for example, these apps are not always available, and version mismatches may make files impossible to open.\nWhile their simplicity may seem limiting, plain-text files are generally preferable for reproducible science. Additionally, many common omics file formats, like FASTA, FASTQ, and GFF, are in plain-text.\nTherefore, we will be learning a number of Unix commands to view (this section) and process/summarize (next section) plain-text files in various formats.\ncd ../data   # Move to the data dir for the next commands\n\n\n2.2 cat\nThe cat command will print the entire contents of one or more files to screen:\ncat Marra2014_about.txt\nData published by:\nMarra NJ, DeWoody JA (2014) Transcriptomic characterization of the immunogenetic repertoires of heteromyid rodents. BMC Genomics 15: 929. http://dx.doi.org/10.1186/1471-2164-15-929\n\nData description:\nFile D_spec_spleen_filtered.fasta (57.01Mb) contains Dipodomys spectabilis spleen transcriptome data. Combined assembly of 454 reads and fragmented Illumina assembly (see methods of associated paper) in gsAssembler version 2.6.\nNote that we truncated the original file to 1% of its original size and named it Marra2014_data.txt\n\nData taken from:\nMarra NJ, DeWoody JA (2014) Data from: Transcriptomic characterization of the immunogenetic repertoires of heteromyid rodents. Dryad Digital Repository. http://dx.doi.org/10.5061/dryad.qn474\n\n\n\n2.3 head and tail\nSome files, especially when working with omics data, can be huge, so printing the whole file with cat is not always ideal. The twin commands head and tail can be useful, as they will print only the first (head) or last (tail) lines of a file.\nhead & tail’s defaults are to print 10 lines:\nhead Gesquiere2011_data.csv\nmaleID  GC      T\n1       66.9    64.57\n1       51.09   35.57\n1       65.89   114.28\n1       80.88   137.81\n1       32.65   59.94\n1       60.52   101.83\n1       65.89   65.84\n1       52.72   43.98\n1       84.85   102.31\nUse the -n option to specify the number of lines to print:\nhead -n 3 Gesquiere2011_data.csv\nmaleID  GC      T\n1       66.9    64.57\n1       51.09   35.57\nA neat trick with tail is to start at a specific line, often used to skip the header line, like in this example3:\ntail -n +2 Gesquiere2011_data.csv\n1       66.9    64.57\n1       51.09   35.57\n1       65.89   114.28\n1       80.88   137.81\n1       32.65   59.94\n1       60.52   101.83\n1       65.89   65.84\n1       52.72   43.98\n1       84.85   102.31\n1       98.25   149.61\n[...output truncated...]\n\n\n\n2.4 less: A file pager\nThe less command is rather different from the previous commands, which simply printed file contents to the screen and gave us our shell prompt back. Instead, less will open a file for you to browse through, and you need to explicitly quit the program to get your prompt back.\nLet’s try it:\nless Gesquiere2011_data.csv\nYou can move around in the file in several ways:\n\nBy scrolling with your mouse\nWith up ↑ and down ↓ arrows to move line-by-line\nIf you have them, with PgUp and PgDn keys to move page-by-page\nWith u (up) and d (down) to half a page at a time\n\nTo exit/quit less and get your shell prompt back, simply type q:\nq"
  },
  {
    "objectID": "week02/w2_02_shell.html#redirection-and-pipes",
    "href": "week02/w2_02_shell.html#redirection-and-pipes",
    "title": "Unix shell: working with files",
    "section": "3 Redirection and pipes",
    "text": "3 Redirection and pipes\n\n3.1 wc -l to count lines\nBefore we can move on to redirection and pipes, we’ll learn about a useful little command that the examples below will use. The wc command by default counts the number of lines, words, and characters in its input — but it is most commonly used to only count lines, with the -l option:\nwc -l Marra2014_about.txt\n9 Marra2014_about.txt\nThis can be surprisingly useful, as the number of lines in many types of files represent a count of the number of entries.\nNow, let’s move back into the sandbox dir:\ncd ../sandbox\n\n\n\n3.2 Standard output and redirection\nThe regular output of a command that is printed to the screen (like a list of files by ls, or a number of lines by wc -l) is technically called “standard out” or in short “stdout”.\nSometimes, we may want to do something else with this output, like storing it in a file. Luckily, this is easy to do.\nWith “&gt;”, we redirect output to a file:\n\nIf the file doesn’t exist, it will be created.\nIf the file does exist, any contents will be overwritten.\n\nFirst, let’s remind ourselves what echo does without redirection — it will simply print the characters we provide it with to the screen:\necho \"My first line\"\nMy first line\nNow, let’s redirect echo’s standard out to a new file test.txt:\necho \"My first line\" &gt; test.txt\nNo output was printed to the screen, as it instead went into the file:\ncat test.txt\nMy first line\nLet’s redirect another line into that same file:\necho \"My second line\" &gt; test.txt\ncat test.txt\nMy second line\nThat may not have been what we intended! As explained above, the earlier file contents was overwritten.\nWith “&gt;&gt;”, however, we append the output to a file:\necho \"My third line\" &gt;&gt; test.txt\ncat test.txt\nMy second line\nMy third line\n\n\n\n3.3 Standard input and pipes\nLet’s say that we want to use Unix commands to count the number of entries (files and “sub”dirs) in a directory. We could do that as follows:\n# First we redirect the ls output to a file\nls ../data/Saavedra2013 &gt; filelist.txt\n\n# Let's check what that looks like -- note that each file is on its own line:\nhead -n 5 filelist.txt\nn10.txt\nn11.txt\nn12.txt\nn13.txt\nn14.txt\n# Then we count the nr. of lines, which is the number of files+dirs in Saavedra2013:\nwc -l filelist.txt\n59 filelist.txt\nThat worked, but we needed two separate lines of code, and we are left with a file filelist.txt that we would probably want to remove since it has served its sole purpose.\nA more conventient way to do this is with a “pipe”, as follows:\nls ../data/Saavedra2013 | wc -l\n59\nWhen we use the pipe, the output of the command on the left-hand side (a file listing, in this case) is redirected into the wc command. This command (and many others) will gladly accept input that way instead of via an file name argument, like in the earlier example.\nPipes are useful because they avoid having to write/read intermediate files — this saves typing, makes the operation quicker, and reduces file clobber. In the example above, we don’t need to make a filelist.txt file to count the number of files."
  },
  {
    "objectID": "week02/w2_02_shell.html#unix-data-tools",
    "href": "week02/w2_02_shell.html#unix-data-tools",
    "title": "Unix shell: working with files",
    "section": "4 Unix data tools",
    "text": "4 Unix data tools\nWe’ll now turn to some commands that may be described as Unix “data tools”. These commands are very useful when working in the shell generally, and when working with omics data specifically.\nTheir strength is especially in relatively simple data processing and summarizing steps, and they are excellent in dealing with very large files.\nHere, we will cover:\n\ngrep to search for text in files\ncut to select one or more columns from tabular data\nsort to sort lines, or tabular data by column\nuniq to remove duplicates\ntr to replace characters\n\nWe’ll start with taking a look at one of the example data files, and discussing tabular plain-text files.\n\n4.1 Tabular plain-text files and file extensions\nThe examples below will use the file Pacifici2013_data.csv, so let’s have a look at the contents of that file first:\ncd ../data\nhead -n 3 Pacifici2013_data.csv\nTaxID;Order;Family;Genus;Scientific_name;AdultBodyMass_g;Sources_AdultBodyMass;Max_longevity_d;Sources_Max_longevity;Rspan_d;AFR_d;Data_AFR;Calculated_GL_d;GenerationLength_d;Sources_GL\n7580;Rodentia;Cricetidae;Eligmodontia;Eligmodontia typus;17.37;PanTHERIA;292;PanTHERIA;254.64;73.74;calculated;147.5856;147.5856;Rspan-AFR(SM+Gest)\n42632;Rodentia;Cricetidae;Microtus;Microtus oregoni;20.35;PanTHERIA;456.25;PanTHERIA;445.85;58.06;calculated;187.3565;187.3565;Rspan-AFR(SM+Gest)\nThis file contains some natural history data on mammals from the paper Generation length for mammals by Pacifici et al. 2013. (In the exercises and assignment, you will also work with sequence data files.)\n“Tabular” files contain data that is arranged in a rows-and-columns format, like a table or an Excel worksheet.\nBecause plain-text files do not have an intrinsic way to define columns or cells, certain characters are used as “delimiters” in plain-text tabular files, like Pacifici2013_data.csv. Most commonly, these are:\n\nA Tab, and such files are often stored with a .tsv extension for Tab-Separated Values (“TSV file”).\nA comma, and such files are often stored with a .csv extension for Comma-Separated Values (“CSV file”).\n\n\n\n\nWhat delimiter does the Pacifici2013_data.csv file appear to contain? (Click for the answer)\n\nFrom looking at the first couple of lines that we printed above, the delimiter is a semicolon ;.\n\n\n\n\n\n\n\n\nPlain-text file extensions are flexible and for human-readibility\n\n\n\nIn the above example, a file with a semicolon as the delimiter was stored with a .csv extension — this may be a bit surprising but is not incorrect. More unusual, and technically incorrect, is the Gesquiere2011_data.csv file we saw earlier, which is Tab-delimited yes has a .csv extension.\nThis brings us to an important side note on plain-text file extensions like .txt, .csv, .tsv, etc.: all plain text files, including the tabular files discussed above, most sequence data files, and scripts are fundamentally “just plain-text files”. Changing the extension does not change the file. Instead, different file extensions are used primarily to make it clear to humans (as opposed to the computer) what the file contains.\n\n\n\n\n\n4.2 grep to print lines that match a pattern\nThe grep command is extremely useful and will find specific text or patterns in a file. By default, it will print each line that contains a “match” in full. It’s basic syntax is grep \"&lt;pattern&gt;\" &lt;file-path&gt;.\nFor example, this will print all lines from Pacifici2013_data.csv that contain “Vombatidae”:\ngrep \"Vombatidae\" Pacifici2013_data.csv\n40555;Diprotodontia;Vombatidae;Lasiorhinus;Lasiorhinus latifrons;26163.8;PanTHERIA;9928;\"PanTHERIA;AnAge\";9317.5;652.24;calculated;3354.315;3354.315;Rspan-AFR(SM+Gest)\n40556;Diprotodontia;Vombatidae;Vombatus;Vombatus ursinus;26000;PanTHERIA;10238.25;\"PanTHERIA;AnAge\";9511.6;783.65;calculated;3542.014;3542.014;Rspan-AFR(SM+Gest)\n11343;Diprotodontia;Vombatidae;Lasiorhinus;Lasiorhinus krefftii;31849.99;PanTHERIA;10950;\"PanTHERIA;AnAge\";no information;no information;no information;no information;3354.315;Mean_congenerics_same_body_mass\nInstead of printing matching lines, we can also count them with the -c option:\n# How many entries for bats (Chiroptera) does the file have?\ngrep -c \"Chiroptera\" Pacifici2013_data.csv\n1144\nThe option -v inverts grep’s behavior and will print all lines not matching the pattern — here, we’ll combine -v and -c to count the number of lines that do not contain “Vombatidae”:\ngrep -vc \"Vombatidae\" Pacifici2013_data.csv\n5424\n\n\n\n\n\n\nAdditional grep tips\n\n\n\n\n\n\nWhile not always necessary, it is a good habit to consistently use quotes (\"...\") around the search pattern.\nIncomplete matches, including in individual words, work: “Vombat” matches Vombatidae.\ngrep has many other useful options, such as:\n\n-i to ignore case\n-r to search files recursively\n-w to match “words”\n\n\n\n\n\n\n\n\n4.3 Selecting columns using cut\nThe cut command will select or we could say “cut out” one or more columns from a tabular file:\n\nWe’ll always have to use the -f option to specify the desired column(s) / “field”(s).\nBecause its default column delimiter is a Tab, for this file, we’ll have to specify the delimiter with -d.\n\n# Select the first column of the file:\ncut -d \";\" -f 1 Pacifici2013_data.csv\nTaxID\n7580\n42632\n42653\n42662\n16652\n[...output truncated...]\nThat worked, but a ton of output was printed, and we may find ourselves scrolling to the top to see the first few lines – in many cases, it can be useful to pipe the output to head to see if our command works:\ncut -d \";\" -f 1 Pacifici2013_data.csv | head -n 3\nTaxID\n7580\n42632\n\n\n\n\n\n\nSelecting multiple columns with cut (Click to expand)\n\n\n\n\n\nTo select multiple columns, use a range or comma-delimited list:\ncut -d \";\" -f 1-4 Pacifici2013_data.csv | head -n 3\nTaxID;Order;Family;Genus\n7580;Rodentia;Cricetidae;Eligmodontia\n42632;Rodentia;Cricetidae;Microtus\ncut -d \";\" -f 2,8 Pacifici2013_data.csv | head -n 3\nOrder;Max_longevity_d\nRodentia;292\nRodentia;456.25\nHowever, it is not possible to change the order of columns with cut! (The more advanced awk command can do this.)\n\n\n\n\n\n\n4.4 Combining cut, sort, and uniq to create a list\nLet’s say we want an alphabetically sorted list of animal orders from the Pacifici2013_data.csv file. To do this, we’ll need to learn about two new commands:\n\nsort to sort/order/arrange rows, by default in alphanumeric order.\nuniq to remove duplicates (i.e., keep all distinct/unique) entries from a sorted file/list.\n\nWe’ll build up a small “pipeline” to do this, step-by-step, and piping the output into head at every step. First, we get rid of the header line with our tail trick:\ntail -n +2 Pacifici2013_data.csv | head -n 5\n7580;Rodentia;Cricetidae;Eligmodontia;Eligmodontia typus;17.37;PanTHERIA;292;PanTHERIA;254.64;73.74;calculated;147.5856;147.5856;Rspan-AFR(SM+Gest)\n42632;Rodentia;Cricetidae;Microtus;Microtus oregoni;20.35;PanTHERIA;456.25;PanTHERIA;445.85;58.06;calculated;187.3565;187.3565;Rspan-AFR(SM+Gest)\n42653;Rodentia;Cricetidae;Peromyscus;Peromyscus gossypinus;27.68;PanTHERIA;471.45833335;PanTHERIA;444.87833335;72.58;calculated;201.59471667;201.5947166715;Rspan-AFR(SM+Gest)\n42662;Macroscelidea;Macroscelididae;Elephantulus;Elephantulus myurus;59.51;PanTHERIA;401.5;PanTHERIA;412.34;90.48;calculated;210.0586;210.0586;Rspan-AFR(SM+Gest)\n16652;Rodentia;Cricetidae;Peromyscus;Peromyscus boylii;23.9;PanTHERIA;547.5;PanTHERIA;514.13;79.97;calculated;229.0677;229.0677;Rspan-AFR(SM+Gest)\nSecond, we select our column of interest with cut:\ntail -n +2 Pacifici2013_data.csv | cut -d \";\" -f 2 | head -n 5\nRodentia\nRodentia\nRodentia\nMacroscelidea\nRodentia\nThird, we pipe to sort to alphabetically sort the result:\ntail -n +2 Pacifici2013_data.csv | cut -d \";\" -f 2 | sort | head -n 5\nAfrosoricida\nAfrosoricida\nAfrosoricida\nAfrosoricida\nAfrosoricida\nFinally, we use uniq to only keep unique rows (values):\ntail -n +2 Pacifici2013_data.csv | cut -d \";\" -f 2 | sort | uniq\nAfrosoricida\nCarnivora\nCetartiodactyla\nChiroptera\nCingulata\nDasyuromorphia\nDermoptera\nDidelphimorphia\n[...output truncated...]\nAmazingly, with a very small modification to our pipeline, we can generate a “count table” instead of a simple list – we just have to add uniq’s -c option (for count):\ntail -n +2 Pacifici2013_data.csv | cut -d \";\" -f 2 | sort | uniq -c\n     54 Afrosoricida\n    280 Carnivora\n    325 Cetartiodactyla\n   1144 Chiroptera\n     21 Cingulata\n[...output truncated...]\n\n\n\n4.5 Substituting characters using tr\ntr for translate will substitute all instances of characters – here, any a for a b:\necho \"aaaaccc\" | tr a b\nbbbbccc\nOddly enough, tr does not take a file name as an argument, so how can we provide it with input from a file? The easiest way is by piping the output of cat into tr as follows:\ncat Pacifici2013_data.csv | tr \";\" \"\\t\" | head -n 3\nTaxID   Order   Family  Genus   Scientific_name AdultBodyMass_g Sources_AdultBodyMass   Max_longevity_d Sources_Max_longevity   Rspan_d AFR_d   Data_AFR        Calculated_GL_d   GenerationLength_d      Sources_GL\n7580    Rodentia        Cricetidae      Eligmodontia    Eligmodontia typus      17.37   PanTHERIA       292     PanTHERIA       254.64  73.74   calculated      147.5856  147.5856        Rspan-AFR(SM+Gest)\n42632   Rodentia        Cricetidae      Microtus        Microtus oregoni        20.35   PanTHERIA       456.25  PanTHERIA       445.85  58.06   calculated      187.3565  187.3565        Rspan-AFR(SM+Gest)\nThe example above converted the ; delimited to a Tab (i.e., a CSV file to a TSV file), where \\t is a regular expression meaning Tab4. (Though note that we didn’t modify the original file nor saved the output in a new file.)\n\n\n\n\n\n\nDeletion and “squeezing” with tr (Click to expand)\n\n\n\n\n\nYou can delete characters with the -d option – for example, to delete all a’s:\necho \"aabbccddee\" | tr -d a\nbbccddee\nYou can “squeeze” i.e. remove consecutive duplicates with the -s option – for example, to turn variable numbers of spaces into a single space:\necho \"a     b   c   d\" | tr -s \" \"\na b c d\n\n\n\n\n\n Exercise: Redirecting tr output\nModify our command in which we changed the delimiter to a Tab to redirect the output to a new file, Pacifici2013_data.tsv (note the extension). This file should be located in the sandbox dir, which is not your current working dir.\n\n\nClick to see the solution\n\ncat Pacifici2013_data.csv | tr \";\" \"\\t\" &gt; ../sandbox/Pacifici2013_data.tsv\n\n\n\n\n\n\n\nDon’t redirect back to the input file!\n\n\n\n\n\nYou should never redirect the output of Unix commands “back” to the input file. This will corrupt the input file because of the way that Unix commands work, which is in “streaming” line-by-line fashion5.\nTherefore, if you really want to edit/overwrite the original file instead of creating a separate edited copy, you will need multiple steps: first produce a separate copy and then rename that."
  },
  {
    "objectID": "week02/w2_02_shell.html#wrap-up-the-unix-philosophy",
    "href": "week02/w2_02_shell.html#wrap-up-the-unix-philosophy",
    "title": "Unix shell: working with files",
    "section": "5 Wrap-up & the Unix philosophy",
    "text": "5 Wrap-up & the Unix philosophy\n\n5.1 The Unix philosophy\n\nThis is the Unix philosophy: Write programs that do one thing and do it well. Write programs to work together. Write programs to handle text streams, because that is a universal interface. — Doug McIlory\n\nAdvantages of a modular approach:\n\nEasier to spot errors\nEasy to swap out components, including in other languages\nEasier to learn\n\nText “streams”?\nRather than loading entire files into memory, process them one line at a time. Very useful with large files!\n# This command would combine all files in the working dir ending in `.fa`\n# (i.e. FASTA files) into a single file -- even if that's multiple GBs,\n# this will not be a heavy lift at all!\ncat *.fa &gt; combined.fa\n\n\n\n5.2 The Unix shell in the weeks ahead\n\nA couple of sessions that will be focused on further developing shell skills:\n\nWeek 3, Thu session: Advanced file management in the shell\nWeek 5: Shell scripting and using external CLI tools\n\nIn many other course weeks, we’ll work in the Unix shell, even when our focus is on a specific tool, such as Git in week 4."
  },
  {
    "objectID": "week02/w2_02_shell.html#footnotes",
    "href": "week02/w2_02_shell.html#footnotes",
    "title": "Unix shell: working with files",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n Note: when I use &lt; &gt; around words in a line of code, those are descriptive rather than literal. In this example, &lt;source&gt; should be replaced in an actual line of code by whatever “source” you want to use, and likewise for the &lt;destination&gt;.↩︎\n For better or worse, non-recursiveness in Unix commands is meant as a sort of safety mechanism, as you may otherwise more easily operate on very large amounts of data accidentally.↩︎\nWe’ll see this in action in a bit↩︎\nWe’ll learn more about regular expressions in Week 4.↩︎\n This is also why they can so easily work with huge files: the entire file contents are not loaded into the computer’s memory.↩︎"
  },
  {
    "objectID": "week02/w2_exercises.html",
    "href": "week02/w2_exercises.html",
    "title": "Week 2 Exercises",
    "section": "",
    "text": "The following are exercises from Chapter 1 of the CSB book, some slightly modified.\nThe solutions are at the bottom of the page."
  },
  {
    "objectID": "week02/w2_exercises.html#getting-set-up",
    "href": "week02/w2_exercises.html#getting-set-up",
    "title": "Week 2 Exercises",
    "section": "Getting set up",
    "text": "Getting set up\nYou should already have the book’s GitHub repository with exercise files in your personal dir within /fs/ess/PAS2880 from Thursday’s class. (If not, cd to /fs/ess/PAS2880/users/$USER, and run git clone https://github.com/CSB-book/CSB.git — this downloads the CSB directory referred to in the first step of the first exercise.)"
  },
  {
    "objectID": "week02/w2_exercises.html#exercise-1-navigation",
    "href": "week02/w2_exercises.html#exercise-1-navigation",
    "title": "Week 2 Exercises",
    "section": "Exercise 1: Navigation",
    "text": "Exercise 1: Navigation\n(a) Go to /fs/ess/PAS2880/users/$USER\n(b) Navigate to the sandbox directory within the CSB/unix directory.\n(c) Use a relative path to go to the data directory within the python directory.\n(d) Use an absolute path to go to the sandbox directory within CSB/python.\n(e) Return to the data directory within the python directory.\n\n\nShow hints\n\nWe didn’t see this in class, but the CSB book (page 21) mentions a shortcut you can use with cd to go back to the dir you were in previously (a little like a Browser’s “back” button)."
  },
  {
    "objectID": "week02/w2_exercises.html#exercise-2-working-with-files",
    "href": "week02/w2_exercises.html#exercise-2-working-with-files",
    "title": "Week 2 Exercises",
    "section": "Exercise 2: Working with files",
    "text": "Exercise 2: Working with files\n(a) Go to the data directory within CSB/unix.\n(b) How many lines are in the file Marra2014_data.fasta?\n(c) Create the empty file toremove.txt in the CSB/unix/sandbox directory without leaving the current directory.\n(d) List the contents of the directory unix/sandbox.\n(e) Remove the file toremove.txt."
  },
  {
    "objectID": "week02/w2_exercises.html#exercise-3-summarizing-tabular-files",
    "href": "week02/w2_exercises.html#exercise-3-summarizing-tabular-files",
    "title": "Week 2 Exercises",
    "section": "Exercise 3: Summarizing tabular files",
    "text": "Exercise 3: Summarizing tabular files\n(a) If we order all species names (fifth column) of Pacifici2013_data.csv (in CSB/unix/data/) in alphabetical order, which is the first species? And which is the last?\n\n\nShow hints\n\nYou will want to use cut followed by sort, and you will need to specify the delimiter with cut.\nTo view just the first or the last line, so you don’t have to scroll to get your answer, pipe to head or tail.\n\n(b) How many families are represented in the database?\n\n\nShow hints\n\n\nCheck the first line of the file to see which column contains the family, and then select the relevant column with cut.\nUse the “tail trick” we saw in class to exclude the first line.\nRemember to sort before using uniq, or uniq won’t work properly."
  },
  {
    "objectID": "week02/w2_exercises.html#exercise-4-next-generation-sequencing-data",
    "href": "week02/w2_exercises.html#exercise-4-next-generation-sequencing-data",
    "title": "Week 2 Exercises",
    "section": "Exercise 4: Next-Generation Sequencing Data",
    "text": "Exercise 4: Next-Generation Sequencing Data\nIn this exercise, we work with next generation sequencing (NGS) data. Unix is excellent at manipulating the huge FASTA files that are generated in NGS experiments.\nFASTA files contain sequence data in text format. Each sequence segment is preceded by a single-line description. The first character of the description line is a “greater than” sign (&gt;).\nThe NGS data set we will be working with was published by Marra and DeWoody (2014), who investigated the immunogenetic repertoire of rodents. You will find the sequence file Marra2014_data.fasta in the directory CSB/unix/data. The file contains sequence segments (contigs) of variable size. The description of each contig provides its length, the number of reads that contributed to the contig, its isogroup (basically, a gene-level grouping of the transcripts: multiple transcripts can be produced by a genes), and some kind of status.\n1. Change directory to CSB/unix/sandbox.\n2. What is the size of the file Marra2014_data.fasta?\n\n\nShow hints\n\nRecall that you can see file sizes by using specific options to the ls command.\n\n3. Create a copy of Marra2014_data.fasta in the sandbox, and name it my_file.fasta.\n4. How many “contigs” (FASTA entries, in this case) are classified as isogroup00036?\n\n\nShow hints\n\nIs there a grep option that counts the number of occurrences? Alternatively, you can pipe (|) the output of grep to wc -l.\n\n5. Modify my_file.fasta to replace the original “two-spaces” delimiter with a comma (i.e. don’t just print the output to screen, but end up with a modified file). You’ll probably want to take a look at the “output file hints” below to see how you can end up with modified file contents.\n\n\nShow output file hints\n\n Due to the “streaming” nature of Unix commands, we can’t write output to a file that also serves as input (see here). So the following is not possible:\ncat myfile.txt | tr \"a\" \"b\" &gt; myfile.txt # Don't do this! \nIn this case, you’ll have to save the output in a different file. Then, if you do want to end up with a modified original file, you can overwrite the original file using mv.\n\n\n\nShow other hints\n\n\nIn the file, the information on each contig is separated by two spaces:\n&gt;contig00001  length=527  numreads=2  ...\nWe would like to obtain:\n&gt;contig00001,length=527,numreads=2,...\nUse cat to print the file, and substitute the spaces using the command tr. Note that you’ll first have to reduce the two spaces two one – can you remember an option to do that?\n\n\n6. How many unique isogroups are in the file?\n\n\nShow hints\n\nYou can use grep to match any line containing the word isogroup. Then, use cut to isolate the part detailing the isogroup. Finally, you want to remove the duplicates, and count.\n\n7. Which contig has the highest number of reads (numreads)? How many reads does it have?\n\n\nShow hints\n\nUse a combination of grep and cut to extract the contig names and read counts. The command sort allows you to choose the delimiter and to order numerically — we didn’t see those sort options in class, so check the book for details."
  },
  {
    "objectID": "week02/w2_exercises.html#exercise-5-hormone-levels-in-baboons",
    "href": "week02/w2_exercises.html#exercise-5-hormone-levels-in-baboons",
    "title": "Week 2 Exercises",
    "section": "Exercise 5: Hormone Levels in Baboons",
    "text": "Exercise 5: Hormone Levels in Baboons\nGesquiere et al. (2011) studied hormone levels in the blood of baboons. The data file is in CSB/unix/data/Gesquiere2011_data.csv.\nEvery individual was sampled several times. How many times were the levels of individuals 3 and 27 recorded?\n\n\nShow hints\n\n\nYou can first use cut to extract just the maleID column from the file.\nTo match an individual (3 or 27), you can use grep with the -w option to match whole “words” only: this will prevent and individual ID like “13” to match when you search for “3”."
  },
  {
    "objectID": "week02/w2_exercises.html#solutions",
    "href": "week02/w2_exercises.html#solutions",
    "title": "Week 2 Exercises",
    "section": "Solutions",
    "text": "Solutions\n\nExercise 1\n\n\nSolution\n\n(a) Go to your home directory. Go to /fs/ess/PAS2700/users/$USER.\ncd /fs/ess/PAS2700/users/$USER # To home would have been: \"cd ∼\"\n(b) Navigate to the sandbox directory within the CSB/unix directory.\ncd CSB/unix/sandbox\n(c) Use a relative path to go to the data directory within the python directory.\ncd ../../python/data\n(d) Use an absolute path to go to the sandbox directory within python.\ncd /fs/ess/PAS2700/users/$USER/CSB/python/sandbox\n(e) Return to the data directory within the python directory.\n\nWith cd -:\n\n# The '-' shortcut for cd will move you back to the previously visited dir\n# (Note: you can't keep going back with this: using it a second time will toggle you \"forward\" again.)\ncd -\n\nUsing a relative path:\n\ncd ../data\n\n\n\n\nExercise 2\n\n\nSolution\n\n(a) Go to the data directory within CSB/unix.\ncd /fs/ess/PAS2700/users/$USER/CSB/unix/data\n(b) How many lines are in file Marra2014_data.fasta?\nwc -l Marra2014_data.fasta\n9515 Marra2014_data.fasta\n(c) Create the empty file toremove.txt in the CSB/unix/sandbox directory without leaving the current directory.\ntouch ../sandbox/toremove.txt\n(d) List the contents of the directory unix/sandbox.\nls ../sandbox\nPapers and reviews  toremove.txt\n(e) Remove the file toremove.txt.\nrm ../sandbox/toremove.txt\n\n\n\n\nExercise 3\n\n\nSolution\n\n(a) If we order all species names (fifth column) of Pacifici2013_data.csv in alphabetical order, which is the first species? And which is the last?\n# First species:\ncut -d \";\" -f 5 Pacifici2013_data.csv | sort | head -n 1\nAbditomys latidens\n# Last species:\ncut -d \";\" -f 5 Pacifici2013_data.csv | sort | tail -n 1\nZyzomys woodwardi\n(b) How many families are represented in the database?\ncut -d \";\" -f 3 Pacifici2013_data.csv | tail -n +2 | sort | uniq | wc -l\n152\n\n\n\n\nExercise 4\n\n\n1. Change directory to CSB/unix/sandbox.\n\ncd /fs/ess/PAS2700/users/$USER/CSB/unix/sandbox\n\n\n\n2. What is the size of the file Marra2014_data.fasta?\n\nls -lh ../data/Marra2014_data.fasta\n-rw-rw----+ 1 jelmer PAS0471 553K Feb 24 20:30 ../data/Marra2014_data.fasta\nAlternatively, the command du (disk usage) can be used for more compact output:\ndu -h ../data/Marra2014_data.fasta \n560K    ../data/Marra2014_data.fasta\n\n\n\n3. Create a copy of Marra2014_data.fasta in the sandbox, and name it my_file.fasta.\n\ncp ../data/Marra2014_data.fasta my_file.fasta\n\n\n\n4. How many contigs are classified as isogroup00036?\n\nTo count the occurrences of a given string, use grep with the option -c:\ngrep -c isogroup00036 my_file.fasta \n16\nSlightly less efficient is to use a “regular” grep and then pipe to wc -l:\ngrep isogroup00036 my_file.fasta | wc -l\n16\n\n\n\n5. Replace the original “two-spaces” delimiter with a comma.\n\n\nWe use the tr option -s (squeeze) to change two spaces two one, and then replace the space with a ,. Importantly, we also write the output to a new file (see the Hints for details):\ncat my_file.fasta | tr -s ' ' ',' &gt; my_file.tmp\nIf we want to change the original file, we can now overwrite it as follows:\nmv my_file.tmp my_file.fasta\nLet’s take a look to check whether out delimiter replacement worked:\ngrep \"&gt;\" my_file.fasta | head\n&gt;contig00001,length=527,numreads=2,gene=isogroup00001,status=it_thresh\n&gt;contig00002,length=551,numreads=8,gene=isogroup00001,status=it_thresh\n&gt;contig00003,length=541,numreads=2,gene=isogroup00001,status=it_thresh\n&gt;contig00004,length=291,numreads=3,gene=isogroup00001,status=it_thresh\n&gt;contig00005,length=580,numreads=12,gene=isogroup00001,status=it_thresh\n&gt;contig00006,length=3288,numreads=35,gene=isogroup00001,status=it_thresh\n&gt;contig00008,length=1119,numreads=10,gene=isogroup00001,status=it_thresh\n&gt;contig00010,length=202,numreads=4,gene=isogroup00001,status=it_thresh\n&gt;contig00011,length=5563,numreads=61,gene=isogroup00001,status=it_thresh\n&gt;contig00012,length=824,numreads=10,gene=isogroup00001,status=it_thresh\n\n\n\n\n6. How many unique isogroups are in the file?\n\n\nFirst, searching for &gt; with grep will extract all lines with contig information:\ngrep '&gt;' my_file.fasta | head -n 2\n&gt;contig00001,length=527,numreads=2,gene=isogroup00001,status=it_thresh\n&gt;contig00002,length=551,numreads=8,gene=isogroup00001,status=it_thresh\nNow, add cut to extract the 4th column:\ngrep '&gt;' my_file.fasta | cut -d ',' -f 4 | head -n 2\ngene=isogroup00001\ngene=isogroup00001\nFinally, add sort -&gt; uniq -&gt; wc -l to count the number of unique occurrences:\ngrep '&gt;' my_file.fasta | cut -d ',' -f 4 | sort | uniq | wc -l\n43\n\n\n\n\n7. Which contig has the highest number of reads (numreads)? How many reads does it have?\n\n\nFirst, we need to isolate the number of reads as well as the contig names. We can use a combination of grep and cut:\ngrep '&gt;' my_file.fasta | cut -d ',' -f 1,3 | head -n 3\n&gt;contig00001,numreads=2\n&gt;contig00002,numreads=8\n&gt;contig00003,numreads=2\nNow we want to sort according to the number of reads. However, the number of reads is part of a more complex string. We can use -t '=' to split according to the = sign, and then take the second column (-k 2) to sort numerically (-n):\ngrep '&gt;' my_file.fasta | cut -d ',' -f 1,3 | sort -t '=' -k 2 -n | head -n 5\n&gt;contig00089,numreads=1\n&gt;contig00176,numreads=1\n&gt;contig00210,numreads=1\n&gt;contig00001,numreads=2\n&gt;contig00003,numreads=2\nAdding the sort option -r, we can sort in reverse order, which tells us that contig00302 has the highest coverage, with 3330 reads:\ngrep '&gt;' my_file.fasta | cut -d ',' -f 1,3 | sort -t '=' -k 2 -n -r | head -n 1\n&gt;contig00302,numreads=3330\n\n\n\n\n\nExercise 5\n\n\nHow many times were the levels of individuals 3 and 27 recorded?\n\n\nFirst, let’s move back into the data dir:\ncd ../data\nNext, let’s take a look at the structure of the file:\nhead -n 3 Gesquiere2011_data.csv\nmaleID        GC      T\n1     66.9    64.57\n1     51.09   35.57\nWe want to extract all the rows in which the first column is 3 (or 27), and count them. To extract only the first column, we can use cut:\ncut -f 1 Gesquiere2011_data.csv | head -n 3\nmaleID\n1\n1\n\n\nThen we can pipe the results to grep -c to count the number of occurrences (note the option -w to match whole “words” – this will make it match 3 but not 13 or 23):\n# For maleID 3\ncut -f 1 Gesquiere2011_data.csv | grep -c -w 3\n61\n# For maleID 27\ncut -f 1 Gesquiere2011_data.csv | grep -c -w 27\n5"
  },
  {
    "objectID": "week03/w3_1_project-org.html#overview-of-this-week",
    "href": "week03/w3_1_project-org.html#overview-of-this-week",
    "title": "Project (file) organization",
    "section": "1 Overview of this week",
    "text": "1 Overview of this week\n\nThis page:\n\nLearn some best practices for project organization, documentation, and management.\n\nAlso today\n\nGet to know our text editor, VS Code.\nLearn how to use Markdown for documentation (and beyond).\n\nSecond session\n\nLearn how to manage files in the Unix shell."
  },
  {
    "objectID": "week03/w3_1_project-org.html#project-organization-best-practices-recommendations",
    "href": "week03/w3_1_project-org.html#project-organization-best-practices-recommendations",
    "title": "Project (file) organization",
    "section": "2 Project organization: best practices & recommendations",
    "text": "2 Project organization: best practices & recommendations\nGood project documentation and file organization facilitates:\n\nCollaborating with others (and with your future self…)\nReproducibility\nAutomation\nVersion control\nPreventing your files from slowly devolving into a state of incomprehensible chaos\n\nIn short, good project documentation and file organization is a necessary foundation to use this course’s tools and to reach some of its goals.\n\n\n2.1 Some underlying principles\n\nUse one dir (dir hierarchy) for one project\nUsing one directory hierarchy for one project means:\n\nDon’t mix files/subdirs for multiple distinct projects inside one dir.\nDon’t keep files for one project in multiple places.\n\nWhen you have a single directory hierarchy for each project, it is:\n\nEasier to find files, share your project, avoid throwing away stuff in error, etc.\nPossible to use relative paths within a project’s scripts, which makes it more portable (more on that in a bit).\n\n\n\n\n\nTwo project dir hierarchies, nicely separated and self-contained.\n\n\n\n\n\nSeparate different kinds of files using a consistent dir structure\nWithin your project’s directory hierarchy:\n\nSeparate code from data.\nSeparate raw data from processed data & results.\n\nAlso:\n\nTreat raw data as read-only.\nTreat generated output as (somewhat) disposable and as possible to regenerate.\n\nAnd, as we’ll talk about below:\n\nUse consistent dir and file naming that follow certain best practices.\nSlow down and document what you’re doing.\n\n\n\n\n\n2.2 Absolute versus relative paths\nRecall that:\n\nAbsolute paths start from the computer’s root dir and do not depend on your working dir.\nRelative paths start from a specific working dir (and won’t work if you’re elsewhere).\n\n\n\nDon’t absolute paths sound better? What could be a disadvantage of them?\n\nAbsolute paths:\n\nDon’t generally work across computers\nBreak when you move your entire project dir\n\nRelative paths, on the other hand, keep working when moving the project within and between computers — as long as you consistently use the top-level dir of the project as the working dir.\n\n\n\n\n\nTwo project dir hierarchies, and the absolute and relative path to a FASTQ file.\n\n\n\n\n\n\nNow everything was moved into Dropbox.The absolute path has changed, but the relative path remains the same.\n\n\n\n\n\n2.3 But how to define and separate projects?\nFrom Wilson et al. 2017 - Good Enough Practices in Scientific Computing:\n\nAs a rule of thumb, divide work into projects based on the overlap in data and code files:\n\nIf 2 research efforts share no data or code, they will probably be easiest to manage independently.\nIf they share more than half of their data and code, they are probably best managed together.\nIf you are building tools that are used in several projects, the common code should probably be in a project of its own.\n\n\n\nProjects with shared data or code\nTo access files outside of the project (e.g., shared across projects), it is easiest to create links to these files:\n\n\n\nThe data is located in project1 but used in both projects.project2 contains a link to the data.\n\n\nBut shared data or scripts are generally better stored in separate dirs, and then linked to by each project using them:\n\n\n\nNow, the data is in it’s own top-level dir, with links to it in both projects.\n\n\nThese strategies do decrease the portability of your project, and moving the shared files even within your own computer will cause links to break.\nA more portable method is to keep shared (multi-project) files online — this is especially feasible for scripts under version control:\n\n\n\nA set of scripts shared by two projects is stored in an online repository like at GitHub.\n\n\n\nFor data, this is also possible but often not practical due to file sizes. It’s easier after data has been deposited in a public repository.\n\n\n\n\n\n2.4 Example project dir structure\nHere is one good way of organizing a project with top-levels dirs:\n\n\n\n\n\n\n\n\n\n\n\nSome other reasonable options\n\n\n\nThese recommendations only go so far, and several things do depend on personal preferences and project specifics:\n\ndata as single top-level dir, or separate metadata, refdata, raw_data dirs?\n\nNaming of some dirs, like:\n\nresults vs analysis (Buffalo)\nsrc (“source”) vs scripts\n\nSometimes the order of subdirs can be done in multiple different ways. For example, where to put QC figures — results/plots/qc or results/qc/plots/?\n\n\n\nAnother important good practice is to use subdirectories liberally and hierarchically. For example, in omics data analysis, it often makes sense to create subdirs within results for each piece of software that you are using:\n\n\n\nAn example showing subdirs within results organized by the software that produced the results. Separate runs for a single program (multiqc) are grouped into further subdirs by date."
  },
  {
    "objectID": "week03/w3_1_project-org.html#file-naming",
    "href": "week03/w3_1_project-org.html#file-naming",
    "title": "Project (file) organization",
    "section": "3 File naming",
    "text": "3 File naming\nHere are three key principles for good file names (from Jenny Bryan) — they should:\n\nBe machine-readable\nBe human-readable\nPlay well with default ordering\n\nWe’ll go into each of these aspects below.\n\nMachine-readable\nConsistent and informative naming helps you to programmatically find and process files.\n\nIn file names, you can provide metadata like Sample ID, date, and treatment:\n\nsample032_2016-05-03_low.txt\n\nsamples_soil_treatmentA_2019-01.txt\n\nWith such file names, you can easily select samples from e.g. a certain month or treatment (more on Thursday):\nls *2016-05*\n\nls *treatmentA*\nDon’t use spaces in file names, as these lead to inconvenience at best and disaster at worst when working in the Shell and to some extent with other programming languages (see example below).\nMore generally, only use the following characters in file names:\n\nAlphanumeric characters A-Za-z0-9\nUnderscores _\nHyphens (dashes) -\nPeriods (dots) .\n\n\n\n\n\n Spaces in file names — what could go wrong?\n\nSay, you have a dir with some raw data in the dir raw:\nls\nraw\nNow you create a dir for sequences, with a space in the file name — this is possible as shown below by quoting the full name:\nmkdir \"raw sequences\"\nYou don’t want this dir after all, and carelessly try to remove it\nrm -r raw sequences\n\n\n\n\nWhat will happen when that last command is run? (Click for the answer)\n\nBecause the \"...\" quotes were omitted with the rm command, it will interpret raw and sequences as two separate arguments, i.e. as two separate files/dirs to remove.\nTherefore, the rm command will not remove the raw sequences dir, but it will remove the “earlier” raw dir — now your project’s data has been erased! 😳\n(Additionally, it will produce an error because it cannot find the sequences dir.)\n\n\n\n\nHuman-readable\n\n“Name all files to reflect their content or function. For example, use names such as bird_count_table.csv, manuscript.md, or sightings_analysis.py.”\n— Wilson et al. 2017\n\n\n\n\nCombining machine- and human-readable\n\nOne good way (opinionated recommendations):\n\nUse underscores (_) to delimit units you may later want to separate on: sampleID, batch, treatment, date.\nWithin such units, use dashes (-) to delimit words: grass-samples.\nLimit the use of periods (.) to indicate file extensions.\nGenerally avoid capitals.\n\nFor example:\nmmus001_treatmentA_filtered-mq30-only_sorted_dedupped.bam\nmmus002_treatmentA_filtered-mq30-only_sorted_dedupped.bam\n.\n.\nmmus086_treatmentG_filtered-mq30-only_sorted_dedupped.bam\n\n\n\n\nPlaying well with default ordering\n\nUse leading zeros for lexicographic sorting: sample005.\n(If you don’t do this, sample11 will appear before sample2, etc.)\nDates should be written as YYYY-MM-DD: for example, 2020-10-11. Besides leading to correct ordering, this format is also unambiguous.\nGroup similar files together by starting with same phrase, and number scripts by execution order:\nDE-01_normalize.R\nDE-02_test.R\nDE-03_process-significant.R"
  },
  {
    "objectID": "week03/w3_1_project-org.html#slow-down-and-document",
    "href": "week03/w3_1_project-org.html#slow-down-and-document",
    "title": "Project (file) organization",
    "section": "4 Slow down and document",
    "text": "4 Slow down and document\n\nUse README files to document\nUse README files to document the following:\n\nYour methods\nWhere/when/how each data and metadata file originated\nVersions of software, databases, reference genomes\n…Everything needed to rerun whole project\n\n\n\n\n\n\n\nSee this week’s Buffalo chapter (Ch. 2) for further details.\n\n\n\n\n\n\n\n\n\nFor documentation, use plain text files\nAs pointed out before, plain text files offer several benefits over proprietary & binary formats like .docx and .xlsx — and these considerations apply not just to files for documentation, but also for data and results:\n\nCan be accessed on any computer, including over remote connections\nAre future-proof\nAllow to be version-controlled\n\nMarkdown files are plain-text and strike a nice balance between ease of writing and reading, and added functionality — we’ll talk about those next."
  },
  {
    "objectID": "week03/w3_overview.html#content-overview",
    "href": "week03/w3_overview.html#content-overview",
    "title": "Week 3: Project organization and Markdown",
    "section": "1 Content overview",
    "text": "1 Content overview\nThis week, we’ll talk about some best practices for project organization, managing your project’s files in the Unix shell, and documenting your project with Markdown files. We’ll also spend a bit of time getting to know VS Code, the text editor that you will spend a lot of time in during this course.\nSome of the things you will learn this week:\n\nProject organization (Tuesday)\n\nA number of best practices for project organization, documentation, and management.\n\n\n\nMarkdown & VS Code (Tuesday)\n\nHow to use Markdown for documentation (and beyond).\nGet to know our text editor for the course, VS Code.\n\n\n\nAdvanced file management in the shell (Thursday)\n\nWildcard expansion to select and operate on multiple files at once\nCommand substitution to save the output of commands\nFor loops to repeat operations, e.g. across files\nRenaming multiple files using for loops"
  },
  {
    "objectID": "week03/w3_overview.html#exercises-assignments",
    "href": "week03/w3_overview.html#exercises-assignments",
    "title": "Week 3: Project organization and Markdown",
    "section": "2 Exercises & assignments",
    "text": "2 Exercises & assignments\n\nExercises\nNo assignments this week, but next week’s assignment will in part be based on this material."
  },
  {
    "objectID": "week03/w3_overview.html#readings",
    "href": "week03/w3_overview.html#readings",
    "title": "Week 3: Project organization and Markdown",
    "section": "3 Readings",
    "text": "3 Readings\n\nOptional readings\n\nBuffalo Chapter 2: Setting up and Managing a Bioinformatics Project\nBuffalo Chapter 3: Remedial Unix Shell\nWilson et al. 2017, PLOS Computational Biology: Good enough practices in scientific computing"
  },
  {
    "objectID": "week04/w4_3_advanced-git.html",
    "href": "week04/w4_3_advanced-git.html",
    "title": "Branching, collaborating, and undoing",
    "section": "",
    "text": "This page contains optional self-study material if you want to dig deeper into Git. Some of it may also be useful as a reference in case you run into problems while trying to use Git."
  },
  {
    "objectID": "week04/w4_3_advanced-git.html#branching-merging",
    "href": "week04/w4_3_advanced-git.html#branching-merging",
    "title": "Branching, collaborating, and undoing",
    "section": "1 Branching & merging",
    "text": "1 Branching & merging\nIn this section, you’ll learn about using so-called “branches” in Git. Branches are basically parallel versions of your repository, which allow you or your collaborators to experiment or create variants without affecting existing functionality or others’ work.\n\n\n1.1 A repo with a couple of commits\nFirst, you’ll create a dummy repo with a few commits by running a script (following CSB).\ncd /fs/ess/PAS2700/users/$USER/CSB/git/sandbox\nTake a look at the script you will run to create your repo:\ncat ../data/create_repository.sh\n#!/bin/bash\n\n# function of the script:\n# sets up a repository and\n# immitates workflow of\n# creating and commiting two text files\n\nmkdir branching_example\ncd branching_example\ngit init\necho \"Some great code here\" &gt; code.txt\ngit add .\ngit commit -m \"Code ready\"\necho \"If everything would be that easy!\" &gt; manuscript.txt \ngit add .\ngit commit -m \"Drafted paper\"\nRun the script:\nbash ../data/create_repository.sh\nInitialized empty Git repository in /fs/ess/PAS2700/users/jelmer/CSB/git/sandbox/branching_example/.git/\n[main (root-commit) 3c59d8a] Code ready\n 1 file changed, 1 insertion(+)\n create mode 100644 code.txt\n[main 7ba8ca4] Drafted paper\n 1 file changed, 1 insertion(+)\n create mode 100644 manuscript.txt\nAnd move into the repository’s dir:\ncd branching_example\nLet’s see what has been done in this repo:\ngit log --oneline\n7ba8ca4 (HEAD -&gt; main) Drafted paper\n3c59d8a Code ready\nWe will later modify the file code.txt — let’s see what it contains now:\ncat code.txt\nSome great code here\n\n\n\n1.2 Using branches in Git\nYou now want to improve the code, but these changes are experimental, and you want to retain your previous version that you know works. This is where branching comes in. With a new branch, you can make changes that don’t affect the main branch, and can also keep working on the main branch:\n\n\n\nFigure modified after Allesino & Wilmes (2019).(Note that the main branch is here called “master”.)\n\n\n\nCreating a new branch\nFirst, create a new branch as follows, naming it fastercode:\ngit branch fastercode\nList the branches:\n# Without args, git branch will list the branches\ngit branch\n  fastercode\n* main\nIt turns out that you created a new branch but are still on the main branch, as the * indicates.\nYou can switch branches with git checkout:\ngit checkout fastercode\nSwitched to branch 'fastercode'\nAnd confirm your switch with git branch:\ngit branch\n* fastercode\n  main\nNote that you can also tell from the git status output on which branch you are:\ngit status\nOn branch fastercode\nnothing to commit, working tree clean\n\n\n\nMaking experimental changes on the new branch\nYou edit the code, stage and commit the changes:\necho \"Yeah, faster code\" &gt;&gt; code.txt\ncat code.txt\nSome great code here\nYeah, faster code\ngit add code.txt\ngit commit -m \"Managed to make code faster\"\n[fastercode 21f1828] Managed to make code faster\n 1 file changed, 1 insertion(+)\nLet’s check the log again, which tells you that the last commit was made on the fastercode branch:\ngit log --oneline\n21f1828 (HEAD -&gt; fastercode) Managed to make code faster\n7ba8ca4 (main) Drafted paper\n3c59d8a Code ready\n\n\n\nMoving back to the main branch\nYou need to switch gears and add references to the paper draft. Since this has nothing to do with your attempt at faster code, you should make these changes back on the main branch:\n# Move back to the 'main' branch\ngit checkout main\nSwitched to branch 'main'\nWhat does code.txt, which we edited on fastercode, now look like?\ncat code.txt\nSome great code here\nSo, by switching between branches, your working dir contents has changed!\nNow, while still on the main branch, add the reference, stage and commit:\necho \"Marra et al. 2014\" &gt; references.txt\ngit add references.txt\ngit commit -m \"Fixed the references\"\n[main 1bf123f] Fixed the references\n 1 file changed, 1 insertion(+)\n create mode 100644 references.txt\nNow that you’ve made changes to both branches, let’s see the log in “graph” format with --graph, also listing all branches with --all — note how it tries to depict these branches:\ngit log --oneline --graph --all\n* 1bf123f (HEAD -&gt; main) Fixed the references\n| * 21f1828 (fastercode) Managed to make code faster\n|/  \n* 7ba8ca4 Drafted paper\n* 3c59d8a Code ready\n\n\n\nFinishing up on the experimental branch\nEarlier, you finished speeding up the code in the fastercode branch, but you still need to document your changes. So, you go back:\ngit checkout fastercode\nSwitched to branch 'fastercode'\nDo you still have the references.txt file from the main branch?\nls\ncode.txt  manuscript.txt\nNope, your working dir has changed again.\nThen, add the “documentation” to the code, and stage and commit:\necho \"# My documentation\" &gt;&gt; code.txt\ngit add code.txt\ngit commit -m \"Added comments to the code\"\n[fastercode d09f611] Added comments to the code\n 1 file changed, 1 insertion(+)\nCheck the log graph:\ngit log --oneline --all --graph\n* d09f611 (HEAD -&gt; fastercode) Added comments to the code\n* 21f1828 Managed to make code faster\n| * 1bf123f (main) Fixed the references\n|/  \n* 7ba8ca4 Drafted paper\n* 3c59d8a Code ready\n\n\n\nMerging the branches\nYou’re happy with the changes to the code, and want to make the fastercode version the default version of the code. This means you should merge the fastercode branch back into main. To do so, you first have to move back to main:\ngit checkout main\nSwitched to branch 'main'\nNow you are ready to merge with the git merge command. You’ll also have to provide a commit message, because a merge is always accompanied by a commit:\ngit merge fastercode -m \"Much faster version of code\"\nMerge made by the 'ort' strategy.\n code.txt | 2 ++\n 1 file changed, 2 insertions(+)\nOnce again, check the log graph, which depicts the branches coming back together:\ngit log --oneline --all --graph\n*   5bb84cd (HEAD -&gt; main) Much faster version of code\n|\\  \n| * d09f611 (fastercode) Added comments to the code\n| * 21f1828 Managed to make code faster\n* | 1bf123f Fixed the references\n|/  \n* 7ba8ca4 Drafted paper\n* 3c59d8a Code ready\n\n\n\nCleaning up\nYou no longer need the fastercode branch, so you can delete it as follows:\ngit branch -d fastercode\nDeleted branch fastercode (was d09f611).\n\n\n\n\n1.3 Branching and merging – Workflow summary\n\n\n\nFigure from after Allesino & Wilmes (2019)\n\n\n\nOverview of commands used in the branching workflow\n# (NOTE: Don't run this)\n\n# Create a new branch:\ngit branch mybranch\n\n# Move to new branch:\ngit checkout mybranch\n\n# Add and commit changes:\ngit add --all\ngit commit -m \"my message\"\n\n# Done with branch - move back to main trunk and merge\ngit checkout main\ngit merge mybranch -m \"Message for merge\"\n\n# And [optionally] delete the branch:\ngit -d mybranch\n\n\n\n Exercise (Intermezzo 2.2)\n\n(a) Move to the directory CSB/git/sandbox.\n\n\n\nSolution\n\ncd /fs/ess/PAS2700/users/$USER/CSB/git/sandbox\n\n\n(b) Create a directory thesis and turn it into a Git repository.\n\n\n\nSolution\n\nmkdir thesis\ncd thesis\ngit init\n\n\n(c) Create the file introduction.txt with the line “Best introduction ever.”\n\n\n\nSolution\n\necho \"The best introduction ever\" &gt; introduction.txt\n\n\n(d) Stage introduction.txt and commit with the message “Started introduction.”\n\n\n\nSolution\n\ngit add introduction.txt\ngit commit -m \"Started introduction\"\n\n\n\n(e) Create the branch newintro and change into it.\n\n\n\nSolution\n\ngit branch newintro\ngit checkout newintro\n\n\n(f) Overwrite the contents of introduction.txt, create a new file methods.txt, stage, and commit.\n\n\n\nSolution\n\necho \"A much better introduction\" &gt; introduction.txt\ntouch methods.txt\ngit add --all\ngit commit -m \"A new introduction and methods file\"\n\n\n(g) Move back to main. What does your working directory look like now?\n\n\n\nSolution\n\ngit checkout main\nls     # Changes made on the other branch are not visible here!\ncat introduction.txt\n\n\n(h) Merge in the newintro branch, and confirm that the changes you made there are now in your working dir.\n\n\n\nSolution\n\ngit merge newintro -m \"New introduction\"\nls\ncat introduction.txt\n\n\n(i) Bonus: Delete the branch newintro.\n\n\n\nSolution\n\ngit branch -d newintro"
  },
  {
    "objectID": "week04/w4_3_advanced-git.html#collaboration-with-git-multi-user-remote-workflows",
    "href": "week04/w4_3_advanced-git.html#collaboration-with-git-multi-user-remote-workflows",
    "title": "Branching, collaborating, and undoing",
    "section": "2 Collaboration with Git: multi-user remote workflows",
    "text": "2 Collaboration with Git: multi-user remote workflows\nIn a multi-user workflow, your collaborator can make changes to the repository (committing to local, then pushing to remote), and you need to make sure that you stay up-to-date with these changes.\nSynchronization between your and your collaborator’s repository happens via the remote, so now you will need a way to download changes from the remote that your collaborator made. This happens with the git pull command.\n\n\n\n\n\nAFirst, a second user, your collaborator, downloads (clones) the online repo. They should also receive admin rights on the repo (not shown - done on GitHub).\n\n\n\n\n\n\n\n\nBThen, your collaborator commits changes to their local copy of the repository.\n\n\n\n\n\n\n\n\n\n\nCBefore you can receive these changes, your collaborator will need to push their changes to the remote, which you can access too.\n\n\n\n\n\n\n\n\nDTo update your local repo with the changes made by your collaborator, you pull in the changes from the remote. Now all 3 copies of the repo are in sync again!\n\n\n\n\n\nIn a multi-user workflow, changes made by different users are shared via the online copy of the repo. But syncing is not automatic:\n\nChanges to your local repo remain local-only until you push to remote.\nSomeone else’s changes to the remote repo do not make it into your local repo until you pull from remote.\n\nHowever, when your collaborator has made changes, Git will tell you about “divergence” between your local repository and the remote when you run git status:\n# (Don't run this)\ngit status\n\n\n\n\n\nIn a multi-user workflow, you should use use git pull often, since staying up-to-date with your collaborator’s changes will reduce the chances of merge conflicts.\n\n\n2.1 Add a collaborator in GitHub\nYou can add a collaborator to a repository on GitHub as follows:\n\nGo to the repository’s settings:\n\n\n\n\n\n\n\nFind and click Manage access:\n\n\n\n\n\n\n\nClick Invite a collaborator:\n\n\n\n\n\n\n\n\n\n2.2 Merge conflicts\nA so-called merge conflict means that Git is not able to automatically merge two branches, which occurs when all three of the following conditions are met:\n\nYou try to merge two branches (including when pulling from remote: a pull includes a merge)\nOne or more file changes have been committed on both of these branches since their divergence.\nSome of these changes were made in the same part(s) of file(s).\n\nWhen this occurs, Git has no way of knowing which changes to keep, and it will report a merge conflict as follows:\n\n\n\n\n\n\nResolving a merge conflict\nWhen Git reports a merge conflict, follow these steps:\n\nUse git status to find the conflicting file(s).\n\n\n\n\n\n\n\nOpen and edit those file(s) manually to a version that fixes the conflict (!).\nNote below that Git will have changed these file(s) to add the conflicting lines from both versions of the file, and to add marks that indicate which lines conflict.\nYou have to manually change the contents in your text editor to keep the conflicting content that you want, and to remove the indicator marks that Git made.\nOn the Origin of Species       # Line preceding conflicting line\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD                   # GIT MARK 1: Next line = current branch\nLine 2 - from main             # Conflict line: current branch\n=======                        # GIT MARK 2: Dividing line\nLine 2 - from conflict-branch  # Conflict line: incoming branch\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; conflict-branch        # GIT MARK 3: Prev line = incoming branch\nUse git add to tell Git you’ve resolved the conflict in a particular file:\ngit add origin.txt\n\n\n\n\n\n\n\nOnce all conflicts are resolved, use git status to check that all changes have been staged. Then, use git commit to finish the merge commit:\ngit commit -m \"Solved the merge conflict\"\n\n\n\n\n\n\n\n\nVS Code functionality for resolving Merge Conflicts\n\n\n\nVS Code has some nice functionality to make Step 2 (resolving the conflict) easier:\ncode &lt;conflicting-file&gt;  # Open the file in VS Code\n\n\n\n\n\nIf you click on “Accept Current Change” or “Accept Incoming Change”, etc., it will keep the desired lines and remove the Git indicator marks. Then, save and exit."
  },
  {
    "objectID": "week04/w4_3_advanced-git.html#contributing-to-repositories-forking-pull-requests",
    "href": "week04/w4_3_advanced-git.html#contributing-to-repositories-forking-pull-requests",
    "title": "Branching, collaborating, and undoing",
    "section": "3 Contributing to repositories: Forking & Pull Requests",
    "text": "3 Contributing to repositories: Forking & Pull Requests\n\n3.1 What can you do with someone else’s GitHub repository?\nIn some cases, you may be interested in working in some way with someone else’s repository that you found on GitHub. If you do not have rights to push, you can:\n\nClone the repo and make changes locally (as we have been doing with the CSB repo). When you do this, you can also periodically pull to remain up-to-date with changes in the original repo.\nFork the repository on GitHub and develop it independently. Forking creates a new personal GitHub repo, to which you can push.\nUsing a forked repo, you can also submit a Pull Request with proposed changes to the original repo: for example, if you’ve fixed a bug in someone else’s program.\n\nIf you’re actually collaborating on a project, though, you should ask your collaborator to give you admin rights for the repo, which makes things a lot easier than working via Pull Requests.\n\n\n\n\n\n\nForking, Pull Requests, and Issues are GitHub functionality, and not part of Git.\n\n\n\n\n\n\n\nForking a GitHub repository\nYou can follow along by e.g. forking my originspecies repo.\n\nGo to a GitHub repository, and click the “Fork” button in the top-right:\n\n\n\n\n\n\n\nYou may be asked which account to fork to: select your account.\nNow, you have your own version of the repository, and it is labeled explicitly as a fork:\n\n\n\n\n\n\n\n\nForking workflow\nYou can’t directly modify the original repository, but you can:\n\nFirst, modify your fork (with local edits and pushing).\nThen, submit a so-called Pull Request to the owner of the original repo to pull in your changes.\nAlso, you can also easily keep your fork up-to-date with changes to the original repository.\n\n\n\n\nFigure from Happy Git and GitHub for the useR\n\n\n\n\n\nEditing the forked repository\nTo clone your forked GitHub repository to a dir at OSC, start by creating a dir there — for example:\nmkdir /fs/ess/PAS2700/users/$USER/week03/fork_test\ncd /fs/ess/PAS2700/users/$USER/week03/fork_test\nThen, find the URL for your forked GitHub repository by clicking the green Code button. Make sure you get the SSH URL (rather than the HTTPS URL), and click the clipboard button next to the URL to copy it:\n\n\n\n\n\nThen, type git clone and a space, and paste the URL, e.g.:\ngit clone git@github.com:jelmerp/originspecies.git\nCloning into 'originspecies'...\nremote: Enumerating objects: 31, done.\nremote: Counting objects: 100% (31/31), done.\nremote: Compressing objects: 100% (19/19), done.\nremote: Total 31 (delta 4), reused 30 (delta 3), pack-reused 0\nReceiving objects: 100% (31/31), done.\nResolving deltas: 100% (4/4), done.\nNow, you can make changes to the repository in the familiar way, for example:\necho \"# Chapter 1. Variation under domestication\" &gt; origin.txt\ngit add origin.txt\ngit commit -m \"Suggested title for first chapter.\"\nAnd note that you can push without any setup — because you cloned the repository, the remote setup is already done (and you have permission to push because its your own repo on GitHub and you have set up GitHub authentication):\ngit push\n\n\n\nCreating a Pull Request\nIf you then go back to GitHub, you’ll see that your forked repo is “x commit(s) ahead” of the original repo:\n\n\n\n\n\nClick Pull request, and check whether the right repositories and branches are being compared (and here you can also see the changes that were made in the commits):\n\n\n\n\n\nIf it looks good, click the green Create Pull Request button:\n\n\n\n\n\nGive your Pull Request a title, and write a brief description of your changes:\n\n\n\n\n\n\n\n\nKeeping your fork up-to-date\nAs you saw, you can’t directly push to original repo but instead have to submit a Pull Request (yes, this terminology is confusing!).\nBut, you can create an ongoing connection to the original repo, which you can use to periodically pull to keep your fork up-to-date. This works similarly to connecting your own GitHub repo, but you should give the remote a different nickname than origin — the convention is upstream:\n# Add the \"upstream\" connection\ngit remote add upstream git@github.com:jelmerp/originspecies.git\n\n# List the remotes:\ngit remote -v\norigin   git@github.com:pallass-boszanger/originspecies.git  (fetch)\norigin   git@github.com:pallass-boszanger/originspecies.git  (push)\nupstream   git@github.com:jelmerp/originspecies.git  (fetch)\nupstream   git@github.com:jelmerp/originspecies.git  (push)\n# Pull from the upstream repository:\ngit pull upstream main\n\n\n\n\n\n\n“upstream” is an arbitrary but convential name, compare with “origin” which is used for your own version of the online repo."
  },
  {
    "objectID": "week04/w4_3_advanced-git.html#undoing-viewing-changes-that-have-been-committed",
    "href": "week04/w4_3_advanced-git.html#undoing-viewing-changes-that-have-been-committed",
    "title": "Branching, collaborating, and undoing",
    "section": "4 Undoing (& viewing) changes that have been committed",
    "text": "4 Undoing (& viewing) changes that have been committed\nWhereas in the first Git session, we learned about undoing changes that have not been committed, here you’ll see how you can undo changes that have been committed.\n\n\n4.1 Viewing past versions of the repository\nBefore undoing committed changes, you may want to look at earlier states of your repo, e.g. to know what to revert to:\n\nFirst, print an overview of past commits and their messages:\n# (NOTE: example code in this and the next few boxes - don't run as-is)\ngit log --oneline\nFind a commit you want to go back to, and look around in the past:\ngit checkout &lt;sha-id&gt; # Replace &lt;sha-id&gt; by an actual hash\n\nless myfile.txt       # Etc. ...\nThen, you can go back to where you were originally as follows:\ngit checkout main\n\nThe next section will talk about strategies to move your repo back to an earlier state that you found this way.\n\n\n\n\n\n\nJust need to retrieve an older version of a single file?\n\n\n\nIf you just want to retrieve/restore an older version of a single file that you found while browsing around in the past, then a quick way can be: simply copy the file to a location outside of your repo, move yourself back to the “present”, and move the file back into your repo, now in the present.\n\n\n\n\n\nA visual of using git checkout to view files from older versions of your repo.Figure from https://software-carpentry.org.\n\n\n\n\n\n\n\n\nThe multiple uses of git checkout\n\n\n\nNote the confusing re-use of git checkout! We have now seen git checkout being used to:\n\nMove between branches\nMove to previous commits to explore (figure below)\n(Revert files back to previous states — as an alternative to git restore)\n\n\n\n\n\n\n4.2 Undoing entire commits\nTo undo commits, i.e. move the state of your repository back to how it was before the commit you want to undo, there are two main commands:\n\ngit revert: Undo the changes made by commits by reverting them in a new commit.\ngit reset: Delete commits as if they were never made.\n\n\nUndoing commits with git revert\nA couple of examples of creating a new commit that will revert all changes made in the specified commit:\n# Undo changes by the most recent commit:\ngit revert HEAD\n  \n# Undo changes by the second-to-last commit:\ngit revert HEAD^\n\n# Undo changes by a commit identified by its checksum:\ngit revert e1c5739\n\n\nUndoing commits with git reset\ngit reset is quite complicated as it has three modes (--hard, --mixed (default), and --soft) and can act either on individual files and on entire commits. To undo a commit, and:\n\nStage all changes made by that commit:\n# Resetting to the 2nd-to-last commit (HEAD^) =&gt; undoing the last commit\ngit reset --soft HEAD^\nPut all changes made by that commit as uncomitted working-dir changes:\n# Note that '--mixed' is the default, so you could omit that\ngit reset --mixed HEAD^\nCompletely discard all changes made by that commit:\ngit reset --hard HEAD^ \n\n\n\n\n\n\n\ngit reset erases history\n\n\n\nUndoing with git revert is much safer than with git reset, because git revert does not erase any history.\nFor this reason, some argue you should not use git reset on commits altogether. At any rate, you should never use git reset for commits that have already been pushed online.\n\n\n\n\n\n\n4.3 Viewing & reverting to earlier versions of files\nAbove, you learned to undo at a project/commit-wide level. But you can also undo things for specific files:\n\nGet a specific version of a file from a past commit:\n# Retrieve the version of README.md from the second-to-last commit\ngit checkout HEAD^^ -- README.md\n# Or: Retrieve the version of README.md from a commit IDed by the checksum\ngit checkout e1c5739 -- README.md\nNow, your have the old version in the working dir & staged, which you can optionally check with:\n# Optional: check the file at the earlier state\ncat README.md\ngit status\nYou can go on to commit this version from the past, or go back to the current version, as we will do below:\ngit checkout HEAD -- README.md\n\n\n\n\n\n\n\nBe careful with git checkout\n\n\n\nBe careful with git checkout: any uncommitted changes to this file would be overwritten by the past version you retrieve!\n\n\n\nAn alternative method to view and revert to older versions of specific files is to use git show.\n\nView a file from any commit as follows:\n# Retrieve the version of README.md from the last commit\ngit show HEAD:README.md\n# Or: Retrieve the version of README.md from a commit IDed by the checksum\ngit show ad4ca74:README.md\nRevert a file to a previous version:\ngit show ad4ca74:README.md &gt; README.md"
  },
  {
    "objectID": "week04/w4_3_advanced-git.html#miscellaneous-git",
    "href": "week04/w4_3_advanced-git.html#miscellaneous-git",
    "title": "Branching, collaborating, and undoing",
    "section": "5 Miscellaneous Git",
    "text": "5 Miscellaneous Git\n\n5.1 Amending commits\nLet’s say you forgot to add a file to a commit, or notice a silly typo in something we just committed. Creating a separate commit for this seems “wasteful” or even confusing, and including these changes along with others in a next commit is also likely to be inappropriate. In such cases, you can amend the previous commit.\nFirst, stage the forgotten or fixed file:\n# (NOTE: don't run this)\ngit add myfile.txt\nThen, amend the commit, adding --no-edit to indicate that you do not want change the commit message:\n# (NOTE: don't run this)\ngit commit --amend --no-edit\n\n\n\n\n\n\nAmending commits is a way of “changing history”\n\n\n\nBecause amending a commit “changes history”, some recommend avoiding this altogether. For sure, do not amend commits that have been published in (pushed to) the online counterpart of the repo.\n\n\n\n\n\n5.2 git stash\nGit stash can be useful when you need to pull from the remote, but have changes in your working dir that:\n\nAre not appropriate for a separate commit\nAre not worth starting a new branch for\n\nHere is an example of the sequence of commands you can use in such cases.\n\nStash changes to tracked files with git stash:\n# (Note: add option '-u' if you need to include untracked files) \ngit stash\nPull from the remote repository:\ngit pull\n“Apply” (recover) the stashed changes back to your working dir:\ngit stash apply\n\n\n\n\n5.3 A few more tips\n\nGit will not pay attention to empty directories in your working dir.\nYou can create a new branch and move to it in one go using:\ngit checkout -b &lt;new-branch-name&gt;\nTo show commits in which a specific file was changed, you can simply use:\ngit log &lt;filename&gt;\n“Aliases” (command shortcuts) can be useful with Git, and can be added in two ways:\n\nBy adding lines like the below to the ~/.gitconfig file:\n[alias]\n  hist = log --graph --pretty=format:'%h %ad | %s%d [%an]' --date=short\n  last = log -1 HEAD  # Just show the last commit\nWith the git config command:\ngit config --global alias.last \"log -1 HEAD\""
  },
  {
    "objectID": "week04/w4_exercises.html",
    "href": "week04/w4_exercises.html",
    "title": "Week 4 exercises: Version control with Git and GitHub",
    "section": "",
    "text": "In this exercise, you will primarily be practicing your Git skills.\nA general tips to keep in mind is to keep checking the status of your repository (repo) with git status before and after nearly all other Git commands that you issue. This will help prevent mistakes, and will also help you understand Git better."
  },
  {
    "objectID": "week04/w4_exercises.html#solutions",
    "href": "week04/w4_exercises.html#solutions",
    "title": "Week 4 exercises: Version control with Git and GitHub",
    "section": "Solutions",
    "text": "Solutions\n\n\n1. Create a new directory at OSC for these exercises, and move there.\n\nFor example:\nmkdir /fs/ess/PAS2880/users/$USER/week03/exercises\ncd /fs/ess/PAS2880/users/$USER/week03/exercises\n\n\n\n2. Initialize a local Git repository inside your new directory.\n\ngit init\n\n\n\n3. Create a README file in Markdown format.\n\necho \"# This dir is for the week3 exercises on Git\" &gt; README.md\n\n\n\n4. Stage and then commit the README file with an appropriate commit message.\n\ngit add README.md\ngit commit -m \"Added a README file\"\n\n\n\n5. Create a second Markdown file with some more contents.\n\nCreate a new empty file:\ntouch git_notes.md\nThen, open the file in the VS Code editor and describe the Git workflow in your own words.\n\n\n\n6. Create at least two commits while you work on the Markdown file.\n\n\nFirst commit, after adding a first batch of content to the Markdown file:\n\ngit add git_notes.md\ngit commit -m \"Started a document with notes on Git\"\n\nMake additional changes to git_notes.md, and commit again:\n\ngit add git_notes.md\ngit commit -m \"Descriptive commit message #2\"\n\n\n\n8. Update the README.md file.\n\necho \"Added a file with notes on Git\" &gt;&gt; README.md\n\n\n\n8. Stage and commit the updated README file.\n\ngit add README.md\ngit commit -m \"Added a description of the repo's contents to the README\"\n\n\n\n9. Create a results dir and file, and make Git ignore this directory.\n\n\nCreate the dir and file that should be ignored:\n\nmkdir results\ntouch results/results.txt\n\nCreate a gitignore file with instructions to ignore the results dir:\n\necho \"results/\" &gt; .gitignore\n\nAdd and commit the .gitignore file:\n\ngit add .gitignore\ngit commit -m \"Added a gitignore file\"\n\n\n\n11. Push your local repo to the online one you just created.\n\n\nStep 1: Set up the connection to the remote repo:\n\n# (replace &lt;SSH-URL-to-repo&gt; with your actual SSH (not HTTPS!) URL)\ngit remote add origin &lt;SSH-URL-to-repo&gt;\n\nStep 2: Push your local repo contents to the remote repo:\n\ngit push -u origin main"
  },
  {
    "objectID": "week04/w4_exercises.html#footnotes",
    "href": "week04/w4_exercises.html#footnotes",
    "title": "Week 4 exercises: Version control with Git and GitHub",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n Recall that in VS Code, you can open the Markdown preview on the side, so you can experiment and see whether your formatting is working the way you intend.↩︎"
  },
  {
    "objectID": "week04/w4_overview.html#content-overview",
    "href": "week04/w4_overview.html#content-overview",
    "title": "Week 4: Git & GitHub",
    "section": "1 Content overview",
    "text": "1 Content overview\nThis week, you will learn about the why and how of using Git version control for your projects, and sharing your code on GitHub.\nBe aware that Git is a challenging topic. Therefore, if you can, complete the main reading before Tuesday’s lecture, and also read the Buffalo chapter at some point this week.\nA good way to get used to Git is to make dummy repositories where you’re just editing one or a few simple text files with dummy lines of text. That way, you can get used to the basic workflow, and freely experiment also with commands to undo things and move back in time. We’ll do this in our Zoom meetings and I recommend you do it outside of there, too.\nSome of the things you will learn this week:\nGetting started with Git\n\nUnderstand why you should use a formal Version Control System (VCS) for research projects.\nLearn the basics of the most widely used VCS: Git.\n\nRemotes on GitHub\n\nLearn how to put your local repositories online at GitHub, and how to keep local and online (“remote”) repositories in sync.\nLearn about single-user and multi-user workflows with Git and GitHub.\n\nOptional self-study content\n\nLearn how to use Git branches to safely make experimental changes.\nLearn how to undo things and “travel back in time” for your project using Git."
  },
  {
    "objectID": "week04/w4_overview.html#assignments-and-exercises",
    "href": "week04/w4_overview.html#assignments-and-exercises",
    "title": "Week 4: Git & GitHub",
    "section": "2 Assignments and exercises",
    "text": "2 Assignments and exercises\n\nUngraded assignment: Create a GitHub account (Do this before Thursday’s class!)\nGraded assignment: Git and Markdown (Due 9/22)\nExercises"
  },
  {
    "objectID": "week04/w4_overview.html#readings",
    "href": "week04/w4_overview.html#readings",
    "title": "Week 4: Git & GitHub",
    "section": "3 Readings",
    "text": "3 Readings\n\nOptional readings\n\nOn this website: Branching, collaborating, and undoing.\nCSB Chapter 2: “Version Control”\nBuffalo Chapter 5: “Git for Scientists”\n\n\n\nFurther resources\n\nGitHub has a nice little overview of some Git and GitHub functionality including branching and Pull Requests, and how to do these things in your browser at GitHub.\nFor some more background on why to use version control, and another perspective on some Git basics, I recommend the article “Excuse me, do you have a moment to talk about version control?” by Jenny Bryan.\nEspecially if you work with R a lot, I would recommend checking out Happy Git and GitHub for the useR, also by Jenny Bryan. This is a very accessible introduction to Git.\nGit-it is a small application to learn and practice Git and GitHub basics.\nIf you want to try some online exercises with Git with helpful visuals of what Git commands do, try https://learngitbranching.js.org/. (But be aware that this does fairly quickly move to fairly advanced topics, including several that we will not touch on in the course.)\nA good list of even more Git resources…"
  },
  {
    "objectID": "week04/w4_1_git.html#an-introduction-to-version-control",
    "href": "week04/w4_1_git.html#an-introduction-to-version-control",
    "title": "Getting started with Git",
    "section": "1 An introduction to version control",
    "text": "1 An introduction to version control\n\n1.1 Why use a Version Control System (VCS)?\nHere are some “versioning”- and backup-related challenges for your research project files that you may run into when not using a formal Version Control System (VCS):\n\nWhat to save periodic copies of?\n\nDo you only save versions of individual files?\nSpace-efficient, but doesn’t allow you to go back to the state of other project files at the same point in time.\nDo you save a copy of the full project periodically?\nBetter than the above option, but can become prohibitive in terms of disk storage.\n\nHow to know what changes were made between saved versions?\nHow to collaborate, especially when working simultaneously?\nHow to restore an accidentally deleted, modified, or overwritten file? This can especially be an issue at OSC where there is no recycle bin or undo button.\nHow to manage simultaneous variants of files, such as when making experimental changes?\n\nA formal VCS can help you with these challenges. With a VCS:\n\nYou can easily see your history of changes.\nYou have a time machine: you can go back to past states of your project (and not just of individual files!).\nYou can do simultaneous collaborative work — you can always track down who made which changes.\nSharing your code and other aspects of your project is easy.\nYou can make experimental changes without affecting current functionality.\n\n\nOr, as the CSB book puts it:\n\nVersion control is a way to keep your scientific projects tidily organized, collaborate on science, and have the whole history of each project at your fingertips.\n— CSB Chapter 2\n\n\n\n\n1.2 How Git works\nGit is the most widely used Version Control System1. With Git, you save “snapshots” of your project with every minor piece of progress. Git manages this cleverly without having to create full copies of the project for every snapshot:\n\n\n\nThe boxes with dashed lines depict files that have not changed: these will not be saved repeatedly.Figure from https://git-scm.com.\n\n\n\nAs illustrated above, files that haven’t changed between snapshots are not saved again and again with every snapshot. But Git doesn’t even save full copies of files that have changed: it tracks changes on a line-by-line basis, and saves changed lines (!).\nNote that one Git database (repository) manages files inside a single directory structure, so to use Git, it’s important that your projects are properly organized or at least kept in separate dirs, as discussed last week.\n\n\nKey Git term 1: Repository (repo)\nA Git “repository” (or “repo”) is the version-control database for a project. Note that:\n\nYou can start a Git repository in any dir on your computer.\nThe Git database is saved in a hidden dir .git in the dir in which you started the repo.\nIt is typical (& recommended) that you should have one Git repository for each research project.\nYou can also download any public online Git repository. (We already did this in week 1 of the course, when we used git clone to download the CSB book’s repository.)\n\n\n\n\n\n\n\nHidden files and dirs\n\n\n\nWhen a file or dir name has a leading ., it’s “hidden”. These don’t show up in file browsers by default, nor in ls file listings unless you use the -a (“all”) option. Hidden files and dirs are often generated automatically by software.\n\n\n\n\nKey Git term 2: Commit\nA Git “commit” is a saved snapshot of the project. For now, note that:\n\nIt is always possible to go the exact state of the entire project or individual files for any commit.\nWhenever you create a commit, you also include a message describing the changes you made.\n\n\n\n\n\n1.3 What do I put under version control?\nThe primary files to put under version control are:\n\nScripts2.\nProject documentation files.\nMetadata.\nManuscripts, if you write them in a plain text format.\n\nWhat about data and results?\n\nRaw data may or may not be included — for omics data, this is generally not feasible due to large file sizes.\nResults from analyses should generally not be included.\n\n\n\nSource versus derived files\nThe general idea behind what you should and should not include is that you should version-control the source, but not derived files. For instance:\n\nVersion-control your Markdown file, not the HTML it produces.\nVersion-control your script, not the output it produces.\n\n\n\n\n\n\n\nDerived files\n\n\n\nRecall last week’s point that results and other derived files are (or should be) dispensable, because they can be regenerated using the raw data and the scripts.\n\n\n\n\n\nFile limitations\nThere are some limitations to the types and sizes of files that can be committed with Git:\n\nFile type: binary (non-text) files, such a Word or Excel files, or compiled software, can be included but can’t be tracked in quite the same way as plain-text files3.\nRepository size: for performance reasons, it’s best to keep individual repositories under about 1 GB.\nFile size: while you can have them in your Git repo, GitHub will not allow you to upload files &gt;100 MB.\n\nAs such, omics data is usually too large to be version-controlled. To make your data available to others, you can use dedicated repositories like the NCBI’s Sequence Read Archive (SRA).\n\n\n\n\n1.4 User Interfaces for Git\n\n\n\nBy xkcd\n\n\nYou can work with Git in several different ways — using:\n\nThe native command-line interface (CLI).\nThird-party graphical user interfaces (GUIs) such as Git Kraken.\nIDEs/editors with Git integration like RStudio and VS Code.\n\nIn this course, we will mainly focus on the CLI because it’s the most universal and powerful interface. But it’s absolutely fine to switch to GUI usage later, which will not be hard if you’ve learned the basics with the CLI.\nGit takes some getting used to, regardless of the interface. Many people have one or more “false starts” with it. I hope that being “forced” to use it in a course4 will take you past that!"
  },
  {
    "objectID": "week04/w4_1_git.html#the-basic-git-workflow",
    "href": "week04/w4_1_git.html#the-basic-git-workflow",
    "title": "Getting started with Git",
    "section": "2 The basic Git workflow",
    "text": "2 The basic Git workflow\nGit commands always start with git followed by a second command/subcommand or “verb”: git add, git commit, etc. Only three commands tend to make up the vast majority of your Git work:\n\ngit add does two things:\n\n\nStart “tracking” files (i.e., files in your directory structure are not automatically included in the repo).\nMark changed/new files as ready to be committed, which is called “staging” files.\n\ngit commit\nCreate a new snapshot of the project by commiting all currently staged files (changes).\ngit status\nGet the status of your repo: which files have changed, which new files are present, tips on next steps, etc.\n\n\n\n\nAdding and committing changes with Git commands.The Git database, which is in a hidden folder .git, is depicted with a gray background.\n\n\n\n\n\n\nAnother way of visualizing the adding and committing of changes in Git.Note that git add has a dual function: it starts tracking files and stages them."
  },
  {
    "objectID": "week04/w4_1_git.html#getting-set-up",
    "href": "week04/w4_1_git.html#getting-set-up",
    "title": "Getting started with Git",
    "section": "3 Getting set up",
    "text": "3 Getting set up\nWe will start with loading Git at OSC5 and then do some one-time personal Git configuration:\n\nLaunch VS Code at https://ondemand.osc.edu as before, at the dir /fs/ess/PAS2880/users/$USER, and open a terminal in VS Code.\nUse git config to make your (actual) name known to Git:\ngit config --global user.name 'John Doe'\nUse git config to make your email address known (use the same email address you signed up for GitHub with):\ngit config --global user.email 'doe.391@osu.edu'\nUse git config to set a default text editor for Git.\n(Occasionally6, Git will open up a text editor for you. Even though we’re using VS Code, here it is better to select a text editor that runs directly in the shell, like nano.)\ngit config --global core.editor \"nano -w\"\nActivate Git output with colors:\ngit config --global color.ui true\nChange the default “branch” name to main:\ngit config --global init.defaultbranch main\nCheck whether you successfully changed the settings:\ngit config --global --list\n# user.name=John Doe\n# user.email=doe.39@osu.edu\n# colour.ui=true\n# core.editor=nano -w\n# init.defaultbranch=main"
  },
  {
    "objectID": "week04/w4_1_git.html#your-first-git-repository",
    "href": "week04/w4_1_git.html#your-first-git-repository",
    "title": "Getting started with Git",
    "section": "4 Your first Git repository",
    "text": "4 Your first Git repository\nYou’ll create a Git repository for a mock book project: writing Charles Darwin’s “On the Origin of Species”.\n\n4.1 Start a new Git repository\nCreate a new dir for a mock project that we will version-control with Git, and move there:\n# Before starting, you should be in /fs/PAS2880/users/$USER, cd there first if needed\nmkdir -p week03/originspecies\ncd week03/originspecies\nThe command to initialize a new Git repository is git init — use that to start a repo for the originspecies dir:\ngit init\nInitialized empty Git repository in /fs/ess/PAS2880/users/jelmer/week03/originspecies/.git/\nCan we confirm that the Git repo dir is there?\n# The -a option to ls will also show hidden files\nls -a\n.  ..  .git\nNext, check the status of your new repository with git status:\ngit status\nOn branch main\n\nNo commits yet\n\nnothing to commit (create/copy files and use \"git add\" to track)\nGit reports that you:\n\nAre on a “branch” that is called ‘main’. We won’t talk about Git branches in class, but this is discussed in the optional self-study material and CSB Chapter 2.6. Basically, these are “parallel versions” of your repository.\nHave not created any commits yet.\nHave “nothing to commit” because there are no files in this dir.\n\n\n\n\n4.2 Your first Git commit\nYou will start writing the book (😉) by echo-ing some text into a new file called origin.txt:\necho \"An Abstract of an Essay on ...\" &gt; origin.txt\nNow, check the status of the repository again:\ngit status\nOn branch main\n\nNo commits yet\n\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n        origin.txt\n\nnothing added to commit but untracked files present (use \"git add\" to track)\nGit has clearly detected the new file. But as mentioned, Git does not automatically start “tracking” files, which is to say it won’t automatically include files in the repository. Instead, it tells you the file is “Untracked” and gives a hint on how to add it to the repository.\nSo, start tracking the file and stage it all at once with git add:\n# (Note that tab-completion on file names will work here, too)\ngit add origin.txt\nCheck the status of the repo again:\ngit status\nOn branch main\n\nNo commits yet\n\nChanges to be committed:\n  (use \"git rm --cached &lt;file&gt;...\" to unstage)\n        new file:   origin.txt\nNow, your file has been added to the staging area (also called the Index) and is listed as a “change to be committed”7. This means that if you now run git commit, the file would be included in that commit.\nSo, with your file tracked & staged, let’s make your first commit. Note that you must add the option -m followed by a “commit message”: a short description of the changes you are including in the current commit.\n# We use the commit message (option '-m') \"Started the book\" to describe our commit\ngit commit -m \"Started the book\"\n[main (root-commit) 3df4361] Started the book\n 1 file changed, 1 insertion(+)\n create mode 100644 origin.txt\nNow that you’ve made your first Git commit, check the status of the repo again:\ngit status\nOn branch main\nnothing to commit, working tree clean\n\n\n\n\n\n\nTry to get used to using git status a lot — as a sanity check before and after other git actions.\n\n\n\n\n\n\nAlso look at the commit history of the repo with git log:\ngit log\ncommit 3df4361c1de9b71e08bf6e050105d53097acec21 (HEAD -&gt; main)\nAuthor: Jelmer Poelstra &lt;jelmerpoelstra@gmail.com&gt;\nDate:   Mon Mar 11 10:55:35 2024 -0400\n\n    Started the book\nNote the “hexadecimal code” (using numbers and the letters a-f) on the first line — this is a unique identifier for each commit, called the SHA-1 checksum. You can reference and access each past commit with these checksums.\n\n\n\n4.3 Your second commit\nStart by modifying the book file — you’ll actually overwrite the earlier content:\necho \"On the Origin of Species\" &gt; origin.txt\nCheck the status of the repo:\ngit status\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n        modified:   origin.txt\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\nGit has noticed the changes, because the file is being tracked: origin.txt is listed as “modified”. But changes to tracked files aren’t automatically staged — use git add to stage the file as a first step to committing these changes:\ngit add origin.txt\nNow, make your second commit:\ngit commit -m \"Changed the title as suggested by Murray\"\n[main f106353] Changed the title as suggested by Murray\n 1 file changed, 1 insertion(+), 1 deletion(-)\nGit gives a brief summary of the changes that were made: you changed 1 file (origin.txt), and since you replaced the line of text in that file, it is interpreting that as 1 insertion (the new line) and 1 deletion (the removed/replace line).\nCheck the history of the repo again — you’ll see that there are now 2 commits:\ngit log\ncommit f1063537b6a1e0d87d2d52c9e96c38694959997a (HEAD -&gt; main)\nAuthor: Jelmer Poelstra &lt;jelmerpoelstra@gmail.com&gt;\nDate:   Mon Mar 11 11:01:49 2024 -0400\n\n    Changed the title as suggested by Murray\n\ncommit 3df4361c1de9b71e08bf6e050105d53097acec21\nAuthor: Jelmer Poelstra &lt;jelmerpoelstra@gmail.com&gt;\nDate:   Mon Mar 11 10:55:35 2024 -0400\n\n    Started the book\n\n\n\n\n\n\n\nOne-line commit log\n\n\n\nAs you start accumulating commits, you might prefer git log --oneline for a one-line-per-commit summary:\ngit log --oneline\n1e2bba4 Changed the title as suggested by Murray\n4fd04af Started the book\n\n\n\n\n\n\n\n\n\nStaging files efficiently\n\n\n\nWhen you have multiple files that you would like to stage, you don’t need to add them one-by-one:\n# NOTE: Don't run any of this - these are hypothetical examples\n\n# Stage all files in the project (either option works):\ngit add --all\ngit add *\n\n# Stage all files in a specific dir (here: 'scripts') in the project:\ngit add scripts/*\n\n# Stage all shell scripts *anywhere* in the project:\ngit add *sh   \nFinally, you can use the -a option for git commit as a shortcut to stage and commit all changes with a single command (but note that this will not add untracked files):\n# Stage & commit all tracked files:\ngit commit -am \"My commit message\"\n\n\n\n\n\n4.4 What to include in individual commits\nThe last example in the box above showed the -a option to git commit, which allows you to at once stage & commit all changes since the last commit. That seems more convenient than separately git adding files before committing.\nHowever, it’s good practice not to simply and only commit, say, at the end of each day, but instead to try and create commits for units of progress worth saving and as such create separate commits for distinct changes.\nFor example, let’s say that you use git status to check which files you’ve changed since your last commit, and you find that you have:\n\nUpdated a README file to include more information about your samples.\nWorked on a script to run quality control of sequence files.\n\nThese are completely unrelated changes, and it would not be recommended to include both in a single commit.\n\n\n Exercise (CSB Intermezzo 2.1)\n\nCreate a new file todo.txt containing the line: “June 18, 1858: read essay from Wallace”.\n\n\n\nClick to see the solution\n\necho \"June 18, 1858: read essay from Wallace\" &gt; todo.txt\n\n\nUse a Git command to stage the file.\n\n\n\nClick to see the solution\n\ngit add todo.txt\n\n\nCreate a Git commit with the commit message “Added to-do list”.\n\n\n\nClick to see the solution\n\ngit commit -m \"Added to-do list\""
  },
  {
    "objectID": "week04/w4_1_git.html#file-states-and-showing-changes",
    "href": "week04/w4_1_git.html#file-states-and-showing-changes",
    "title": "Getting started with Git",
    "section": "5 File states and showing changes",
    "text": "5 File states and showing changes\n\n5.1 File states (and Git’s three “trees”)\nTracked files can be in one of three states:\n\nUnchanged since the last commit: committed (latest version is in the repo/commits).\nModified and staged since the last commit: staged (latest version is in the Index).\nModified but not staged since the last commit: modified (latest version is in the working dir).\n\n\n\n\n\n\n\n\nThe three trees of Git (Click to expand)\n\n\n\n\n\nThese three states correspond to the three “trees” of Git:\n\nHEAD: State of the project in the most recent commit8.\nIndex (Stage): State of the project ready to be committed.\nWorking directory (Working Tree): State of the project as currently on your computer.\n\n\n\n\n\nThe three “trees” of Git: HEAD, the index, and the working dir.The hexadecimals in the Commits rectangles are abbreviated checksums for each commit.\n\n\n\nOr consider this table for a hypothetical example in which HEAD, the Index, and the working dir all differ with regards to the the version of file 1, and there also is an untracked file in the working dir:\n\n\n\n\n\n\n\n\nFile state\nVersion\nWhich tree\n\n\n\n\nCommitted\nfile 1 version X\nHEAD\n\n\nStaged\nfile 1 version Y\nIndex (stage)\n\n\nModified\nfile 1 version Z\nWorking dir\n\n\nUntracked\nfile 2 version X\nWorking dir\n\n\n\n\n\n\n\n\n\n\n\n\nWays to refer to past commits (Click to expand)\n\n\n\n\n\nTo refer to specific past commits, you can:\n\nUsing the hexadecimal checksum (either the full ID or the 7-character abbreviation)\nUse HEAD notation: HEAD is the most recent commit, and there are two ways of indicating ancestors of HEAD:\n\n\n\n\n\nTo refer to past commits, you can use checksums (e.g. dab0dc4 for the second-to-last commit)or HEAD notation (HEAD^^ or HEAD~2 for the second-to-last commit).\n\n\n\n\n\n\n\n\n5.2 Showing changes\nYou can use the git diff command to show changes that you have made. By default, it will show all changes between the working dir and:\n\nThe last commit if nothing has been staged.\nThe stage (Index) if something has been staged.\n\n\n\n\n\n\n\n“Working dir” in the context of Git\n\n\n\nNote that when I talk about the “working dir” in the context of Git, I mean not just your top-level project/repository directory, or any specific dir within there that you may have cd-ed into, but the entire project/repository directory hierarchy.\nIt is mainly used to distinguish the state of your project on your computer (“working dir”) versus that in the repository (“index” and “commits”), and should technically be referred to as the “working dir tree”.\n\n\nRight now, there are no differences to report in our originspecies repository, because our working dir, the stage/Index, and the last commit are all the same:\n# Git diff will not have output if there are no changes to report\ngit diff\nChange the to-do list (note: for this to work, you should have done the exercise above!), and check again:\necho \"June 20, 1858: Send first draft to Huxley\" &gt;&gt; todo.txt\n\ngit diff\ndiff --git a/todo.txt b/todo.txt\nindex e3b5e55..9aca508 100644\n--- a/todo.txt\n+++ b/todo.txt\n@@ -1 +1,2 @@\n June 18, 1858: read essay from Wallace\n+June 20, 1858: Send first draft to Huxley\nWe won’t go into the details of the above “diff format”, but at the bottom of the output above, you can see some specific changes: the line “Send first draft to Huxley” was added (hence the + sign) in our latest version of the file.\n\n\n\n\n\n\n\nVS Code can show file differences in a nicer way\n\n\n\n\nClick on the Git symbol in the narrow side bar (below the search icon) to open the Source Control side bar.\nIn the source control sidebar, you should see not just the originspecies repository listed, but also the CSB repo9. If needed, click on originspecies to expand it:\n\n\n\n\n\n\n\nWithin the originspecies listing, you should see todo.txt: click on the M next to the file todo.txt, and the following should appear in your editor pane:\n\n\n\n\n\n\nThat’s a much more intuitive overview that makes it clear which line was added.\n\n\n\n\n\n\n\n\nMore git diff (Click to expand)\n\n\n\n\n\n\nTo show changes between the Index (stage) and the last commit, use the --staged option to git diff.\nIf you have changed multiple files, but just want to see differences for one of them, you can specify the filename — in our case here, that will give the same output as the plain git diff command above, since we only changed one file:\ngit diff todo.txt\n# Output not shown, same as above\nYou can also compare your repo or individual files between any two arbitrary commits (for the HEAD notation, see the boxes on the “three trees” of Git above.):\n# Last commit vs second-to-last commit - full repo:\ngit diff HEAD HEAD^\n\n# Last commit vs a specified commit - specific file: \ngit diff HEAD d715c54 todo.txt \n\n\n\n\n\n Exercise: another commit\nStage and commit the changes to todo.txt, then check what you have done.\n\n\nClick to see the solution\n\n\nStage the file:\ngit add todo.txt\nCommit:\ngit commit -m \"Update the TODO list\"\n[main 8ec8103] Update the TODO list\n1 file changed, 1 insertion(+)\nCheck the log:\ngit log\ncommit 8ec8103e8d01b342f9470908b87f0649be53edd5\nAuthor: Jelmer Poelstra &lt;jelmerpoelstra@gmail.com&gt;\nDate:   Mon Mar 11 12:30:35 2024 -0400\n\n    Update the TODO list\n\ncommit 9715ab5325429526a90ea49e9d40a923c93ccb72\nAuthor: Jelmer Poelstra &lt;jelmerpoelstra@gmail.com&gt;\nDate:   Mon Mar 11 11:37:32 2024 -0400\n\n    Added a gitignore file\n\ncommit 603d1792619bf628d66cd91a45cd7114e3d6b95b\nAuthor: Jelmer Poelstra &lt;jelmerpoelstra@gmail.com&gt;\nDate:   Mon Mar 11 11:21:36 2024 -0400\n\n    Added to-do list\n\ncommit f1063537b6a1e0d87d2d52c9e96c38694959997a\nAuthor: Jelmer Poelstra &lt;jelmerpoelstra@gmail.com&gt;\nDate:   Mon Mar 11 11:01:49 2024 -0400\n\n    Changed the title as suggested by Murray\n\ncommit 3df4361c1de9b71e08bf6e050105d53097acec21\nAuthor: Jelmer Poelstra &lt;jelmerpoelstra@gmail.com&gt;\nDate:   Mon Mar 11 10:55:35 2024 -0400\n\n    Started the book"
  },
  {
    "objectID": "week04/w4_1_git.html#ignoring-files-and-directories",
    "href": "week04/w4_1_git.html#ignoring-files-and-directories",
    "title": "Getting started with Git",
    "section": "6 Ignoring files and directories",
    "text": "6 Ignoring files and directories\nAs discussed above, it’s best not to track some files, such as very bulky data files, temporary files, and results.\nWe’ve seen that Git will notice and report any “untracked” files in your project whenever you run git status. This can get annoying and can make it harder to spot changes and untracked files that you do want to add — and you might even accidentally start tracking these files such as with git add --all.\nTo deal with this, you can tell Git not to pay attention to certain files by adding file names and wildcard selections to a .gitignore file. This way, these files won’t be listed as untracked files when you run git status, and they wouldn’t be added even when you use git add --all.\nTo see this in action, let’s start by adding some content that we don’t want to commit to our repository: a dir data, and a file ending in a ~ (a temporary file type that e.g. text editors can produce):\nmkdir data\ntouch data/drawings_1855-{01..12} todo.txt~\nWhen we check the status of the repo, we can see that Git has noticed these files:\ngit status\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n        data/\n        todo.txt~\nIf we don’t do anything about this, Git will keep reporting these untracked files whenever we run git status. To prevent this, we will we create a .gitignore file:\n\nThis file should be in the project’s root dir and should be called .gitignore.\n.gitignore is a plain text file that contains dir and file names/patterns, all of which will be ignored by Git.\nAs soon as such a file exists, Git will automatically check and process its contents.\nIt’s a good idea add and commit this file to the repo.\n\nWe will create our .gitignore file and add the following to it to instruct Git to ignore everything in the data/ dir, and any file that ends in a ~:\necho \"data/\" &gt; .gitignore\necho \"*~\" &gt;&gt; .gitignore\ncat .gitignore\ndata/\n*~\nWhen we check the status again, Git will have automatically processed the contents of the .gitignore file, and the files we want to ignore should no longer be listed as untracked files:\ngit status\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n        .gitignore\nHowever, we do now have an untracked .gitignore file, and we should track and commit this file:\ngit add .gitignore\ngit commit -m \"Added a gitignore file\"\n[main 9715ab5] Added a gitignore file\n 1 file changed, 2 insertions(+)\n create mode 100644 .gitignore\n\n\n\n\n\n\n\nGood project file organization helps with version control\n\n\n\nGood project file organization, as discussed last week, can make your life with Git a lot easier. This is especially true when it comes to files that you want to ignore.\nSince you’ll generally want to ignore data and results files, if you keep all of those in their own top-level directories, it will be easy and not error-prone to tell Git to ignore them. But if you were -for example- mixing scripts and either results or data within dirs, it would be much harder to keep this straight."
  },
  {
    "objectID": "week04/w4_1_git.html#moving-and-removing-tracked-files",
    "href": "week04/w4_1_git.html#moving-and-removing-tracked-files",
    "title": "Getting started with Git",
    "section": "7 Moving and removing tracked files",
    "text": "7 Moving and removing tracked files\nWhen wanting to remove, move, or rename files that are tracked by Git, it is good practice to preface regular rm and mv commands with git: so, git rm &lt;file&gt; and git mv &lt;source&gt; &lt;dest&gt;.\nWhen removing or moving/renaming a tracked file with git rm / git mv, changes will be made to your working dir just like with a regular rm/mv, and the operation will also be staged. For example:\n# (NOTE: Don't run this, hypothetical examples)\ngit rm file-to-remove.txt\ngit mv myoldname.txt mynewname.txt\n\ngit status\nOn branch main\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n        renamed:    myoldname.txt -&gt; mynewname.txt\n        deleted:    file-to-remove.txt\n\n\n\n\n\n\nWhat if I forget to use git rm/git mv? (Click to expand)\n\n\n\n\n\nIt is inevitable that you will occasionally forget about this and e.g. use rm instead of git rm. Fortunately, Git will eventually figure out what happened. For example:\n\nFor a renamed file, Git will first be confused and register both a removed file and an added file:\n# (Don't run this, this is a hypothetical example)\nmv myoldname.txt mynewname.txt\n\ngit status\nOn branch main\nChanges not staged for commit:\n  (use \"git add/rm &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n        deleted:    myoldname.txt\n\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n        mynewname.txt\nBut after you stage both changes (the new file and the deleted file), Git realizes it was renamed instead:\ngit add myoldname.txt\ngit add mynewname.txt\n\ngit status\nOn branch main\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n        renamed:    myoldname.txt -&gt; mynewname.txt\n\nSo, there is no need to stress if you forget this, but when you remember, use git mv and git rm.\n\n\n\n\n\n Exercises: .gitignore and git rm\nA) Create a new directory results with files Galapagos.txt and Ascencion.txt. Add a line to the .gitignore file to ignore these results, and commit the changes to the .gitignore file.\n\n\nClick to see the solution\n\n\nCreate the dir and files:\nmkdir results\ntouch results/Galapagos.txt results/Ascencion.txt\nOptional - check that they are detected by Git (note: only the dir will be shown, not its contents):\ngit status\nOn branch main\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n        results/\n\nnothing added to commit but untracked files present (use \"git add\" to track\nAdd the string “results/” to the .gitignore file:\necho \"results/\" &gt;&gt; .gitignore\nOptional - check the status again:\ngit status\nOn branch main\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n        modified:   .gitignore\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\nLooks good, results/ is no longer listed. But we do need to commit the changes to .gitignore.\nCommit the changes to .gitignore:\ngit add .gitignore\ngit commit -m \"Add results dir to gitignore\"\n[main 33b6576] Add results dir to gitignore\n1 file changed, 1 insertion(+)\n\n\nB) Create and commit an empty new file notes.txt. Then, remove it with git rm and commit your file removal.\n\n\nClick to see the solution\n\n\nCreate the file and add and commit it:\ntouch notes.txt\ngit add notes.txt\ngit commit -m \"Add notes\"\n[main 44a37f9] Add notes\n 1 file changed, 0 insertions(+), 0 deletions(-)\n create mode 100644 notes.txt\nOptional - check that the file is there:\nls\ndata  notes.txt  origin.txt  README.md  todo.txt  todo.txt~\nRemove the file with git rm and commit the removal:\ngit rm notes.txt\ngit commit -m \"These notes were made in error\"\n[main 058fd47] These notes were made in error\n 1 file changed, 0 insertions(+), 0 deletions(-)\n delete mode 100644 notes.txt\nOptional - check that the file is no longer there:\nls\ndata  origin.txt  README.md  todo.txt  todo.txt~"
  },
  {
    "objectID": "week04/w4_1_git.html#undoing-changes-that-have-not-been-committed",
    "href": "week04/w4_1_git.html#undoing-changes-that-have-not-been-committed",
    "title": "Getting started with Git",
    "section": "8 Undoing changes that have not been committed",
    "text": "8 Undoing changes that have not been committed\nHere, you’ll learn how to undo changes that have not been committed, like undoing an accidental file removal or overwrite. (In the optional self-study Git material, there is a section on undoing changes that have been committed.)\n\n8.1 Recovering a version from the repo\nWe’ll practice with undoing changes to your working dir (that have not been staged) by recovering a version from the repo: in other words, using Git as an “undo button” after accidental file changes or removal.\n\nLet’s say you accidentally overwrote instead of appended to a file:\necho \"Finish the taxidermy of the finches from Galapagos\" &gt; todo.txt\nAlways start by checking the status:\ngit status\nOn branch main\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n        modified:   todo.txt\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\nYou’ll want to “discard changes in working directory”, and Git told you how to do this — with git restore:\ngit restore todo.txt\n\n\nIf you accidentally deleted a file, you can similarly retrieve it with git checkout:\n\nAccidental removal of todo.txt\nrm todo.txt\nUse git restore to get the file back!\ngit restore todo.txt\n\n\n\n\n\n\n\n\nAlternative, older command used in the CSB book: git checkout (Click to expand)\n\n\n\n\n\nUntil recently, this action used to be done with with the git checkout command, for example:\ngit checkout -- README.md\ngit restore is a relatively new command designed to avoid confusion with the git checkout and git reset commands, which have multiple functions. The CSB book still uses the git checkout command for a similar example10.\n\n\n\n\n\n\n8.2 Unstaging a file\ngit restore can also unstage a file, which is most often needed when you added a file that was not supposed to be part of the next commit. For example:\n\nYou modify two files and use git add --all:\necho \"Variation under domestication\" &gt;&gt; origin.txt\necho \"Prepare for the next journey\" &gt;&gt; todo.txt\n\ngit add --all\nThen you realize that those two file changes should be part of separate commits. Again, check the status first:\ngit status\nOn branch main\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n        modified:   origin.txt\n        modified:   todo.txt\nAnd use git restore --staged as suggested by Git:\ngit restore --staged todo.txt\n\ngit status\nOn branch main\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n        modified:   origin.txt\n\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n        modified:   todo.txt\n\nNow, you can go ahead and add these changes to separate commits: see the exercise below.\n(Finally: in case you merely staged a file prematurely, you can just continue editing the file and re-add it.)\n\n\n\n\n\n\nAlternative, older command used in the CSB book: git reset (Click to expand)\n\n\n\n\n\nLike with discarding changes in the working dir, this action used to be done with another command, this time git reset. For example:\ngit reset HEAD README.md\nThe CSB book uses this but note that there is a mistake in the book: git reset will only unstage and not revert the file back to its state at the last commit. (git reset --hard does revert things back to the state of a desired commit, but only works on commits and not individual files.)\n\n\n\n\n\n Exercise: Commit the changes 1-by-1\n\nCommit the currently staged changes to origin.txt.\nStage and commit the changes to todo.txt.\n\n\n\nClick for the solution\n\n\nCommit the currently staged changes to origin.txt.\ngit commit -m \"Start writing about artificial selection\"\nStage and commit the changes to todo.txt.\ngit add todo.txt\ngit commit -m \"Update the TODO file\"\n\n\n\n\n\n\n\n\n\n\nUndoing staged changes\n\n\n\nWhat if you had made mistaken changes (like an accidental deletion) and also staged those changes? You can simply follow both of the two steps described above in order:\n\nFirst unstage the file with git restore --staged &lt;file&gt;.\nThen discard changes in the working dir with git restore &lt;file&gt;.\n\nFor instance, you overwrote the contents of the book and then staged the misshapen file:\necho \"Instincts of the Cuckoo\" &gt; origin.txt\ngit add origin.txt\n\ncat origin.txt\nInstincts of the Cuckoo\nYou can undo all of this as follows:\ngit restore --staged origin.txt\ngit restore origin.txt\n\ncat origin.txt\nOn the Origin of Species\nVariation under domestication"
  },
  {
    "objectID": "week04/w4_1_git.html#some-git-best-practices",
    "href": "week04/w4_1_git.html#some-git-best-practices",
    "title": "Getting started with Git",
    "section": "9 Some Git best-practices",
    "text": "9 Some Git best-practices\n\nWrite informative commit messages.\nImagine looking back at your project in a few months, after finding an error that you introduced a while ago.\n\nNot-so-good commit message: “Updated file”\nGood commit message: “In file x, updated function y to include z”\n\n\n\n\n\nImage source\n\n\n\n\n\n\n\n\nCommit messages for the truly committed\n\n\n\nIt is often argued that commit messages should preferably be in the form of completing the sentence “This commit will…”: When adhering to this, the above commit message would instead say “In file x, update function y to include z.”.\n\n\n\nCommit often, using small commits.\nThis will also help to keep commit messages informative!\nBefore committing, check what you’ve changed.\nUse git diff [--staged] or VS Code functionality.\nAvoid including unrelated changes in commits.\nSeparate commits if your working dir contains work from disparate edits: use git add + git commit separately for two sets of files.\nDon’t commit unnecessary files.\nThese can also lead to conflicts — especially automatically generated, temporary files.\n\n\n\n\n\n\n\n\nA more advanced tip: tags\n\n\n\nIf you have a repo with general scripts, which you continue to develop and use in multiple projects, and you publish a paper in which you use these scripts, it is a good idea to add a “tag” to a commit to mark the version of the scripts used in your analysis:\ngit tag -a v1.2.0 -m \"Clever release title\"\ngit push --follow-tags"
  },
  {
    "objectID": "week04/w4_1_git.html#footnotes",
    "href": "week04/w4_1_git.html#footnotes",
    "title": "Getting started with Git",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOthers include SVN and Mercurial.↩︎\nAnd if you’re writing software, all its source code.↩︎\nGit will just save an entirely new version whenever there’s been a change rather than tracking changes in individual lines.↩︎\nE.g., you’ll have to use Git for you final project.↩︎\n It is available by default, but that’s a very ancient version.↩︎\nWhen you need to provide Git with a “commit message” to Git and you haven’t entered one on the command line.↩︎\n You also get a hint on how to “unstage” the file: i.e., reverting what you just did with git add and leaving the file untracked once again↩︎\nOn the current “branch” – see the optional self-study page or CSB chapter 2.6 to learn about branches.↩︎\n This is because our VS Code working dir is not originspecies but two levels up from there. Typically, your VS Code working dir should be your project dir which would be the same as the repo dir.↩︎\n In that example, the CSB book example omits the dashes --. These indicate that the checkout command should operate on a file, but since the file name is provided too, this is not strictly necessary.↩︎"
  },
  {
    "objectID": "week10/w10_ga_nextflow.html",
    "href": "week10/w10_ga_nextflow.html",
    "title": "Graded Assignment V: Nextflow",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "week11/w11_overview.html",
    "href": "week11/w11_overview.html",
    "title": "Week 11: R Basics",
    "section": "",
    "text": "Back to top"
  }
]