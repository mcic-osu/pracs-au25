[
  {
    "objectID": "week03/w3_ua_github-account.html",
    "href": "week03/w3_ua_github-account.html",
    "title": "Assignment: Create a GitHub account",
    "section": "",
    "text": "What?\nCreate a personal GitHub account.\n\n\nWhy?\nGitHub is a website that hosts Git repositories, i.e. version-controlled projects. In Week 3 of this course, you will be learning how to use Git together with GitHub. In addition, you will submit your final project assignments through GitHub.\n\n\nHow?\nIf you already have a GitHub account, log in and start at step 6.\n\nGo to https://github.com.\nClick “Sign Up” in the top right.\nFollow the prompts to create your account — some notes:\n\nWhen choosing your username, I would recommend to keep it professional and have it include or resemble your real name. (This is because you will hopefully continue to use GitHub to share your code, for instance when publishing a paper.)\nYou can choose whether you want to use your OSU email address or a non-institutional email address. (And note that you can always associate a second email address with your account.)\n\nCheck your email and click the link to verify your email address.\nBack on GitHub website, now logged in: in the far top-right of the page, click your randomly assigned avatar, and in the dropdown menu, click “Settings”.\nIn the “Emails” tab (left-hand menu), deselect the box “Keep my email addresses private”.\nIn the “Public profile” tab, enter your name.\nStill in the “Profile tab”, upload a Profile picture. This can be a picture of yourself but if you prefer, you can use something else.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "week02/removed.html",
    "href": "week02/removed.html",
    "title": "Removed",
    "section": "",
    "text": "Because spaces are special characters used to separate commands from options and arguments, etc., using them in file names is inconvenient at best:\n# You should be in /fs/ess/PAS2700/users/$USER/CSB/unix/sandbox\nls\n\ncd Papers and reviews     # NOPE!\n\ncd Papers\\ and\\ reviews   # \\ to escape each individual space\ncd \"Papers and reviews\"   # Quotes to escape special characters\nWe’ll talk more about spaces and file names in week 2.\n\n\n\nUse uniq -c to count occurrences of each unique value (more on this in week 4):\ncut -d \";\" -f 2 Pacifici2013_data.csv | tail -n +2 | sort | uniq -c\n   54 Afrosoricida\n  280 Carnivora\n  325 Cetartiodactyla\n 1144 Chiroptera\n   21 Cingulata\ncut -d \";\" -f 2 Pacifici2013_data.csv | tail -n +2 | sort | uniq -c | sort -nr\n 2220 Rodentia\n 1144 Chiroptera\n  442 Eulipotyphla\n  418 Primates\n\n\n\n\nLet’s say we want a list of animals sorted by body weight…\ncd ../sandbox/\ntail -n +2 ../data/Pacifici2013_data.csv\nIn the following commands, we will build up our “pipeline”.\nFirst, we print the file with the exception of the first line (tail -n +2) and then pipe that into cut to select the columns of interest — and to check our partial pipeline, end with a head command to only print the first lines:\n# Using head just to check the output\ntail -n +2 ../data/Pacifici2013_data.csv |\n    cut -d \";\" -f 5-6 | head\nSecond, we’ll add a tr command to change the column delimiter:\ntail -n +2 ../data/Pacifici2013_data.csv |\n    cut -d \";\" -f 5-6 | tr \";\" \" \" | head\nFinally, we’ll sort in reverse numerical order with sort, and redirect the output to a file:\ntail -n +2 ../data/Pacifici2013_data.csv |\n    cut -d \";\" -f 5-6 | tr \";\" \" \" | sort -r -n -k 3 &gt; BodyM.csv\nLet’s take a look at the output:\nhead BodyM.csv"
  },
  {
    "objectID": "week02/removed.html#bonus-material",
    "href": "week02/removed.html#bonus-material",
    "title": "Removed",
    "section": "",
    "text": "Because spaces are special characters used to separate commands from options and arguments, etc., using them in file names is inconvenient at best:\n# You should be in /fs/ess/PAS2700/users/$USER/CSB/unix/sandbox\nls\n\ncd Papers and reviews     # NOPE!\n\ncd Papers\\ and\\ reviews   # \\ to escape each individual space\ncd \"Papers and reviews\"   # Quotes to escape special characters\nWe’ll talk more about spaces and file names in week 2.\n\n\n\nUse uniq -c to count occurrences of each unique value (more on this in week 4):\ncut -d \";\" -f 2 Pacifici2013_data.csv | tail -n +2 | sort | uniq -c\n   54 Afrosoricida\n  280 Carnivora\n  325 Cetartiodactyla\n 1144 Chiroptera\n   21 Cingulata\ncut -d \";\" -f 2 Pacifici2013_data.csv | tail -n +2 | sort | uniq -c | sort -nr\n 2220 Rodentia\n 1144 Chiroptera\n  442 Eulipotyphla\n  418 Primates\n\n\n\n\nLet’s say we want a list of animals sorted by body weight…\ncd ../sandbox/\ntail -n +2 ../data/Pacifici2013_data.csv\nIn the following commands, we will build up our “pipeline”.\nFirst, we print the file with the exception of the first line (tail -n +2) and then pipe that into cut to select the columns of interest — and to check our partial pipeline, end with a head command to only print the first lines:\n# Using head just to check the output\ntail -n +2 ../data/Pacifici2013_data.csv |\n    cut -d \";\" -f 5-6 | head\nSecond, we’ll add a tr command to change the column delimiter:\ntail -n +2 ../data/Pacifici2013_data.csv |\n    cut -d \";\" -f 5-6 | tr \";\" \" \" | head\nFinally, we’ll sort in reverse numerical order with sort, and redirect the output to a file:\ntail -n +2 ../data/Pacifici2013_data.csv |\n    cut -d \";\" -f 5-6 | tr \";\" \" \" | sort -r -n -k 3 &gt; BodyM.csv\nLet’s take a look at the output:\nhead BodyM.csv"
  },
  {
    "objectID": "week02/w2_overview.html#links",
    "href": "week02/w2_overview.html#links",
    "title": "Week 2: Unix Shell Basics",
    "section": "1 Links",
    "text": "1 Links\n\nLecture pages\n\nTue: Shell basics part I\nThu: Shell basics part I\n\n\n\nExercises\n\nWeek 2 exercises"
  },
  {
    "objectID": "week02/w2_overview.html#content-overview",
    "href": "week02/w2_overview.html#content-overview",
    "title": "Week 2: Unix Shell Basics",
    "section": "2 Content overview",
    "text": "2 Content overview\nThis week, we will focus on the basics of the Unix shell, and you will learn:\n\nWhy using a command-line interface can be beneficial.\nWhat the Unix shell is and what you can do with it.\nUsing Unix commands, how to:\n\nNavigate around your computer.\nCreate and manage directories and files.\nView text files.\nSearch within, manipulate, and extract information from text files."
  },
  {
    "objectID": "week02/w2_overview.html#readings",
    "href": "week02/w2_overview.html#readings",
    "title": "Week 2: Unix Shell Basics",
    "section": "3 Readings",
    "text": "3 Readings\n\nRequired readings\n\nCSB Chapter 1: “Unix”\nBuffalo Preface and Chapter 1: “How to Learn Bioinformatics”"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "pracs-au25",
    "section": "",
    "text": "Practical Computing Skills for Omics Data  Ohio State PLNTPTH 5004 Autumn 2025\n\n\n\n\nWeek\nTue\nThu\nHomework\n\n\n\n\n1\n08/26: Course intro & omics data\n08/28: OSC Intro\n   \n\n\n2\n09/02: Unix shell basics I\n09/04: Unix shell basics II\n   \n\n\n3\n09/09: Project file organization & Markdown\n09/11: Managing files in the shell\n   \n\n\n4\n09/16: Version control with Git\n09/18: Git remotes on GitHub\n   \n\n\n5\n09/23: Shell scripting\n09/25: Running CLI tools with shell scripts\n   \n\n\n6\n09/30: More on OSC & data management\n10/02: Software at OSC\n      \n\n\n7\n10/07: OSC Slurm batch jobs I\n10/09: OSC Slurm batch jobs II\n   \n\n\n8\n10/14: Manual versus automated pipelines\n\n   \n\n\n9\n10/21: Building Nextflow pipelines I\n10/23: Building Nextflow pipelines II\n   \n\n\n10\n10/28: Building Nextflow pipelines III\n10/30: Building Nextflow pipelines IV\n   \n\n\n11\n11/04: R - basics\n11/06: R - data wrangling\n   \n\n\n12\n\n11/13: R - data visualization\n   \n\n\n13\n11/18: R - Omics data I\n11/20: R - Omics data II\n   \n\n\n14\n11/25: R - Quarto\n\n   \n\n\n15\n12/02: Nextflow nf-core pipelines\n12/04: Nextflow nf-core pipelines\n   \n\n\n16\n12/09: Student presentations\n\n\n\n\n\n\nHomework column:  = Week overview;  = Exercises;  = Assignments\n\n\n\n\n Back to top"
  },
  {
    "objectID": "week01/w1_01_course-intro.html#introductions-jelmer-instructor",
    "href": "week01/w1_01_course-intro.html#introductions-jelmer-instructor",
    "title": "Course Intro",
    "section": "Introductions: Jelmer (instructor)",
    "text": "Introductions: Jelmer (instructor)\n\nLead of the CFAES Bioinformatics and Microscopy cores\n\nPart of what was until recently called the Molecular & Cellular Imaging Center (MCIC)\nWe are now grouped under CFAES Analytical Resources, core facilities providing services in molecular biology, high-throughput sequencing, bioinformatics, microscopy, and soil analyses.\n\n\n\n\n\nWhat I work on\n\nThe majority of my time is spent providing research assistance,\nworking with grad students and postdocs on omics data\nTeaching, such as this course, workshops, Code Club (https://osu-codeclub.github.io)\n\n\n\n\n\n\nBackground in animal evolutionary genomics & speciation\nIn my free time, I enjoy bird watching – locally & all across the world"
  },
  {
    "objectID": "week01/w1_01_course-intro.html#introductions-ta-co-instructor",
    "href": "week01/w1_01_course-intro.html#introductions-ta-co-instructor",
    "title": "Course Intro",
    "section": "Introductions: TA / co-instructor",
    "text": "Introductions: TA / co-instructor\nTBA"
  },
  {
    "objectID": "week01/w1_01_course-intro.html#introductions-you",
    "href": "week01/w1_01_course-intro.html#introductions-you",
    "title": "Course Intro",
    "section": "Introductions: You",
    "text": "Introductions: You\n\nName\nLab and Department\nResearch interests and/or current research topics\nSomething about you that is not work-related, such as a hobby or fun fact"
  },
  {
    "objectID": "week01/w1_01_course-intro.html#the-core-goals-of-this-course",
    "href": "week01/w1_01_course-intro.html#the-core-goals-of-this-course",
    "title": "Course Intro",
    "section": "The core goals of this course",
    "text": "The core goals of this course\nLearning skills to:\n\nDo your research more reproducibly and efficiently (e.g. by using code)\nWork with large-scale “omics” datasets\n\n\nTBD: explain focus on fundamental computation skills"
  },
  {
    "objectID": "week01/w1_01_course-intro.html#course-background-reproducibility",
    "href": "week01/w1_01_course-intro.html#course-background-reproducibility",
    "title": "Course Intro",
    "section": "Course background: Reproducibility",
    "text": "Course background: Reproducibility\nTwo related ideas:\n\nGetting same results with an independent experiment (replicable)\nGetting same results given the same data (reproducible)\n\n\nOur focus is on #2."
  },
  {
    "objectID": "week01/w1_01_course-intro.html#course-background-reproducibility-cont.",
    "href": "week01/w1_01_course-intro.html#course-background-reproducibility-cont.",
    "title": "Course Intro",
    "section": "Course background: Reproducibility (cont.)",
    "text": "Course background: Reproducibility (cont.)\n\n“The most basic principle for reproducible research is: Do everything via code.”\n—Karl Broman\n\n\nAdditionally, also important for reproducibility are:\n\nProject organization and documentation (week 3)\nSharing your data and code (for code: Git & GitHub, week 4)\nHow you code (covered throughout)\n\n\n\n\n\n\n\n\nAnother motivator: working reproducibly will benefit future you!"
  },
  {
    "objectID": "week01/w1_01_course-intro.html#course-background-efficiency-and-automation",
    "href": "week01/w1_01_course-intro.html#course-background-efficiency-and-automation",
    "title": "Course Intro",
    "section": "Course background: Efficiency and automation",
    "text": "Course background: Efficiency and automation\n\nUsing code enables you to work more efficiently and automatically —\nparticularly useful when having to:\n\nDo repetitive tasks\nRecreate a figure or redo an analysis after adding a sample\nRedo a project after uncovering a mistake in the first data processing step."
  },
  {
    "objectID": "week01/w1_01_course-intro.html#course-background-omics-data",
    "href": "week01/w1_01_course-intro.html#course-background-omics-data",
    "title": "Course Intro",
    "section": "Course background: Omics data",
    "text": "Course background: Omics data\n\nOmics data is increasingly important in biology, and most notably includes:\n\nGenomics\nTranscriptomics\nProteomics\nMetabolomics\n\n\n\n\n\n\n\n\nThe next lecture will introduce omics data in a bit more details.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat this course does and does not focus on\n\n\n\nWhile we’ll be using some example omics datasets, this course will not comprehensively cover the analysis of omics data — our focus is more on fundamental computational skills.\nA highly recommended follow-up course to learn omics data analysis specifics:\nGenome Analytics (HCS 7004) by Jonathan Fresnedo-Ramirez"
  },
  {
    "objectID": "week01/w1_01_course-intro.html#what-is-bioinformatics",
    "href": "week01/w1_01_course-intro.html#what-is-bioinformatics",
    "title": "Course Intro",
    "section": "What is bioinformatics?",
    "text": "What is bioinformatics?\nAlso: computational biology\nTBA"
  },
  {
    "objectID": "week01/w1_01_course-intro.html#the-unix-shell-shell-scripts",
    "href": "week01/w1_01_course-intro.html#the-unix-shell-shell-scripts",
    "title": "Course Intro",
    "section": "The Unix shell & shell scripts",
    "text": "The Unix shell & shell scripts\nBeing able to work in the Unix shell is a fundamental skill in computational biology.\n\n\nYou’ll spend a lot of time with the Unix shell, starting this week, and including in weeks that aren’t solely focused on the shell.\nWe’ll also write shell scripts, and will use an editor called VS Code for this and other purposes.\n\n\n\n\n\n\n\n\n\n\nBash (shell language)\n\n\n\n\n\n\n\n\nVS Code"
  },
  {
    "objectID": "week01/w1_01_course-intro.html#project-organization-documentation",
    "href": "week01/w1_01_course-intro.html#project-organization-documentation",
    "title": "Course Intro",
    "section": "Project organization & documentation",
    "text": "Project organization & documentation\nGood project organization & documentation is a necessary starting point for reproducible research.\n\n\nYou’ll learn best practices for project organization, file naming, etc.\nYou’ll learn how to manage your data and software\nTo document and report what you are doing, you’ll use Markdown files.\n\n\n\n\n\n\n\n\nMarkdown"
  },
  {
    "objectID": "week01/w1_01_course-intro.html#version-control-with-git-and-github",
    "href": "week01/w1_01_course-intro.html#version-control-with-git-and-github",
    "title": "Course Intro",
    "section": "Version control with Git and GitHub",
    "text": "Version control with Git and GitHub\nUsing version control, you can more effectively keep track of project progress, collaborate, share code, revisit earlier versions, and undo.\n\n\nGit is the version control software we will use,\nand GitHub is the website that hosts Git projects (repositories).\nYou’ll also use Git + GitHub to hand in your graded assignments."
  },
  {
    "objectID": "week01/w1_01_course-intro.html#high-performance-computing-with-osc",
    "href": "week01/w1_01_course-intro.html#high-performance-computing-with-osc",
    "title": "Course Intro",
    "section": "High-performance computing with OSC",
    "text": "High-performance computing with OSC\nThanks to supercomputer resources, you can work with very large datasets at speed — running up to 100s of analyses in parallel, and using much larger amounts of memory and storage space than a personal computer has.\n\n\n\nWe will use OSC throughout the course, and you’ll get a brief intro to it today.\nIn week 5, you’ll learn how to manage data and software at OSC.\nIn week 6, we’ll learn to submit shell scripts as OSC “batch jobs” with Slurm."
  },
  {
    "objectID": "week01/w1_01_course-intro.html#automated-workflow-management",
    "href": "week01/w1_01_course-intro.html#automated-workflow-management",
    "title": "Course Intro",
    "section": "Automated workflow management",
    "text": "Automated workflow management\nUsing a workflow written with a workflow manager, you can run and rerun an entire analysis pipeline with a single command, and easily change and rerun parts of it, too.\n\n\nWe’ll use the workflow language Nextflow to build our pipelines\nYou will also learn how to use comprehensive, best-practice omics data Nextflow pipelines produced by the nf-core initiative"
  },
  {
    "objectID": "week01/w1_01_course-intro.html#r",
    "href": "week01/w1_01_course-intro.html#r",
    "title": "Course Intro",
    "section": "R",
    "text": "R\n\nDETAILS TBA\nAddress R vs Python"
  },
  {
    "objectID": "week01/w1_01_course-intro.html#zoom",
    "href": "week01/w1_01_course-intro.html#zoom",
    "title": "Course Intro",
    "section": "Zoom",
    "text": "Zoom\n\nBe muted by default, but feel free to unmute yourself to ask questions any time.\nQuestions can also be asked in the chat.\nHaving your camera turned on as much as possible is appreciated!\n\n\n\n“Screen real estate” — large/multiple monitors or multiple devices best.\nBe ready to share your screen."
  },
  {
    "objectID": "week01/w1_01_course-intro.html#websites-books",
    "href": "week01/w1_01_course-intro.html#websites-books",
    "title": "Course Intro",
    "section": "Websites & Books",
    "text": "Websites & Books\n\nInfo about CarmenCanvas website…\n\n\n\nThe GitHub website contains:\n\nOverviews of each week & readings\nSlide decks and lecture pages\nExercises\nFinal project assignment information\n\n\n\n\n\nBooks:\n\nComputing Skills for Biologists (“CSB”; Allesina & Wilmes 2019)\nBioinformatics Data Skills (“Buffalo”; Buffalo 2015)"
  },
  {
    "objectID": "week01/w1_01_course-intro.html#what-your-grade-is-made-up-of",
    "href": "week01/w1_01_course-intro.html#what-your-grade-is-made-up-of",
    "title": "Course Intro",
    "section": "What your grade is made up of",
    "text": "What your grade is made up of\nYou can earn a total of 100 points across 6 assignments and 4 final project checkpoints."
  },
  {
    "objectID": "week01/w1_01_course-intro.html#graded-assignments",
    "href": "week01/w1_01_course-intro.html#graded-assignments",
    "title": "Course Intro",
    "section": "Graded assignments",
    "text": "Graded assignments\nThese are due on Mondays and are worth 10 points each:\n\nShell basics (week 3)\nMarkdown & Git (week 5)\nShell scripting (week 6)\nOSC batch jobs (week 8)\nNextflow (week 11)\nR (week 14)"
  },
  {
    "objectID": "week01/w1_01_course-intro.html#final-project",
    "href": "week01/w1_01_course-intro.html#final-project",
    "title": "Course Intro",
    "section": "Final project",
    "text": "Final project\nPlan and implement a small computational project, with the following checkpoints:\n\nI: Proposal (due week 13 – 5 points)\nII: Draft (due week 15 – 5 points)\nIII: Oral presentations on Zoom (week 16 – 10 points)\nIV: Final submission (due Dec 15 – 20 points)\n\n\n\n\n\n\n\n\n\nData sets for the final project\n\n\nIf you have your own data set & analysis ideas, that is ideal. If not, I can provide you with this.\nMore information about the final project will follow later in the course."
  },
  {
    "objectID": "week01/w1_01_course-intro.html#ungraded-homework",
    "href": "week01/w1_01_course-intro.html#ungraded-homework",
    "title": "Course Intro",
    "section": "Ungraded homework",
    "text": "Ungraded homework\n\nWeekly readings — somewhat up to you when to do these, ideally before and after the lectures!\nWeekly exercises — I recommend doing these on Fridays\nMiscellaneous small assignments such as surveys and account setup.\n\n\n\n\n\n\n\n\nWeekly materials & homework\n\n\nI will try add the materials for each week on the preceding Friday — at the least the week’s overview and readings.\nNone of this homework had to be handed in."
  },
  {
    "objectID": "week01/w1_01_course-intro.html#weekly-recitation-on-monday",
    "href": "week01/w1_01_course-intro.html#weekly-recitation-on-monday",
    "title": "Course Intro",
    "section": "Weekly recitation on Monday",
    "text": "Weekly recitation on Monday\nWe will have an optional but highly recommended weekly recitation meeting on Monday in which we go through the exercises for the preceding week.\n\n\n\n\n\n\n\nPractice is key!\n\n\nThis course is intended to be highly practical and if you don’t practice the skills we will focus on by yourself, you will not get much out of it.\n\n\n\n\nPlease indicate your availability here: TBA"
  },
  {
    "objectID": "week01/w1_01_course-intro.html#rest-of-this-week",
    "href": "week01/w1_01_course-intro.html#rest-of-this-week",
    "title": "Course Intro",
    "section": "Rest of this week",
    "text": "Rest of this week\n\nIntroduction to omics data\n\n\n\nIntroduction to the Ohio Supercomputer Center (OSC)\n\n\n\nHomework:\n\nTBA"
  },
  {
    "objectID": "week01/w1_02_omics-data.html#the-main-omics-data-types",
    "href": "week01/w1_02_omics-data.html#the-main-omics-data-types",
    "title": "Omics data",
    "section": "The main omics data types",
    "text": "The main omics data types\n\nGenomics (including metagenomics, epigenomics, etc.)\nTranscriptomics (including translatomics)\nProteomics\nMetabolomics\n\nBoth genomics and transcriptomics data is produced by high-throughput sequencing technologies.\nThat will be the focus of this lecture and will be used in examples throughout the course."
  },
  {
    "objectID": "week01/w1_02_omics-data.html#what-does-sequencing-refer-to",
    "href": "week01/w1_02_omics-data.html#what-does-sequencing-refer-to",
    "title": "Omics data",
    "section": "What does sequencing refer to?",
    "text": "What does sequencing refer to?\nThe shorthand sequencing, like in “high-throughput sequencing”, generally refers to determining the nucleotide sequence of fragments of DNA.\n\n\n\n\n\n\n\n\nWhat about RNA or proteins?\n\n\n\nRNA is usually reverse transcribed to DNA (cDNA) prior to sequencing, as in nearly all “RNA-seq”.\nDirect RNA sequencing is possible with one of the sequencing technologies we’ll discuss, but this is under development and not yet widely used.\n\n\n\nProtein sequencing requires different technology altogether, such as mass spectrometry, and is not further discussed in this lecture."
  },
  {
    "objectID": "week01/w1_02_omics-data.html#sequencing-technologies-overview",
    "href": "week01/w1_02_omics-data.html#sequencing-technologies-overview",
    "title": "Omics data",
    "section": "Sequencing technologies: overview",
    "text": "Sequencing technologies: overview\nSanger sequencing (since 1977)\nSequences a single, typically PCR-amplified, short-ish (≤900 bp) DNA fragment at a time\n\n\nHigh-throughput sequencing (HTS, since 2005)\nSequences 105-109s, usually randomly selected, DNA fragments (“reads”) at a time"
  },
  {
    "objectID": "week01/w1_02_omics-data.html#sequencing-technology-development-timeline",
    "href": "week01/w1_02_omics-data.html#sequencing-technology-development-timeline",
    "title": "Omics data",
    "section": "Sequencing technology development timeline",
    "text": "Sequencing technology development timeline\n\n\nModified after Pereira et al. 2020"
  },
  {
    "objectID": "week01/w1_02_omics-data.html#sequencing-technology-development-timeline-1",
    "href": "week01/w1_02_omics-data.html#sequencing-technology-development-timeline-1",
    "title": "Omics data",
    "section": "Sequencing technology development timeline",
    "text": "Sequencing technology development timeline\n\n\nModified after Pereira et al. 2020"
  },
  {
    "objectID": "week01/w1_02_omics-data.html#sequencing-cost-through-time",
    "href": "week01/w1_02_omics-data.html#sequencing-cost-through-time",
    "title": "Omics data",
    "section": "Sequencing cost through time",
    "text": "Sequencing cost through time\n\nhttps://www.genome.gov/about-genomics/fact-sheets/Sequencing-Human-Genome-cost"
  },
  {
    "objectID": "week01/w1_02_omics-data.html#hts-applications",
    "href": "week01/w1_02_omics-data.html#hts-applications",
    "title": "Omics data",
    "section": "HTS applications",
    "text": "HTS applications\n\nWhole-genome assembly\n\n\n\nVariant analysis (for population genetics/genomics, molecular evolution, GWAS, etc.):\n\nWhole-genome “resequencing”\nReduced-representation libraries (e.g. RADseq, GBS)\n\n\n\n\n\nRNA-seq (transcriptome analysis)\n\n\n\n\nOther functional sequencing methods like methylation sequencing, ChIP-seq, etc.\n\n\n\n\nMicrobial community characterization\n\nMetabarcoding\nShotgun metagenomics"
  },
  {
    "objectID": "week01/w1_02_omics-data.html#the-main-hts-types",
    "href": "week01/w1_02_omics-data.html#the-main-hts-types",
    "title": "Omics data",
    "section": "The main HTS types",
    "text": "The main HTS types\n\n\nShort-read HTS\n\nAKA Next-Generation Sequencing (NGS)\nProduces up to billions of 50-300 bp reads\nMarket dominated by Illumina\nSince 2005 — technology fairly stable\n\n\nLong-read HTS\n\nReads much longer than in NGS but fewer, less accurate, and more costly per base\nTwo main companies: Oxford Nanopore Technologies (ONT) and Pacific Biosciences (PacBio)\nSince 2011 — remains under rapid development\n\n\n\n\n\n\n\n\n\n\nShort videos explaining the technology (90 s - 5 m each)\n\n\n\nNanopore: https://www.youtube.com/watch?v=RcP85JHLmnI\nIllumina: https://www.youtube.com/watch?v=fCd6B5HRaZ8\nPacBio: https://www.youtube.com/watch?v=_lD8JyAbwEo"
  },
  {
    "objectID": "week01/w1_02_omics-data.html#read-lengths",
    "href": "week01/w1_02_omics-data.html#read-lengths",
    "title": "Omics data",
    "section": "Read lengths",
    "text": "Read lengths\n\nShort-read (Illumina) HTS: 50-300 bp reads\nLong-read HTS: longer & more variable read lengths (PacBio: 10-50 kbp, ONT: 10-100+ kbp)\n\n\n\n\n\nWhen are longer reads useful?\n\n\nGenome assembly\nHaplotype and large structural variant calling\nTranscript isoform identification\nTaxonomic identification of single reads (microbial metabarcoding)\n\n\n\n\n\n\n\nWhen does read length not matter (as much)?\n\n\nSNP variant analysis\nRead-as-a-tag: the goal is just to know a read’s origin in a reference genome, like in counting applications such as RNA-seq"
  },
  {
    "objectID": "week01/w1_02_omics-data.html#error-rates",
    "href": "week01/w1_02_omics-data.html#error-rates",
    "title": "Omics data",
    "section": "Error rates",
    "text": "Error rates\nCurrently, no sequencing technology is error-free.\n\nIllumina error rates are mostly below 0.1%\nTBA\n\n\n\n\n\n\n\nError rates are changing\n\n\nError rates in one recent type of PacBio sequencing where individual fragments are sequenced multiple times (“HiFi”) are now lower than in Illumina.\nError rates of ONT sequencing are also continuously decreasing.\n\n\n\n\n\n\n\n\n\n\n\nQuality scores in sequence data\n\n\nWhen you get sequences from a high-throughput sequencer, base calls have typically already been made. Every base is also accompanied by a quality score (inversely related to the estimated error probability). We’ll talk about those in some more detail in a bit."
  },
  {
    "objectID": "week01/w1_02_omics-data.html#overcoming-sequencing-errors",
    "href": "week01/w1_02_omics-data.html#overcoming-sequencing-errors",
    "title": "Omics data",
    "section": "Overcoming sequencing errors",
    "text": "Overcoming sequencing errors\nSequencing every bases multiple times, i.e. having a &gt;1x so-called “depth of coverage” allows to infer the correct sequence:\n\n\n\n\nOvercoming sequencing errors is made more challenging by natural genetic variation among and within individuals.\nTypical depths of coverage: at least 50-100x for genome assembly; 10-30x for resequencing."
  },
  {
    "objectID": "week01/w1_02_omics-data.html#libraries-and-library-prep",
    "href": "week01/w1_02_omics-data.html#libraries-and-library-prep",
    "title": "Omics data",
    "section": "Libraries and library prep",
    "text": "Libraries and library prep\nIn a sequencing context, a “library” is a collection of nucleic acid fragments ready for sequencing.\n\nIn Illumina and other HTS libraries, these fragments number in the millions or billions and are often randomly generated from input such as genomic DNA:\n\n\n\n\n\n\n\n\n\nThis procedure is called library prep, and is typically done for you by a sequencing facility or company."
  },
  {
    "objectID": "week01/w1_02_omics-data.html#libraries-and-library-prep-cont.",
    "href": "week01/w1_02_omics-data.html#libraries-and-library-prep-cont.",
    "title": "Omics data",
    "section": "Libraries and library prep (cont.)",
    "text": "Libraries and library prep (cont.)\nAfter library prep (here, for Illumina sequencing), each DNA fragment is flanked by several types of short sequences that together make up the “adapters”:"
  },
  {
    "objectID": "week01/w1_02_omics-data.html#paired-end-vs.-single-end-sequencing",
    "href": "week01/w1_02_omics-data.html#paired-end-vs.-single-end-sequencing",
    "title": "Omics data",
    "section": "Paired-end vs. single-end sequencing",
    "text": "Paired-end vs. single-end sequencing\nIn Illumina sequencing, DNA fragments can be sequenced from both ends as shown below — this is called “paired-end” (PE) sequencing:\n\n\n\n\n\n\n\nWhen sequencing is instead single-end (SE), no reverse read is produced:"
  },
  {
    "objectID": "week01/w1_02_omics-data.html#insert-size",
    "href": "week01/w1_02_omics-data.html#insert-size",
    "title": "Omics data",
    "section": "Insert size",
    "text": "Insert size\nThe total size of the biological DNA fragment (without adapters) is often called the insert size:"
  },
  {
    "objectID": "week01/w1_02_omics-data.html#insert-size-variation",
    "href": "week01/w1_02_omics-data.html#insert-size-variation",
    "title": "Omics data",
    "section": "Insert size variation",
    "text": "Insert size variation\nInsert size varies — because the library prep protocol can aim for various sizes, and because of variation due to limited precision in size selection. In some cases, the insert size can be:\n\nShorter than the combined read length, leading to overlapping reads (this can be useful):\n\n\n\n\n\n\n\n\nShorter than the single read length, leading to “adapter read-through” (i.e., the ends of the resulting reads will consist of adapter sequence, which should be removed):\n\n\n\n\n\n\n\n\n\n\n\n\nMultiplexing!\n\n\nUsing the indices/barcodes in adapters, up to 96 samples can be multiplexed into a single library."
  },
  {
    "objectID": "week01/w1_02_omics-data.html#genomes",
    "href": "week01/w1_02_omics-data.html#genomes",
    "title": "Omics data",
    "section": "Genomes",
    "text": "Genomes\nMost HTS applications either require a “reference genome” or involve its production.\n\n\nWhat exactly does “reference genome” refer to? We’ll discuss three components to this phrase:\n\nAssembly\nIt includes a representation of most of the genome DNA sequence: the genome assembly\n\n\n\n\nAnnotation\nIt (preferably) includes an “annotation” that provides the locations of genes and other genomic features, as well as functional information on these features\n\n\n\n\nTaxonomic identity\nTypically considered at the species level, so then it should involve the focal species. But:\n\nIf needed, it is often possible to work with reference genomes of closely related species\nConversely, multiple reference genomes may exist, e.g. for different subspecies"
  },
  {
    "objectID": "week01/w1_02_omics-data.html#genome-size-variation",
    "href": "week01/w1_02_omics-data.html#genome-size-variation",
    "title": "Omics data",
    "section": "Genome size variation",
    "text": "Genome size variation\n\n\n\n\n\n\n\nhttps://en.wikipedia.org/wiki/Genome_size"
  },
  {
    "objectID": "week01/w1_02_omics-data.html#growth-of-genome-databases",
    "href": "week01/w1_02_omics-data.html#growth-of-genome-databases",
    "title": "Omics data",
    "section": "Growth of genome databases",
    "text": "Growth of genome databases\n\nKonkel & Slot 2023"
  },
  {
    "objectID": "week01/w1_02_omics-data.html#genome-assemblies",
    "href": "week01/w1_02_omics-data.html#genome-assemblies",
    "title": "Omics data",
    "section": "Genome assemblies",
    "text": "Genome assemblies\n\n\nWith increasing usage & quality of long-read HTS, we are generating better assemblies\nFor chromosome-level assemblies, i.e. with one contiguous sequence for each chromosome, additional technologies than sequencing are often needed (e.g. Hi-C, optical mapping)\nMany assemblies are not “chromosome-level”, but consist of –often 1000s of– contigs and scaffolds.\nEven chromosome-level assemblies are not 100% complete\n\n\n\n\n\n\nQuestion: Contigs vs. scaffolds?\n\nContigs are contiguous, known stretches of DNA created by the assembly process, basically by overlapping reads.\nOften, the order and orientation of two or more contigs is known, but there is a gap of unknown size between them. Such contigs are connected into scaffolds with a stretch of Ns in between."
  },
  {
    "objectID": "week01/w1_02_omics-data.html#overview",
    "href": "week01/w1_02_omics-data.html#overview",
    "title": "Omics data",
    "section": "Overview",
    "text": "Overview\nAll common genetic/genomic data files are plain-text, meaning that they can be opened by any text editor. However, they are often compressed to save space. The main types are:\n\nFASTA\nSimple sequence files, where each entry contains a header and a DNA/AA sequence.\nVersatile, anything from a genome assemblies, proteomes, and single sequence fragments to alignments can be in this format.\n\n\n\n\nFASTQ\nThe standard format for HTS reads — contains a quality score for each nucleotide.\nSAM/BAM\nAn alignment format for HTS reads.\n\n\n\n\n\nGTF/GFF\nTables (tab-delimited) with information such as genomic coordinates on “genomic features” such as genes and exons. The files contain reference genome annotations."
  },
  {
    "objectID": "week01/w1_02_omics-data.html#fasta-files",
    "href": "week01/w1_02_omics-data.html#fasta-files",
    "title": "Omics data",
    "section": "FASTA files",
    "text": "FASTA files\nFASTA files contain one or more (sometimes called multi-FASTA) DNA or amino acid sequences, with no limits on the number of sequences or the sequence lengths.\n\n\nAs mentioned, they are versatile, and are the standard format for:\n\nGenome assembly sequences\nTranscriptomes and proteomes\nSequence downloads from NCBI such as a single gene/protein or other GenBank entry\nSequence alignments (but not from HTS reads)"
  },
  {
    "objectID": "week01/w1_02_omics-data.html#fasta-files-cont.",
    "href": "week01/w1_02_omics-data.html#fasta-files-cont.",
    "title": "Omics data",
    "section": "FASTA files (cont.)",
    "text": "FASTA files (cont.)\nThe following example FASTA file contains two entries:\n&gt;unique_sequence_ID Optional description\nATTCATTAAAGCAGTTTATTGGCTTAATGTACATCAGTGAAATCATAAATGCTAAAAA\n&gt;unique_sequence_ID2\nATTCATTAAAGCAGTTTATTGGCTTAATGTACATCAGTGAAATCATAAATGCTAAATG\n\n\nEach entry contains a header and the sequence itself, and:\n\nHeader lines start with a &gt; and are otherwise “free form” but usually provide an identifier (and sometimes metadata) for the sequence\nThe sequence can be spread across multiple lines with a fixed width\n\n\n\n\n\n\n\n\n\n\nFASTA file name extensions are variable:\n\n\n\nGeneric extensions are .fasta and .fa\nOther extensions explicitly indicate whether sequences are nucleotide (.fna) or amino acids (.faa)"
  },
  {
    "objectID": "week01/w1_02_omics-data.html#fastq",
    "href": "week01/w1_02_omics-data.html#fastq",
    "title": "Omics data",
    "section": "FASTQ",
    "text": "FASTQ\nFASTQ is the standard format for HTS reads.\nEach read forms one FASTQ entry and is represented by four lines, which contain, respectively:\n\n\nA header that starts with @ and e.g. uniquely identifies the read\nThe sequence itself\nA + (plus sign)\nOne-character quality scores for each base (hence FASTQ as in “Q” for “quality”)"
  },
  {
    "objectID": "week01/w1_02_omics-data.html#fastq-quality-scores",
    "href": "week01/w1_02_omics-data.html#fastq-quality-scores",
    "title": "Omics data",
    "section": "FASTQ quality scores",
    "text": "FASTQ quality scores\nThe quality scores we saw in the read on the previous slide represent an estimate of the error probability of the base call.\nSpecifically, they correspond to a numeric “Phred” quality score (Q), which is a function of the estimated probability that a base call is erroneous (P):\n\nQ = -10 * log10(P)\n\n\n\nFor some specific probabilities and their rough qualitative interpretation for Illumina data:\n\n\n\nPhred quality score\nError probability\nRough interpretation\n\n\n\n\n10\n1 in 10\nterrible\n\n\n20\n1 in 100\nbad\n\n\n30\n1 in 1,000\ngood\n\n\n40\n1 in 10,000\nexcellent"
  },
  {
    "objectID": "week01/w1_02_omics-data.html#fastq-quality-scores-cont.",
    "href": "week01/w1_02_omics-data.html#fastq-quality-scores-cont.",
    "title": "Omics data",
    "section": "FASTQ quality scores (cont.)",
    "text": "FASTQ quality scores (cont.)\nThis numeric quality score is represented in FASTQ files not by the number itself, but by a corresponding “ASCII character”.\nThis allows for a single-character representation of each possible score — as a consequence, each quality score character can conveniently correspond to (& line up with) a base character in the read.\n\n\n\n\nPhred quality score\nError probability\nASCII character\n\n\n\n\n10\n1 in 10\n+\n\n\n20\n1 in 100\n5\n\n\n30\n1 in 1,000\n?\n\n\n40\n1 in 10,000\nI\n\n\n\n\n\n\n\n\n\nA rule of thumb\n\n\nIn practice, you almost never have to manually check the quality scores of bases in FASTQ files, but if you do, a rule of thumb is that letter characters are good (Phred of 32 and up)."
  },
  {
    "objectID": "week01/w1_02_omics-data.html#fastq-cont.",
    "href": "week01/w1_02_omics-data.html#fastq-cont.",
    "title": "Omics data",
    "section": "FASTQ (cont.)",
    "text": "FASTQ (cont.)\nFASTQ files have no size limit, so you may receive a single file per sample, although:\n\n\nWith paired-end (PE) sequencing, forward and reverse reads are split into two files:\nforward reads contain R1 and reverse reads contain R2 in the file name.\nIf sequencing was done on multiple “lanes”, you get separate files for each lane.\n\n\n\n\nFASTQ files have the extension .fastq or .fq (but are commonly compressed, leading to fastq.gz etc.). All in all, having paired-end FASTQ files for 2 samples could look like this:\n# A listing of (unusually simple) file names:\nsample1_R1.fastq.gz\nsample1_R2.fastq.gz\nsample2_R1.fastq.gz\nsample2_R1.fastq.gz"
  },
  {
    "objectID": "week01/w1_02_omics-data.html#gtfgff",
    "href": "week01/w1_02_omics-data.html#gtfgff",
    "title": "Omics data",
    "section": "GTF/GFF",
    "text": "GTF/GFF\nThe GTF and GFF formats are tab-delimited tabular files that contain genome annotations, with:\n\nOne row for each annotated “genomic feature” (gene, exon, etc.)\nOne column for each piece of information about a feature, like its genomic coordinates\n\n\nSee the sample below, with an added header line (not normally present) with column names:\nseqname     source  feature start   end     score  strand  frame    attributes\nNC_000001   RefSeq  gene    11874   14409   .       +       .       gene_id \"DDX11L1\"; transcript_id \"\"; db_xref \"GeneID:100287102\"; db_xref \"HGNC:HGNC:37102\"; description \"DEAD/H-box helicase 11 like 1 (pseudogene)\"; gbkey \"Gene\"; gene \"DDX11L1\"; gene_biotype \"transcribed_pseudogene\"; pseudo \"true\"; \nNC_000001   RefSeq  exon    11874   12227   .       +       .       gene_id \"DDX11L1\"; transcript_id \"NR_046018.2\"; db_xref \"GeneID:100287102\"; gene \"DDX11L1\"; product \"DEAD/H-box helicase 11 like 1 (pseudogene)\"; pseudo \"true\"; \n\n\n\n\n\n\n\n\nSome details on the more important/interesting columns:\n\n\n\nseqname — Name of the chromosome, scaffold, or contig\nfeature — Name of the feature type, e.g. “gene”, “exon”, “intron”, “CDS”\nstart & end— Start & end position of the feature\nstrand — Whether the feature is on the + (forward) or - (reverse) strand\nattribute — A semicolon-separated list of tag-value pairs with additional information"
  },
  {
    "objectID": "week01/w1_02_omics-data.html#sambam",
    "href": "week01/w1_02_omics-data.html#sambam",
    "title": "Omics data",
    "section": "SAM/BAM",
    "text": "SAM/BAM\nUsing specialized bioinformatics tools, you can align HTS reads (in FASTQ files) to a reference genome assembly (in a FASTA file).\nThe resulting alignments are stored in the SAM (uncompressed) / BAM (compressed) format.\n\n\nSAM/BAM are tabular files with one line per alignment, each of which includes:\n\nThe position in the genome that the read aligned to\nA mapping score based on the length of the alignment and the number of mismatches\nThe sequence of aligned the read itself\n\n\n\n\n\n\n\n\n\n\nFile conversions\n\n\n\nFASTQ files can be converted to FASTA files (losing quality information) but not vice versa\nSAM/BAM files can be converted to FASTQ files (losing alignment information) but not vice versa\nProteome FASTA files can be produced from the combination of a FASTA genome assembly and a GFF/GTF genome annotation"
  },
  {
    "objectID": "ref/shell.html",
    "href": "ref/shell.html",
    "title": "shell",
    "section": "",
    "text": "TBA\n\n\n\n Back to top"
  },
  {
    "objectID": "week01/w1_ua_osc-setup.html",
    "href": "week01/w1_ua_osc-setup.html",
    "title": "Pre-course assignment: OSC access",
    "section": "",
    "text": "Overview\nBefore Thursday’s session, you should make sure that you have access to the Ohio Supercomputer Center (OSC), and the OSC Project for this course (PAS2880).\n\n\nBackground\nYou will do much of your work during this course through your browser at the Ohio Supercomputer Center (OSC). To access OSC, you will need to have an account. This course has its own OSC “Project”, and membership of this specific project will allow you to access our shared files and to reserve computing resources.\n\n\nWhat you should do\nAfter completing the pre-course survey (LINK TBA), you will receive an invitation email from OSC referencing the course’s project number PAS2880:\n\nIf you don’t have an OSC account yet\nFollow the instructions in the email to sign up for OSC and accept the invitation.\nIf you already have an OSC account\nThe email will likely tell you that you have been added to the project and don’t have to do anything.\nIn either case, check whether you can log in\nGo to https://ondemand.osc.edu and log in by typing your username and password on the left-hand side of the page. If you just created your account, it may take up to half an hour or so before you can log in.\n\nIf you have any questions about this or run into problems, don’t hesitate to contact Jelmer.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "week01/w1_overview.html#links",
    "href": "week01/w1_overview.html#links",
    "title": "Week 1: Intro to the course, omics data and OSC",
    "section": "1 Links",
    "text": "1 Links\n\nUngraded assignments for the first day\n\nPre-course survey - link TBA\nGet access to the Ohio Supercomputer Center (OSC)\n\n\n\nLecture pages\n\nTue: Course intro (slides)\nTue/Thu: Omics data (slides)\nThu: OSC intro"
  },
  {
    "objectID": "week01/w1_overview.html#content-overview",
    "href": "week01/w1_overview.html#content-overview",
    "title": "Week 1: Intro to the course, omics data and OSC",
    "section": "2 Content overview",
    "text": "2 Content overview\nThis week, you will get an overview of the course and a brief intro to the Ohio Supercomputer (OSC) during Tuesday’s session, and will be taught the basics of working in a Unix shell environment during Thursday’s meeting.\nMore specifically, this week you will learn:\n\nCourse intro & overview (Tuesday class)\n\nWhat to expect from this course.\nWhich tools and languages we will use.\nWhat is expected of you during this course.\nGet up to speed on the infrastructure of the course.\n\n\n\nOmics data (Tuesday class)\nTBA\n\n\nOSC Intro (Thursday class)\n\nWhat is a supercomputer and why is it useful?\nOverview of the resources the Ohio Supercomputer Center (OSC) provides.\nHow to use OSC OnDemand and access a Unix Shell in your browser."
  },
  {
    "objectID": "week01/w1_overview.html#readings",
    "href": "week01/w1_overview.html#readings",
    "title": "Week 1: Intro to the course, omics data and OSC",
    "section": "3 Readings",
    "text": "3 Readings\nThe first few pages of the book Computing Skills for Biologists by Allesina & Wilmes (CSB for short), should give you an introduction to the rationale behind the book as well as this course.\n\nRequired readings\n\nTBA ADD AN OMICS DATA INTRODUCTION PAPER\nCSB Chapter 0: “Introduction Building a Computing Toolbox” – Introduction & Section 0.1 only\n\n\n\nOptional readings\n\nBuffalo Preface and Chapter 1: “How to Learn Bioinformatics”"
  },
  {
    "objectID": "week01/w1_03_osc.html#goals-for-this-session",
    "href": "week01/w1_03_osc.html#goals-for-this-session",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "1 Goals for this session",
    "text": "1 Goals for this session\nThis session will provide an introduction to high-performance computing in general and to the Ohio Supercomputer Center (OSC) more specifically.\nThis is only meant as a brief overview to give context about the working environment that we will start using next week: we will do all of our coding and computing at OSC during this course. During the course, you’ll learn a lot more about most topics touched on in this page — week 6 and 7 in particular focus on OSC."
  },
  {
    "objectID": "week01/w1_03_osc.html#high-performance-computing",
    "href": "week01/w1_03_osc.html#high-performance-computing",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "2 High-performance computing",
    "text": "2 High-performance computing\nA supercomputer (also known as a “compute cluster” or simply a “cluster”) consists of many computers that are connected by a high-speed network, and that can be accessed remotely by its users. In more general terms, supercomputers provide high-performance computing (HPC) resources.\nThis is what Cardinal, one of the OSC supercomputers, physically looks like:\n\n\n\nThe Cardinal OSC cluster\n\n\nHere are some possible reasons to use a supercomputer instead of your own laptop or desktop:\n\nYour analyses take a long time to run, need large numbers of CPUs, or a large amount of memory.\nYou need to run some analyses many times.\nYou need to store a lot of data.\nYour analyses require software available only for the Linux operating system, but you use Windows.\nYour analyses require specialized hardware, such as GPUs (Graphical Processing Units).\n\nWhen you’re working with omics data, many of these reasons typically apply. This can make it hard or sometimes simply impossible to run all your analyses on your personal workstation, and supercomputers provide a solution.\n\n\nThe Ohio Supercomputer Center (OSC)\nThe Ohio Supercomputer Center (OSC) is a facility provided by the state of Ohio in the US. It has several supercomputers, lots of storage space, and an excellent infrastructure for accessing these resources.\n\n\n\n\n\n\n\nOSC websites and “Projects”\n\n\n\nOSC has three main websites — we will mostly or only use the first:\n\nhttps://ondemand.osc.edu: A web portal to use OSC resources through your browser (login needed).\nhttps://my.osc.edu: Account and project management (login needed).\nhttps://osc.edu: General website with information about the supercomputers, installed software, and usage.\n\n\nAccess to OSC’s computing power and storage space goes through OSC “Projects”:\n\nA project can be tied to a research project or lab, or be educational like this course’s project, PAS2880.\nEach project has a budget in terms of “compute hours” and storage space1.\nAs a user, it’s possible (and common) to be a member of multiple different projects."
  },
  {
    "objectID": "week01/w1_03_osc.html#the-structure-of-a-supercomputer-center",
    "href": "week01/w1_03_osc.html#the-structure-of-a-supercomputer-center",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "3 The structure of a supercomputer center",
    "text": "3 The structure of a supercomputer center\n\n3.1 Terminology\nLet’s start with some (super)computing terminology, going from smaller things to bigger things:\n\nNode\nA single computer that is a part of a supercomputer.\nSupercomputer / Cluster\nA collection of computers connected by a high-speed network. OSC currently has three: “Ascend”, “Cardinal”, and “Pitzer”.\nSupercomputer Center\nA facility like OSC that has one or more supercomputers.\n\n\n\n3.2 Supercomputer components\nWe can think of a supercomputer as having three main parts:\n\nFile Systems: Where files are stored (these are shared between the two OSC supercomputers!)\nLogin Nodes: The handful of computers everyone shares after logging in\nCompute Nodes: The many computers you can reserve to run your analyses\n\n\n\n\nThe structure of a supercomputer with three main components: file systems, login nodes, and compute nodes\n\n\n\n\nFile systems\nOSC has several distinct file systems:\n\n\n\n\n\n\n\n\n\n\n\nFile system\nLocated within\nQuota\nBacked up?\nAuto-purged?\nOne for each…\n\n\n\n\nHome\n/users/\n500 GB / 1 M files\nYes\nNo\nUser\n\n\nProject\n/fs/ess/\nFlexible\nYes\nNo\nOSC Project\n\n\nScratch\n/fs/scratch/\n100 TB\nNo\nAfter 90 days\nOSC Project\n\n\n\nDuring the course, we will be working in the project directory of the course’s OSC Project PAS2880: /fs/ess/PAS2880.\n\n\n\n\n\n\nTerminology\n\n\n\n\nDirectory is just another word for folder, often written as “dir” for short\nPaths TBA\n\n\n\n\n\n\nLogin Nodes\nLogin nodes are set aside as an initial landing spot for everyone who logs in to a supercomputer. There are only a handful of them on each supercomputer, they are shared among everyone, and they cannot be “reserved”.\nAs such, login nodes are meant only to do things like organizing your files and creating scripts for compute jobs, and are not meant for any serious computing, which should be done on the compute nodes.\n\n\n\nCompute Nodes\nData processing and analysis is done on compute nodes. You can only use compute nodes after putting in a request for resources (a “job”). The Slurm job scheduler, which we will learn to use later in this course, will then assign resources to your request.\n\n\n\n\n\n\nCompute node types\n\n\n\nCompute nodes come in different shapes and sizes. Standard, default nodes work fine for the vast majority of analyses, even with large-scale omics data. But you will sometimes need non-standard nodes, such as when you need a lot of RAM memory or need GPUs2.\n\n\n\n\n\n\n\n\nAt-home reading: What works differently on a supercomputer like at OSC? (Click to expand)\n\n\n\n\n\nCompared to command-line computing on a laptop or desktop, a number of aspects are different when working on a supercomputer like at OSC. We’ll learn much more about these later on in the course, but here is an overview:\n\n“Non-interactive” computing is common\nIt is common to write and “submit” scripts to a queue instead of running programs interactively.\nSoftware\nYou generally can’t install “the regular way”, and a lot of installed software needs to be “loaded”.\nOperating system\nSupercomputers run on the Linux operating system.\nLogin versus compute nodes\nAs mentioned, the nodes you end up on after logging in are not meant for heavy computing and you have to request access to “compute nodes” to run most analyses."
  },
  {
    "objectID": "week01/w1_03_osc.html#osc-ondemand",
    "href": "week01/w1_03_osc.html#osc-ondemand",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "4 OSC OnDemand",
    "text": "4 OSC OnDemand\nThe OSC OnDemand web portal allows you to use a web browser to access OSC resources such as:\n\nA file browser where you can also create and rename folders and files, etc.\nA Unix shell\n“Interactive Apps”: programs such as RStudio, Jupyter, VS Code and QGIS.\n\n Go to https://ondemand.osc.edu and log in (use the boxes on the left-hand side)\nYou should see a landing page similar to the one below:\n\n\n\nThe OSC OnDemand landing page\n\n\nWe will now go through some of the dropdown menus in the blue bar along the top.\n\n\n4.1 Files menu\nHovering over the Files dropdown menu gives a list of directories that you have access to. If your account is brand new, and you were added to PAS2880, you should only have three directories listed:\n\nA Home directory (starts with /users/)\nThe PAS2880 project’s “scratch” directory (/fs/scratch/PAS2880)\nThe PAS2880 project’s “project” directory (/fs/ess/PAS2880)\n\nYou will only ever have one Home directory at OSC, but for every additional project you are a member of, you should usually see additional /fs/ess and /fs/scratch directories appear.\n Click on our focal directory /fs/ess/PAS2880.\nOnce there, you should see whichever directories and files are present at the selected location, and you can click on the directories to explore the contents further:\n\n\n\nThe OnDemand file browser\n\n\nThis interface is much like the file browser on your own computer, so you can also create, delete, move and copy files and folders, and even upload (from your computer to OSC) and download (from OSC your computer) files3 — see the buttons across the top.\n\n\n\n4.2 Interactive Apps menu\nWe can access programs with Graphical User Interfaces (GUIs; point-and-click interfaces) via the Interactive Apps dropdown menu:\n\n\n\nOptions in the OSC Ondemand Interactive Apps dropdown menu\n\n\nNext week, we will start using the VS Code text editor, which is listed here as Code Server. Later, we will also use RStudio Server to code in R.\n\n\n\n4.3 Clusters menu\n\nSystem Status\nIn the “Clusters” dropdown menu, click on the item at the bottom, “System Status”:\n\n\n\nThe Clusters dropdown menu\n\n\nThis page shows an overview of the live, current usage of the two clusters — that can be interesting to get a good idea of the scale of the supercomputer center, which cluster is being used more, what the size of the “queue” (which has jobs waiting to start) is, and so on.\n\n\n\n\n\n\n\nShell Acccess\nInteracting with a supercomputer is most commonly done using a so-called “Unix shell”, which is a command-line interface to a computer that we will be using throughout this course. Still under the Clusters dropdown menu (see the screenshot above), you can access a Unix shell on Ascend, Cardinal, or Pitzer.\nI’m selecting a shell on the Cardinal cluster (Cardinal Shell Access item), which is the cluster we will be using throughout this course. This will open a new browser tab, where the bottom of the page looks like this:\n\n\n\n\n\nWe’ll return to this Unix shell next week."
  },
  {
    "objectID": "week01/w1_03_osc.html#footnotes",
    "href": "week01/w1_03_osc.html#footnotes",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n But we don’t have to pay anything for educational projects like this one. Otherwise, for OSC’s rates for academic research, see this page.↩︎\nGPUs are e.g. used for Nanopore basecalling↩︎\nThough this is not meant for large (&gt;1 GB) transfers. Different methods are available — we’ll talk about those later on.↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About this site and course",
    "section": "",
    "text": "This is the GitHub website for the course Practical Computing Skills for Omics Data (PLNTPTH 5006), a 3-credit course taught at Ohio State University during the Fall semester of 2025.\nThe course is taught by by Jelmer Poelstra from OSU’s Molecular and Cellular Imaging Center (MCIC) for the Department of Plant Pathology.\n\n\nCourse description\nAs datasets have rapidly grown larger in biology, coding has been recognized as an increasingly important skill for biologists. This is especially true in “omics” research with data from e.g. genomics and transcriptomics, which typically cannot be analyzed on a desktop computer, where cutting-edge software has a command-line interface, and where workflows can include many steps that need to be coordinated.\nIn this course, students will gain hands-on experience with a set of general and versatile tools for data-intensive research in omics and beyond. The course will focus on foundational skills such as working in the Unix shell and writing shell scripts, managing software and submitting jobs at a compute cluster (the Ohio Supercomputer Center), and building flexible, automated workflows. Additionally, the course will cover reproducibly organizing, documenting, and version-controlling research projects. Taken together, this course will allow students to reproduce their own research, and enable others to reproduce their research, with as little as a single command.\n\n\n\nMore information\n\nTopics taught\n\nUnix shell: basics, scripting, running command-line programs\nR: Basics, data wrangling & visualization, Quarto, and specifics to omics data\nSupercomputer usage: basics, software, and running Slurm batch jobs\nVersion control with Git and GitHub\nReproducibility including project documentation with Markdown, project file organization, and data management\nRunning and building Nextflow pipelines\n\n\n\nCourse books\n\nAllesina S, Wilmes M (2019). Computing Skills for Biologists. Princeton UP.\nBuffalo V (2015). Bioinformatics Data Skills: Reproducible and Robust Research with Open Source Tools. O’Reilly Media, Inc.\n\n\n\nCarmenCanvas website\nIf you are a student in this course, you should also refer to the CarmenCanvas site for this course (link TBA).\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "week02/w2_01_shell.html#goals-for-this-and-the-next-session",
    "href": "week02/w2_01_shell.html#goals-for-this-and-the-next-session",
    "title": "Unix Shell Basics - part I",
    "section": "1 Goals for this and the next session",
    "text": "1 Goals for this and the next session\nIn this week’s sessions, we’ll cover much of CSB’s Chapter 1 to learn:\n\nWhy using a command-line interface can be beneficial.\nWhat the Unix shell is and what you can do with it.\nUsing the shell, how to:\n\nNavigate around your computer.\nCreate and manage directories and files.\nView text files.\nSearch within, manipulate, and extract information from text files.\n\n\n\n\n\n\n\n\nSee the “Topic Overview” page on the Unix shell for an overview of shell commands we’ll cover during this course."
  },
  {
    "objectID": "week02/w2_01_shell.html#introduction",
    "href": "week02/w2_01_shell.html#introduction",
    "title": "Unix Shell Basics - part I",
    "section": "2 Introduction",
    "text": "2 Introduction\n\n2.1 Some terminology and concepts\n\nUnix (vs. Linux)\nWe can conceptualize Unix (or “Unix-like” / “Unix-based”)1 as a family of operating systems, which includes Linux and Mac but not Windows.\nFor scientific computing, Unix-based operating systems are generally preferable. Supercomputers, like the Ohio Supercomputer Center (OSC), use Linux. In this course, your laptop/desktop can run on any operating system, though, precisely because will connect to OSC and do our work there.\n\n\nUnix shell-related terms\n\nCommand Line — the most general term, an interface where you type commands\nTerminal — the program/app/window that can run a Unix shell\nShell — a command line interface to your computer\nUnix Shell — the type of shell on Unix-based (Linux + Mac) computers\nBash — the specific Unix shell language that is most common on Unix computers\n\nWhile these are not synonyms, in day-to-day computing, they are often used interchangeably.\n\n\n\n\n\n\nUser interfaces: CLI vs. GUI\n\n\n\nAlso, two common related acronyms:\n\nCommand-line Interface (CLI)\nGraphical User Interface (GUI)\n\nThese refer to the user interfaces to computers or individual program that are operated either by typing commands (CLI) or by pointing-and-clicking (GUI).\n\n\n\n\n\n\n2.2 Why use the Unix shell?\nThe Unix shell has been around for a long time and can sometimes seem a bit archaic. But astonishingly, a system largely built decades ago in an era with very different computers and datasets has stood the test of time, and the ability to use it is a crucial skill in applied bioinformatics.\nSpecifically, you may want to or need to use the Unix shell as opposed to programs with Graphical User Interfaces (GUIs) because of:\n\nSoftware needs\nWhen working with omics data, we don’t code our entire analysis from scratch – for example, we don’t build our own genome assembly or read alignment algorithm. Many of the external tools that we use for this can only be run using the shell: they only have a Command-Line Interface (CLI).\nAutomation and fewer errors\nThe shell allows you to repeat and automate tasks easily and without introducing errors.\nReproducibility\nIt’s straightforward to keep a record of what you have done.\nWorking with large files\nShell commands are good at processing large files, which are common when working with omics data.\nRemote computing – especially supercomputers\nIt is often only possible to work in a terminal when doing remote computing.\n\nBut what about versus other coding languages? Why do we need to know the Unix shell on top of R and/or Python?\n\nEfficiency\nFor many simpler tasks, built-in shell tools are faster — in terms of coding time, processing time, and the ease of processing very large files.\nRunning external software\nAs mentioned above, we need to use many external programs in omics data analysis with a CLI and the shell has a direct interface to these."
  },
  {
    "objectID": "week02/w2_01_shell.html#getting-started-with-unix",
    "href": "week02/w2_01_shell.html#getting-started-with-unix",
    "title": "Unix Shell Basics - part I",
    "section": "3 Getting started with Unix",
    "text": "3 Getting started with Unix\n\nUnix directory structure\n\n“Directory” (or “dir” for short) is the term for folder that is commonly used in Unix contexts.\nThe Unix directory structure is hierarchical, with a single starting point: the root, depicted as /.\nA “path” gives the location of a file or directory, in which directories are separated by forward slashes /.\nSo: a leading / in a path is the root dir, and any subsequent / are used to separate dirs. For example, the path to our OSC project’s dir is /fs/ess/PAS2880. This means: the dir PAS2880 is located inside the dir ess, which in turn is inside the dir fs, which in turn is in the computer’s root directory.\nThe OSC dir structure is somewhat different from that of a personal computer. Our Home dir is not /home/&lt;username&gt; like in the book and the schematic on the left, but /users/PASXXXX/&lt;username&gt;.\n\n\n\n\n\n\nGeneric computer dir structure, from O’Neil 2017\n\n\n\n\n\n\nKey dirs in the OSC dir structure\n\n\n\n\n\n\n\n Exercise: Figure out the path\nIn the above schematic on the left, what is the path to todo_list.txt.\n\n\nSolution\n\ncat Pacifici2013_data.csv | tr \";\" \"\\t\" &gt; ../Pacifici2013_data.tsv"
  },
  {
    "objectID": "week02/w2_01_shell.html#getting-started-with-the-shell",
    "href": "week02/w2_01_shell.html#getting-started-with-the-shell",
    "title": "Unix Shell Basics - part I",
    "section": "4 Getting started with the shell",
    "text": "4 Getting started with the shell\n\n4.1 Accessing a shell at OSC\n\nGo to https://ondemand.osc.edu in your browser, and log in.\nClick on Clusters and then Pitzer Shell Access.\n\n\n\n\n\n\n\n\nCopying and pasting in this shell\n\n\n\nYou can’t right-click in this shell, so to copy-and-paste:\n\nCopy simply by selecting text (you should see a copy () icon appear).\nPaste using Ctrl+V.\n\n Try copying and pasting a random word into your shell. This may just work, you may get a permission pop-up, or it may silently fail — if the latter, click on the clipboard icon in your browser’s address bar (see red circle in screenshot below):\n\n\n\n\n\n4.2 The prompt\nInside your terminal, the “prompt” indicates that the shell is ready for a command. What is shown exactly varies across shells and can also be customized, but our prompts at OSC should show the following information:\n&lt;username&gt;@&lt;node-name&gt; &lt;working-dir&gt;]$\nFor example (and note that ~ means your Home dir):\n[jelmer@pitzer-login02 ~]$ \nWe type our commands after the dollar sign, and then press Enter to execute the command. When the command has finished executing, we’ll get our prompt back and can type a new command.\n\n\n\n\n\n\nClearing the screen\n\n\n\nOSC prints welcome messages and storage quota information when you open a shell. To reduce the amount of text on the screen, I will clear the screen now and regularly throughout. This can be done with the keyboard shortcut Ctrl+L.\n\n\n\n\n\n4.3 A few simple commands: date and pwd\nThe Unix shell comes with hundreds of “commands”: small programs that perform specific actions. If you’re familiar with R or Python, a Unix command is like an R/Python function.\nLet’s start with a few simple commands:\n\nThe date command prints the current date and time:\ndate\nThu Feb 29 14:58:19 EST 2024\nThe pwd (Print Working Directory) command prints the path to the directory you are currently located in:\npwd\n/users/PAS0471/jelmer\n\nBoth of those commands provided us with some output. That output was printed to screen, which is the default behavior for nearly every Unix command.\n\n\n\n4.4 The cal command — and options & arguments\nThe cal command is another example of a command that simply prints some information to the screen, in this case a calendar. We’ll use it to learn about command options and arguments.\nInvoking cal without options or arguments will show the current month:\ncal\n    February 2024   \nSu Mo Tu We Th Fr Sa\n             1  2  3\n 4  5  6  7  8  9 10\n11 12 13 14 15 16 17\n18 19 20 21 22 23 24\n25 26 27 28 29\n\nOption examples\nUse the option -j (dash and then j) for a Julian calendar, in which day numbering is continuous instead of restarting each month:\n# Make sure to leave a space between `cal` and `-j`!\ncal -j\n       February 2024       \nSun Mon Tue Wed Thu Fri Sat\n                 32  33  34\n 35  36  37  38  39  40  41\n 42  43  44  45  46  47  48\n 49  50  51  52  53  54  55\n 56  57  58  59  60\nUse the -3 option to show 3 months (adding the previous and next month):\ncal -3\n    January 2024          February 2024          March 2024     \nSu Mo Tu We Th Fr Sa  Su Mo Tu We Th Fr Sa  Su Mo Tu We Th Fr Sa\n    1  2  3  4  5  6               1  2  3                  1  2\n 7  8  9 10 11 12 13   4  5  6  7  8  9 10   3  4  5  6  7  8  9\n14 15 16 17 18 19 20  11 12 13 14 15 16 17  10 11 12 13 14 15 16\n21 22 23 24 25 26 27  18 19 20 21 22 23 24  17 18 19 20 21 22 23\n28 29 30 31           25 26 27 28 29        24 25 26 27 28 29 30\n                                            31   \nWe can always combine multiple options, for example:\ncal -j -3\n        January 2024                February 2024                  March 2024        \nSun Mon Tue Wed Thu Fri Sat  Sun Mon Tue Wed Thu Fri Sat  Sun Mon Tue Wed Thu Fri Sat\n      1   2   3   4   5   6                   32  33  34                       61  62\n  7   8   9  10  11  12  13   35  36  37  38  39  40  41   63  64  65  66  67  68  69\n 14  15  16  17  18  19  20   42  43  44  45  46  47  48   70  71  72  73  74  75  76\n 21  22  23  24  25  26  27   49  50  51  52  53  54  55   77  78  79  80  81  82  83\n 28  29  30  31               56  57  58  59  60           84  85  86  87  88  89  90\n                                                           91 \nHandily, options can be “pasted together” like so (output not shown - same as above):\ncal -j3\n\n\nOptions summary\nAs we’ve seen, options are specified with a dash - (or --, as you’ll see later). So far, we’ve only worked with the type of options that are also called “flags”, which change some functionality in an ON/OFF type way:\n\nTurning a Julian calender display ON with -j\nTurning a 3-month display ON with -3.\n\nIn general terms, options change the behavior of a command.\n\n\nArguments\nWhereas options change the behavior of a command, arguments typically tell the command what to operate on. Most commonly, these are file or directory paths.\nAdmittedly, the cal command is not the best illustration of this pattern — when you give it one argument, this is supposed to be the year to show a calendar for:\ncal 2020\n                              2020                               \n\n       January               February                 March       \nSu Mo Tu We Th Fr Sa   Su Mo Tu We Th Fr Sa   Su Mo Tu We Th Fr Sa\n          1  2  3  4                      1    1  2  3  4  5  6  7\n 5  6  7  8  9 10 11    2  3  4  5  6  7  8    8  9 10 11 12 13 14\n12 13 14 15 16 17 18    9 10 11 12 13 14 15   15 16 17 18 19 20 21\n19 20 21 22 23 24 25   16 17 18 19 20 21 22   22 23 24 25 26 27 28\n26 27 28 29 30 31      23 24 25 26 27 28 29   29 30 31\n\n# [...output truncated, entire year is shown...]\nWe can also combine options and arguments:\ncal -j 2020\n                            2020                          \n\n          January                       February         \nSun Mon Tue Wed Thu Fri Sat   Sun Mon Tue Wed Thu Fri Sat\n              1   2   3   4                            32\n  5   6   7   8   9  10  11    33  34  35  36  37  38  39\n 12  13  14  15  16  17  18    40  41  42  43  44  45  46\n 19  20  21  22  23  24  25    47  48  49  50  51  52  53\n 26  27  28  29  30  31        54  55  56  57  58  59  60\n \n# [...output truncated, entire year is shown...]\nSo, arguments to a command:\n\nAre not preceded by a - (or --)\nIf options and arguments are combined, arguments come after options2.\n\n\n\n\n\n4.5 Getting help\nMany commands – and other command-line programs! – have a -h option for help, which usually gives a concise summary of the command’s syntax, i.e. it’s available options and arguments:\ncal -h\n\nUsage:\n cal [options] [[[day] month] year]\n\nOptions:\n -1, --one        show only current month (default)\n -3, --three      show previous, current and next month\n -s, --sunday     Sunday as first day of week\n -m, --monday     Monday as first day of week\n -j, --julian     output Julian dates\n -y, --year       show whole current year\n -V, --version    display version information and exit\n -h, --help       display this help text and exit\n\n\n\n\n\n\nAnother way to see documentation: the man command (Click to expand)\n\n\n\n\n\nAn alternative way of getting help for Unix commands is with the man command:\nman cal\nThis manual page often includes a lot more details than the --help output, and it is opened inside a “pager” rather than printed to screen: type q to exit the pager that man launches.\n\n\n\n\n Exercise: Interpreting the help output\n\nLook through the options listed when you ran cal -h, and try an option we haven’t used yet3. (You can also combine this new option with other options, if you want.)\n\n\n\nSolution\n\n# Print a calendar with Monday as the first day of th week (instead of the default, Sunday) \ncal -m\n    February 2024   \nMo Tu We Th Fr Sa Su\n          1  2  3  4\n 5  6  7  8  9 10 11\n12 13 14 15 16 17 18\n19 20 21 22 23 24 25\n26 27 28 29\n\n\nTry using one or more options in their “long form” (with --). Why would those be useful?\n\n\n\nSolution\n\nFor example:\ncal --julian --monday\n       February 2024       \nMon Tue Wed Thu Fri Sat Sun\n             32  33  34  35\n 36  37  38  39  40  41  42\n 43  44  45  46  47  48  49\n 50  51  52  53  54  55  56\n 57  58  59  60\nThe advantage of using long options is that it is much more likely that any reader of the code (including yourself next week) will immediately understand what these options are doing.\nNote that long options cannot be “pasted together” like short options.\n\n\nBonus: Try to figure out / guess what the cal [options] [[[day] month] year] means. Can you print a calendar for April 2017?\n\n\n\nSolution\n\nFirst of all, the square brackets around options and all of the possible arguments means that none of these are required — as we’ve seen, just cal (with no options or arguments) is a valid command.\nThe structure of the multiple square brackets around the day-month-year arguments indicate that:\n\nIf you provide only one argument, it should be a year\nIf you provide a second argument, it should be a month\nIf you provide a third argument, it should be a day\n\nTherefore, to print a calendar for April 2017:\ncal 4 2017\n     April 2017     \nSu Mo Tu We Th Fr Sa\n                   1\n 2  3  4  5  6  7  8\n 9 10 11 12 13 14 15\n16 17 18 19 20 21 22\n23 24 25 26 27 28 29\n30\n\n\n\n\n\n4.6 cd and command “actions”\nAll the commands so far “merely” provided some information, which was printed to screen.\nBut many commands perform another kind of action. For example, the command cd will change your working directory. And like many commands that perform a potentially invisible action, cd normally has no output at all.\nFirst, let’s check again where we are — we should be in our Home directory:\n# (Note: you will have a different project listed in your Home dir. This is not important.)\npwd\n/users/PAS0471/jelmer\nNow, let’s use cd to move to another directory by specifying the path to that directory as an argument:\ncd /fs/ess/PAS2880\n# Double-check that we moved:\npwd\n/fs/ess/PAS2880\nIn summary:\n\nLike cal, cd accepts an argument. Unlike cal, this argument takes the form of a path that the command should operate on, which is much more typical.\ncd gives no output when it successfully changed the working directory. This is very common behavior for Unix commands that perform operations: when they succeed, they are silent.\n\nLet’s also see what happens when cd does not succeed — it gives an error:\ncd /fs/ess/PAS2880\n-bash: cd: /fs/ess/PAS2880: No such file or directory\n\n\n\n\n\n\n\nGeneral shell tips\n\n\n\n\nEverything in the shell is case-sensitive, including commands and file names (hence the error above).\nYour cursor can be anywhere on a line (not just at the end) when you press Enter to execute a command!\nAny text that comes after a # is considered a comment instead of code!\n\n# This entire line is a comment - you can run it and nothing will happen\npwd    # 'pwd' will be executed but everything after the '#' is ignored\n/users/PAS0471/jelmer\n\n\n\n\n\n\n4.7 Keyboard shortcuts\nUsing keyboard shortcuts help you work much more efficiently in the shell. And some are invaluable:\n\nCancel/stop/abort — If your prompt is “missing”, the shell is still busy executing your command, or you typed an incomplete command. To abort, press Ctrl+C and you will get your prompt back.\nCommand history — Up / Down arrow keys to cycle through your command history.\nTab completion — The shell will auto-complete partial commands or file paths when you press Tab.\n\n\n Practice with Tab completion & command history\n\nType /f and press Tab (will autocomplete to /fs/)\nAdd e (/fs/e) and press Tab (will autocomplete to /fs/ess/).\nAdd PAS (/fs/ess/PAS) and press Tab. Nothing should happen: there are multiple (many!) options.\nPress Tab Tab (i.e., twice in quick succession) and it should say:\nDisplay all 503 possibilities? (y or n)\nType n to answer no: we don’t need to see all the dirs starting with PAS.\nAdd 27 (/fs/ess/PAS27) and press Tab (should autocomplete to /fs/ess/PAS2880).\nPress Enter. What does the resulting error mean?\nbash: /fs/ess/PAS2880/: Is a directory\n\n\n\nClick to see the solution\n\nBasically, everything you type in the shell should start with a command. Just typing the name of a dir or file will not make the shell print some info about or the contents of said dir or file, as you perhaps expected.\n\n\nPress ⇧ to get the previous “command” back on the prompt.\nPress Ctrl+A to move to the beginning of the line at once.\nAdd cd and a space in front of the dir, and press Enter again.\ncd /fs/ess/PAS2880/\n\n\n\n Practice with canceling\nTo simulate a long-running command that we may want to abort, we can use the sleep command, which will make the computer wait for a specified amount of time until giving your prompt back. Run the below command and instead of waiting for the full 60 seconds, press Ctrl + C to get your prompt back sooner!\nsleep 60s\nOr, use Ctrl + C after running this example of an incomplete command (an opening parenthesis ():\n(\n\n\n\n\n\n\n\nTable with useful keyboard shortcuts (Click to expand)\n\n\n\n\n\nNote that even on Macs, you should use Ctrl instead of switching them out for Cmd as you may be used to doing (in some cases, like copy/paste, both work).\n\n\n\n\n\n\n\nShortcut\nFunction\n\n\n\n\nTab\nTab completion\n\n\n⇧ / ⇩\nCycle through previously issued commands\n\n\nCtrl(+Shift)+C\nCopy selected text\n\n\nCtrl(+Shift)+V\nPaste text from clipboard\n\n\nCtrl+A / Ctrl+E\nGo to beginning/end of line\n\n\nCtrl+U /Ctrl+K\nCut from cursor to beginning / end of line\n\n\nCtrl+W\nCut word before before cursor (Only works on Macs in our shell in the browser!)\n\n\nCtrl+Y\nPaste (“yank”) text that was cut with one of the shortcuts above\n\n\nAlt+. / Esc+.\nRetrieve last argument of previous command (very useful!) (Esc+. for Mac)\n\n\nCtrl+R\nSearch history: press Ctrl+R again to cycle through matches, Enter to put command in prompt.\n\n\nCtrl+C\nCancel (kill/stop/abort) currently active command\n\n\nCtrl+D\nExit (a program or the shell, depending on the context) (same as exit command)\n\n\nCtrl+L\nClear the screen (same as clear command)\n\n\n\n\n\n\n\n\n\n4.8 Environment variables\nYou may be familiar with the concept of variables from previous experience with perhaps R or another language. Variables can hold values and other pieces of data and are essential in programming.\n\nAssigning and printing the value of a variable in R:\n\n# (Don't run this)\nx &lt;- 5\nx\n\n[1] 5\n\n\nAssigning and printing the value of a variable in the Unix shell:\nx=5\necho $x\n5\n\n\n\n\n\n\n\nIn the Unix shell code above, note that:\n\n\n\n\nThere cannot be any spaces around the = in x=5.\nYou need a $ prefix to reference (but not to assign) variables in the shell4.\nYou need the echo command, a general command to print text, to print the value of $x (cf. in R).\n\nBy the way, echo can also print literal text (as shown below) or combinations of literals and variables (next exercise):\necho \"Welcome to PLNTPTH 6193\"\nWelcome to PLNTPTH 6193\n\n\n\nEnvironment variables are pre-existing variables that have been automatically assigned values. Two examples:\n# $HOME contains the path to your Home dir:\necho $HOME\n/users/PAS0471/jelmer\n# $USER contains your user name:\necho $USER\njelmer\n\n Exercise: environment variables\nUsing an environment variable, print “Hello, my name is &lt;your username&gt;” (e.g. “Hello, my name is natalie”).\n\n\nClick to see the solution\n\n# (This would also work without the \" \" quotes)\necho \"Hello, my name is $USER\"\nHello, my name is jelmer\n\n\n\n\n\n4.9 Create your own dir & get the CSB data\nOur base OSC directory for the course is the /fs/ess/PAS2880 dir we are currently in. Now, let’s all create our own subdir in here, and get the data from the CSB book.\n\nCreate a directory for yourself using the mkdir (make dir) command:\nmkdir users/$USER\nMove there using cd:\n# (Instead of $USER, you can also start typing your username and press Tab)\ncd users/$USER\nGet the files associated with the CSB book by “cloning” (downloading) its Git repository:\ngit clone https://github.com/CSB-book/CSB.git\nMove into the sandbox dir for the Unix chapter (remember to use tab completion):\ncd CSB/unix/sandbox\n\n\n\n\n4.10 Paths & navigation shortcuts\n\nAbsolute (full) paths versus relative paths\n\nAbsolute (full) paths (e.g. /fs/ess/PAS2880)\nPaths that begin with a / always start from the computer’s root directory, and are called “absolute paths”.\n(They are equivalent to GPS coordinates for a geographical location, as they work regardless of where you are).\nRelative paths (e.g. CSB/unix/sandbox)\nPaths that instead start from your current working directory are called “relative paths”.\n(These work like directions along the lines of “take the second left:” they depend on your current location.)\n\nNext week, we’ll talk more about the distinction between absolute and relative paths, and their respective merits.\n\n\nPath shortcuts\n\n~ (a tilde) — represents your Home directory. For example, cd ~ moves you to your Home dir.\n. (a single period) — represents the current working directory (we’ll soon see how why that can be useful).\n.. (two periods) — Represents the directory “one level up”, i.e. towards the computer’s root dir.\n\nSome examples of ..:\n\nUse .. to go up one level in the dir hierarchy:\npwd\n/fs/ess/PAS2880/users/jelmer/CSB/unix/sandbox\ncd ..\npwd\n/fs/ess/PAS2880/users/jelmer/CSB/unix\nThis pattern can be continued all the way to the root of the computer, so ../.. means two levels up:\ncd ../..\npwd\n/fs/ess/PAS2880/users/jelmer\n\n\n\n\n\n\n\nThese path shortcuts work with many commands\n\n\n\nThese are general shell shortcuts that work with any command that accepts a path/file name.\n\n\n\n\n\n Exercise\n\nA) Use a relative path to move back to the /fs/ess/PAS2880/users/$USER/CSB/unix/sandbox dir.\n\n\n\n(Click for the solution)\n\ncd CSB/unix/sandbox\n\n\nB) Use a relative path (with ..) to move into the /fs/ess/PAS2880/users/$USER/CSB/unix/data dir.\n\n\n\n(Click for the solution)\n\ncd ../data\nYou may have done this in two steps, because you may not have realized that you can “add to” a path after .. like we did above. So you may have done this:\ncd ..\ncd data\nThat’s OK, but is obviously more typing.\n\n\nC) The ls command lists files and dirs, and accepts one or more paths as arguments. Use ls to list the files in your Home dir with a shortcut and without moving there.\n\n\n\n(Click for the solution)\n\n\nWith the ~ shortcut:\n\nls ~\n# (Output not shown)\n\nWith the $HOME environment variable:\n\nls $HOME\n# (Output not shown)"
  },
  {
    "objectID": "week02/w2_01_shell.html#basic-unix-commands",
    "href": "week02/w2_01_shell.html#basic-unix-commands",
    "title": "Unix Shell Basics - part I",
    "section": "5 Basic Unix commands",
    "text": "5 Basic Unix commands\n\n5.1 ls to list files\nThe ls command, short for “list”, will list files and directories — by default those in your current working dir:\n# (You should be in /fs/ess/PAS2880/users/$USER/CSB/unix/data)\nls\nBuzzard2015_about.txt  Gesquiere2011_about.txt  Marra2014_about.txt   miRNA                   Pacifici2013_data.csv  Saavedra2013_about.txt\nBuzzard2015_data.csv   Gesquiere2011_data.csv   Marra2014_data.fasta  Pacifici2013_about.txt  Saavedra2013\n\n\n\n\n\n\nls output colors (click to expand)\n\n\n\n\n\nThe ls output above does not show the different colors you should see in your shell — the most common ones are:\n\nEntries in blue are directories (like miRNA and Saavedra2013 above)\nEntries in black are regular files (like all other entries above)\nEntries in red are compressed files (we’ll see examples of this later).\n\n\n\n\nFor which dir ls lists its contents can be changed with arguments, and how it shows the output can be changed with options. For example, we can call ls with the option -l (lowercase L):\nls -l \ntotal 1793\n-rw-rw----+ 1 jelmer PAS0471     562 Feb 24 20:30 Buzzard2015_about.txt\n-rw-rw----+ 1 jelmer PAS0471   39058 Feb 24 20:30 Buzzard2015_data.csv\n-rw-rw----+ 1 jelmer PAS0471     447 Feb 24 20:30 Gesquiere2011_about.txt\n-rw-rw----+ 1 jelmer PAS0471   38025 Feb 24 20:30 Gesquiere2011_data.csv\n-rw-rw----+ 1 jelmer PAS0471     756 Feb 24 20:30 Marra2014_about.txt\n-rw-rw----+ 1 jelmer PAS0471  566026 Feb 24 20:30 Marra2014_data.fasta\ndrwxrwx---+ 2 jelmer PAS0471    4096 Feb 24 20:30 miRNA\n-rw-rw----+ 1 jelmer PAS0471     520 Feb 24 20:30 Pacifici2013_about.txt\n-rw-rw----+ 1 jelmer PAS0471 1076150 Feb 24 20:30 Pacifici2013_data.csv\ndrwxrwx---+ 2 jelmer PAS0471    4096 Feb 24 20:30 Saavedra2013\n-rw-rw----+ 1 jelmer PAS0471     322 Feb 24 20:30 Saavedra2013_about.txt\nIt lists the same items as earlier, but printed in a different format: one item per line, with additional information such as the date and time each file was last modified, and file sizes in bytes (to the left of the date).\nLet’s add another option, -h:\nls -lh\ntotal 1.8M\n-rw-rw----+ 1 jelmer PAS0471  562 Feb 24 20:30 Buzzard2015_about.txt\n-rw-rw----+ 1 jelmer PAS0471  39K Feb 24 20:30 Buzzard2015_data.csv\n-rw-rw----+ 1 jelmer PAS0471  447 Feb 24 20:30 Gesquiere2011_about.txt\n-rw-rw----+ 1 jelmer PAS0471  38K Feb 24 20:30 Gesquiere2011_data.csv\n-rw-rw----+ 1 jelmer PAS0471  756 Feb 24 20:30 Marra2014_about.txt\n-rw-rw----+ 1 jelmer PAS0471 553K Feb 24 20:30 Marra2014_data.fasta\ndrwxrwx---+ 2 jelmer PAS0471 4.0K Feb 24 20:30 miRNA\n-rw-rw----+ 1 jelmer PAS0471  520 Feb 24 20:30 Pacifici2013_about.txt\n-rw-rw----+ 1 jelmer PAS0471 1.1M Feb 24 20:30 Pacifici2013_data.csv\ndrwxrwx---+ 2 jelmer PAS0471 4.0K Feb 24 20:30 Saavedra2013\n-rw-rw----+ 1 jelmer PAS0471  322 Feb 24 20:30 Saavedra2013_about.txt\n\n\nWhat is different about the output, and what do you think that means? (Click to see the answer)\n\nThe only difference is in the format of the column reporting the sizes of the items listed.\nWe now have “Human-readable filesizes” (hence -h), where sizes on the scale of kilobytes will be shown with Ks, of megabytes with Ms, and of gigabytes with Gs. That can be really useful especially for very large files.\n\n\nWe can also list files in directories other than the one we are in, by specifying that dir as an argument:\nls miRNA\nggo_miR.fasta  hsa_miR.fasta  miR_about.txt  miRNA_about.txt  ppa_miR.fasta  ppy_miR.fasta  ptr_miR.fasta  ssy_miR.fasta\nAnd like we saw with cal, we can combine options and arguments:\nls -lh miRNA\ntotal 320K\n-rw-rw----+ 1 jelmer PAS0471  18K Feb 24 20:30 ggo_miR.fasta\n-rw-rw----+ 1 jelmer PAS0471 131K Feb 24 20:30 hsa_miR.fasta\n-rw-rw----+ 1 jelmer PAS0471  104 Feb 24 20:30 miR_about.txt\n-rw-rw----+ 1 jelmer PAS0471  104 Feb 24 20:30 miRNA_about.txt\n-rw-rw----+ 1 jelmer PAS0471 4.0K Feb 24 20:30 ppa_miR.fasta\n-rw-rw----+ 1 jelmer PAS0471  33K Feb 24 20:30 ppy_miR.fasta\n-rw-rw----+ 1 jelmer PAS0471  29K Feb 24 20:30 ptr_miR.fasta\n-rw-rw----+ 1 jelmer PAS0471  495 Feb 24 20:30 ssy_miR.fasta\nLet’s move into the sandbox dir in preparation for the next sections:\ncd ../sandbox\n\nls\nPapers and reviews\n\n\n\n5.2 cp to copy files\nThe cp command copies files and/or dirs from one location to another. It has two required arguments: what you want to copy (source), and where you want to copy it to (destination). Its basic syntax is cp &lt;source&gt; &lt;destination&gt;.\nFor example, to copy a file to our current working directory using the . shortcut, keeping the original file name:\ncp ../data/Buzzard2015_about.txt .\nWe can also copy using a new name for the copy:\ncp ../data/Buzzard2015_about.txt buzz2.txt\ncp will by default refuse to copy directories and their contents — that is, it is not “recursive” by default. The -r option is needed for recursive copying:\ncp -r ../data/ . \nCheck the contents of the sandbox dir again now that we’ve copied several items there:\nls\nbuzz2.txt  Buzzard2015_about.txt  data  Papers and reviews\n\n\n\n5.3 mv to move and rename files\nUse mv both to move and rename files (this is fundamentally the same operation):\n# Same directory, different file name (\"renaming\")\nmv buzz2.txt buzz_copy.txt\n# Different directory, same file name (\"moving\")\nmv buzz_copy.txt data/\nUnlike cp, mv is recursive by default, so you won’t need the -r option.\n\n\n\n\n\n\nBoth the mv and cp commands will by default:\n\n\n\n\nNot report what they do: no output = success (use the -v option for verbose to make them report what they do).\nOverwrite existing files without reporting this (use the -i option for interactive to make them ask before overwriting).\n\n\n\n\n\n\n5.4 rm to remove files and dirs\nThe rm command removes files and optionally dirs — here, we’ll remove the file copy we made above:\nrm Buzzard2015_about.txt\nLike with cp, the -r option is needed to make the command work recursively:\n# First we create 3 levels of dirs - we need `-p` to make mkdir work recursively:\nmkdir -p d1/d2/d3\n\n# Then we try to remove the d1 dir - which fails:\nrm d1\nrm: cannot remove ‘d1’: Is a directory\n# But it does work (silently!) with the '-r' option:\nrm -r d1\n\n\n\n\n\n\nThere is no thrash bin when deleting files in the shell, so use rm with caution! (Click to expand)\n\n\n\n\n\nrm -r can be very dangerous — for example rm -r / would at least attempt to remove the entire contents of the computer, including the operating system.\nA couple ways to take precautions:\n\nYou can add the -i option, which will have you confirm each individual removal (can be tedious)\nWhen you intend to remove an empty dir, you can use the rmdir command which will do just (and only) that — that way, if the dir isn’t empty after all, you’ll get an error.\n\n\n\n\n\n Bonus exercise: cp and mv behavior\nFor both cp and mv, when operating on files (and this works equivalently for dirs):\n\nIf the destination is an existing dir, the file will go into that dir and keep its original name.\nIf the destination is not an existing dir, the (last bit of the) destination specifies the new file name.\nA trailing slash in the destination makes explicit that you are referring to a dir and not a file.\n\n\nWith that in mind, try to answer the following questions about this command:\ncp Buzzard2015_about.txt more_data/\n\nWhat do you think the command would do or attempt to do?\nDo you think the command will succeed?\nWhat would the command have done if we had omitted the trailing forward slash?\n\n\n\nClick to see the solution\n\n\n\nBecause we put a trailing forward slash in more_data/, we are making clear that we are referring to a directory. So the file should be copied into a dir more_data, and keep the same file name.\nHowever, the more_data/ dir does not exist, and cp will not create a dir on the fly, so this will fail:\ncp Buzzard2015_about.txt more_data/\ncp: cannot create regular file ‘more_data/’: Not a directory\nIf we had omitted the trailing forward slash, we would have created a copy of the file with file name more_data (note that no file extension is needed, per se).\nP.S: To make the original intention work, first create the destination dir:\nmkdir more_data\ncp Buzzard2015_about.txt more_data/\nNote also that once the more_data dir exists, it does not make a difference whether or not you using a trailing slash (!).\n\n\n\n\n\n\n5.5 Viewing and processing text files\ncd ../data   # Move to the data dir for the next commands\n\ncat\nThe cat command will print the entire contents of (a) file(s) to screen:\ncat Marra2014_about.txt\nData published by:\nMarra NJ, DeWoody JA (2014) Transcriptomic characterization of the immunogenetic repertoires of heteromyid rodents. BMC Genomics 15: 929. http://dx.doi.org/10.1186/1471-2164-15-929\n\nData description:\nFile D_spec_spleen_filtered.fasta (57.01Mb) contains Dipodomys spectabilis spleen transcriptome data. Combined assembly of 454 reads and fragmented Illumina assembly (see methods of associated paper) in gsAssembler version 2.6.\nNote that we truncated the original file to 1% of its original size and named it Marra2014_data.txt\n\nData taken from:\nMarra NJ, DeWoody JA (2014) Data from: Transcriptomic characterization of the immunogenetic repertoires of heteromyid rodents. Dryad Digital Repository. http://dx.doi.org/10.5061/dryad.qn474\n\n\n\nhead and tail\nThe head and tail commands will print the first or last lines of a file:\n\nhead & tail’s defaults are to print 10 lines:\nhead Gesquiere2011_data.csv\nmaleID  GC      T\n1       66.9    64.57\n1       51.09   35.57\n1       65.89   114.28\n1       80.88   137.81\n1       32.65   59.94\n1       60.52   101.83\n1       65.89   65.84\n1       52.72   43.98\n1       84.85   102.31\nUse the -n option to specify the number of lines to print:\nhead -n 3 Gesquiere2011_data.csv\nmaleID  GC      T\n1       66.9    64.57\n1       51.09   35.57\nA neat trick with tail is to start at a specific line, often used to skip the header line, like in this example5:\ntail -n +2 Gesquiere2011_data.csv\n1       66.9    64.57\n1       51.09   35.57\n1       65.89   114.28\n1       80.88   137.81\n1       32.65   59.94\n1       60.52   101.83\n1       65.89   65.84\n1       52.72   43.98\n1       84.85   102.31\n1       98.25   149.61\n[...output truncated...]\n\n\n\n\nwc -l\nThe wc command is different from the previous commands, which all printed file contents. By default, wc will count lines, words, and characters — but it is most commonly used to only count lines, with the -l option:\nwc -l Marra2014_about.txt\n9 Marra2014_about.txt"
  },
  {
    "objectID": "week02/w2_01_shell.html#advanced-unix-commands",
    "href": "week02/w2_01_shell.html#advanced-unix-commands",
    "title": "Unix Shell Basics - part I",
    "section": "6 Advanced Unix commands",
    "text": "6 Advanced Unix commands\nStart by moving back to the sandbox dir:\ncd ../sandbox\n\n6.1 Standard output and redirection\nThe regular output of a command is also called “standard out” (“stdout”). As we’ve seen many times now, such output is by default printed to screen, but it can alternatively be “redirected”, such as into a file.\nWith “&gt;”, we redirect output to a file:\n\nIf the file doesn’t exist, it will be created.\nIf the file does exist, any contents will be overwritten.\n\nFirst, let’s remind ourselves what echo does without redirection:\necho \"My first line\"\nMy first line\nNow, let’s redirect to a new file test.txt — no output is printed, as it went into the file:\necho \"My first line\" &gt; test.txt\ncat test.txt\nMy first line\nLet’s redirect another line into that same file:\necho \"My second line\" &gt; test.txt\ncat test.txt\nMy second line\nThat may not have been what we intended! As explained above, the file was overwritten.\nWith “&gt;&gt;”, however, we append the output to a file:\necho \"My third line\" &gt;&gt; test.txt\ncat test.txt\nMy second line\nMy third line\n\n\n\n6.2 Standard input and pipes\nRecall from today’s previous examples that a file name can be given as an argument to many commands — for example, see the following sequence of commands:\n# First we redirect the ls output to a file\nls ../data/Saavedra2013 &gt; filelist.txt\n\n# Let's check what that looks like:\nhead -n 5 filelist.txt\nn10.txt\nn11.txt\nn12.txt\nn13.txt\nn14.txt\n# Then we count the nr. of lines, which is the number of files+dirs in Saavedra2013:\nwc -l filelist.txt\n59 filelist.txt\nHowever, most commands also accept input from so-called “standard input” (stdin) using the pipe, “|”:\nls ../data/Saavedra2013 | wc -l\n59\nWhen we use the pipe, the output of the command on the left-hand side (a file listing, in this case) is no longer printed to screen but is redirected into the wc command, which will gladly accept input that way instead of via an argument with a file name like in the earlier example.\nPipes are useful because they avoid having to write/read intermediate files — this saves typing, makes the operation quicker, and reduces file clobber.\n\n\n\n6.3 Selecting columns using cut\nWe’ll now turn to some commands that may be described as Unix “data tools”: cut, sort, uniq, tr, and grep.\nMost of the examples will use the file Pacifici2013_data.csv, so let’s have a look at the contents of that file first:\ncd ../data\nhead -n 3 Pacifici2013_data.csv\nTaxID;Order;Family;Genus;Scientific_name;AdultBodyMass_g;Sources_AdultBodyMass;Max_longevity_d;Sources_Max_longevity;Rspan_d;AFR_d;Data_AFR;Calculated_GL_d;GenerationLength_d;Sources_GL\n7580;Rodentia;Cricetidae;Eligmodontia;Eligmodontia typus;17.37;PanTHERIA;292;PanTHERIA;254.64;73.74;calculated;147.5856;147.5856;Rspan-AFR(SM+Gest)\n42632;Rodentia;Cricetidae;Microtus;Microtus oregoni;20.35;PanTHERIA;456.25;PanTHERIA;445.85;58.06;calculated;187.3565;187.3565;Rspan-AFR(SM+Gest)\n\n\n\n\n\n\nTabular plain-text files (Click to expand)\n\n\n\n\n\nIn this course, we’ll be working almost exclusively with so-called “plain-text” files. These are simple files that can be opened by any text editor and Unix shell tool, as opposed to more complex “binary” formats like Excel or Word files. Almost all common genomics file formats are plain-text.\n“Tabular” files contain data that is arranged in a rows-and-columns format, like a table or an Excel worksheet.\nPlain-text tabular files, like Pacifici2013_data.csv, contain a specific “delimiter” to delimit columns — most commonly:\n\nA Tab, and such files are often stored with a .tsv extension for Tab-Separated Values (“TSV file”).\nA comma, and such files are often stored with a .csv extension for Comma-Separated Values (“CSV file”).\n\n\n\n\n\n\nWhat delimiter does the Pacifici2013_data.csv file appear to contain? (Click for the answer)\n\nFrom looking at the first couple of lines that we printed above, the delimiter is a semicolon ; — “even though” the file has a .csv extension.\n\n\nThe cut command will select/“cut out” one or more columns from a tabular file:\n\nWe’ll always have to use the -f option to specify the desired column(s) / “field”(s).\nBecause its default column delimiter is a Tab, for this file, we’ll have to specify the delimiter with -d.\n\n# Select the first column of the file:\ncut -d \";\" -f 1 Pacifici2013_data.csv\nTaxID\n7580\n42632\n42653\n42662\n16652\n[...output truncated...]\nThat worked, but a ton of output was printed, and we may find ourselves scrolling to the top to see the first few lines – in many cases, it can be useful to pipe the output to head to see if our command works:\ncut -d \";\" -f 1 Pacifici2013_data.csv | head -n 3\nTaxID\n7580\n42632\n\n\n\n\n\n\nSelecting multiple columns with cut (Click to expand)\n\n\n\n\n\nTo select multiple columns, use a range or comma-delimited list:\ncut -d \";\" -f 1-4 Pacifici2013_data.csv | head -n 3\nTaxID;Order;Family;Genus\n7580;Rodentia;Cricetidae;Eligmodontia\n42632;Rodentia;Cricetidae;Microtus\ncut -d \";\" -f 2,8 Pacifici2013_data.csv | head -n 3\nOrder;Max_longevity_d\nRodentia;292\nRodentia;456.25\n\n\n\n\n\n\n6.4 Combining cut, sort, and uniq to create a list\nLet’s say we want an alphabetically sorted list of animal orders from the Pacifici2013_data.csv file. To do this, we’ll need two new commands:\n\nsort to sort/order/arrange rows, by default in alphanumeric order.\nuniq to remove duplicates (i.e., keep all distinct) entries from a sorted file/list\n\nWe’ll build up a small “pipeline” to do this, step-by-step and piping the output into head every time. First, we get rid of the header line with our tail trick:\ntail -n +2 Pacifici2013_data.csv | head -n 5\n7580;Rodentia;Cricetidae;Eligmodontia;Eligmodontia typus;17.37;PanTHERIA;292;PanTHERIA;254.64;73.74;calculated;147.5856;147.5856;Rspan-AFR(SM+Gest)\n42632;Rodentia;Cricetidae;Microtus;Microtus oregoni;20.35;PanTHERIA;456.25;PanTHERIA;445.85;58.06;calculated;187.3565;187.3565;Rspan-AFR(SM+Gest)\n42653;Rodentia;Cricetidae;Peromyscus;Peromyscus gossypinus;27.68;PanTHERIA;471.45833335;PanTHERIA;444.87833335;72.58;calculated;201.59471667;201.5947166715;Rspan-AFR(SM+Gest)\n42662;Macroscelidea;Macroscelididae;Elephantulus;Elephantulus myurus;59.51;PanTHERIA;401.5;PanTHERIA;412.34;90.48;calculated;210.0586;210.0586;Rspan-AFR(SM+Gest)\n16652;Rodentia;Cricetidae;Peromyscus;Peromyscus boylii;23.9;PanTHERIA;547.5;PanTHERIA;514.13;79.97;calculated;229.0677;229.0677;Rspan-AFR(SM+Gest)\nSecond, we select our column of interest with cut:\ntail -n +2 Pacifici2013_data.csv | cut -d \";\" -f 2 | head -n 5\nRodentia\nRodentia\nRodentia\nMacroscelidea\nRodentia\nThird, we pipe to sort to sort the result:\ntail -n +2 Pacifici2013_data.csv | cut -d \";\" -f 2 | sort | head -n 5\nAfrosoricida\nAfrosoricida\nAfrosoricida\nAfrosoricida\nAfrosoricida\nFourth and finally, we use uniq to only keep unique rows (values):\ntail -n +2 Pacifici2013_data.csv | cut -d \";\" -f 2 | sort | uniq\nAfrosoricida\nCarnivora\nCetartiodactyla\nChiroptera\nCingulata\nDasyuromorphia\nDermoptera\nDidelphimorphia\n[...output truncated...]\n\n\n\n\n\n\nGet a count table with uniq -c (Click to expand)\n\n\n\n\n\nWith a very small modification to our pipeline, using uniq’s -c option (for count), we can generate a “count table” instead of a simple list:\ntail -n +2 Pacifici2013_data.csv | cut -d \";\" -f 2 | sort | uniq -c\n     54 Afrosoricida\n    280 Carnivora\n    325 Cetartiodactyla\n   1144 Chiroptera\n     21 Cingulata\n[...output truncated...]\n\n\n\n\n\n\n6.5 grep to print lines matching a pattern\nThe grep command is useful to find specific text or patterns in a file. By default, it will print each line that contains a “match” in full. It’s basic syntax is grep \"&lt;pattern&gt;\" file.\nFor example, this will print all lines from Pacifici2013_data.csv that contain “Vombatidae”:\ngrep \"Vombatidae\" Pacifici2013_data.csv\n40555;Diprotodontia;Vombatidae;Lasiorhinus;Lasiorhinus latifrons;26163.8;PanTHERIA;9928;\"PanTHERIA;AnAge\";9317.5;652.24;calculated;3354.315;3354.315;Rspan-AFR(SM+Gest)\n40556;Diprotodontia;Vombatidae;Vombatus;Vombatus ursinus;26000;PanTHERIA;10238.25;\"PanTHERIA;AnAge\";9511.6;783.65;calculated;3542.014;3542.014;Rspan-AFR(SM+Gest)\n11343;Diprotodontia;Vombatidae;Lasiorhinus;Lasiorhinus krefftii;31849.99;PanTHERIA;10950;\"PanTHERIA;AnAge\";no information;no information;no information;no information;3354.315;Mean_congenerics_same_body_mass\nInstead of printing matching lines, we can also count them with the -c option:\ngrep -c \"Chiroptera\" Pacifici2013_data.csv\n1144\nThe option -v inverts grep’s behavior and will print all lines not matching the pattern — here, we’ll combine -v and -c to count the number of lines that do not contain “Vombatidae”:\ngrep -vc \"Vombatidae\" Pacifici2013_data.csv\n5424\n\n\n\n\n\n\ngrep tips\n\n\n\n\nIt is best to always use quotes around the pattern.6\nIncomplete matches, including in individual words, work: “Vombat” matches Vombatidae.\ngrep has many other useful options — more in Week 4:\n\n-i to ignore case\n-r to search files recursively\n-w to match “words”\n\n\n\n\n\n\n\n6.6 Substituting characters using tr\ntr for translate will substitute characters – here, any a for a b:\necho \"aaaabbb\" | tr a b\nbbbbbbb\nOddly enough tr does not take a file name as an argument, so how can we provide it with input from a file? The easiest way is by using a pipe (note that the book also shows another method):\ncat Pacifici2013_data.csv | tr \";\" \"\\t\" | head -n 3\nTaxID   Order   Family  Genus   Scientific_name AdultBodyMass_g Sources_AdultBodyMass   Max_longevity_d Sources_Max_longevity   Rspan_d AFR_d   Data_AFR        Calculated_GL_d   GenerationLength_d      Sources_GL\n7580    Rodentia        Cricetidae      Eligmodontia    Eligmodontia typus      17.37   PanTHERIA       292     PanTHERIA       254.64  73.74   calculated      147.5856  147.5856        Rspan-AFR(SM+Gest)\n42632   Rodentia        Cricetidae      Microtus        Microtus oregoni        20.35   PanTHERIA       456.25  PanTHERIA       445.85  58.06   calculated      187.3565  187.3565        Rspan-AFR(SM+Gest)\nThe example above converted the ; delimited to a Tab (i.e., a CSV file to a TSV file), where \\t is a regular expression meaning Tab7. (Though note that we didn’t modify the original file nor saved the output in a new file.)\n\n\n\n\n\n\nDeletion and “squeezing” with tr (Click to expand)\n\n\n\n\n\n\nDelete all a’s:\necho \"aabbccddee\" | tr -d a\nbbccddee\nRemove consecutive duplicates a’s:\necho \"aabbccddee\" | tr -s a\nabbccddee\n\n\n\n\n\n\n\n\n\n\ntr also works well with multiple values and replacements (Click to expand)\n\n\n\n\n\n\nReplace multiple types of characters with one:\n\necho \"123456789\" | tr 1-5 0    # Replace any of 1-5 with 0\n000006789\n\nOne-to-one mapping of input and replacement!\n\necho \"ACtGGcAaTT\" | tr actg ACTG\nACTGGCAATT\necho \"aabbccddee\" | tr a-c 1-3\n112233ddee\n\n\n\n\n Exercise: Redirecting tr output\nModify our command in which we changed the delimiter to a Tab to redirect the output to a new file, Pacifici2013_data.tsv (note the extension) in the sandbox dir (that’s not where you are located yourself).\n\n\nSolution\n\ncat Pacifici2013_data.csv | tr \";\" \"\\t\" &gt; ../Pacifici2013_data.tsv"
  },
  {
    "objectID": "week02/w2_01_shell.html#wrap-up-the-unix-philosophy",
    "href": "week02/w2_01_shell.html#wrap-up-the-unix-philosophy",
    "title": "Unix Shell Basics - part I",
    "section": "7 Wrap-up & the Unix philosophy",
    "text": "7 Wrap-up & the Unix philosophy\n\n7.1 Covered in the chapter but not in today’s lecture\n\nThe less pager to view files\nThe find command to find files\nShowing and changing file permissions\nBasic shell scripting\nfor loops\n$PATH and bash profile settings\n\nWe’ll cover all of these in class over the next few weeks.\n\n\n\n7.2 The Unix philosophy\n\nThis is the Unix philosophy: Write programs that do one thing and do it well. Write programs to work together. Write programs to handle text streams, because that is a universal interface. — Doug McIlory\n\nAdvantages of a modular approach:\n\nEasier to spot errors\nEasy to swap out components, including in other languages\nEasier to learn (?)\n\nText “streams”?\nRather than loading entire files into memory, process them one line at a time. Very useful with large files!\n# This command would combine all files in the working dir ending in `.fa`\n# (i.e. FASTA files) into a single file -- even if that's multiple GBs,\n# this will not be a heavy lift at all!\ncat *.fa &gt; combined.fa\n\n\n\n7.3 The Unix shell in the weeks ahead\n\nIn all course weeks, we will be working in the Unix shell, though our focus in several cases will be on a specific tool, such as Git in week 3.\nIn week 4, we’ll fully focus on the shell and shell scripting."
  },
  {
    "objectID": "week02/w2_01_shell.html#footnotes",
    "href": "week02/w2_01_shell.html#footnotes",
    "title": "Unix Shell Basics - part I",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n Technically, the latter terms are more correct, as Unix formally does refer to a specific operating system.↩︎\n Though some commands are flexible and accept either order.↩︎\nThere really is only one proper additional options: two others reflect the defaults, and then there’s the version option.↩︎\nBecause of this, anytime you see a word/string that starts with a $ in the shell, you can safely assume that it is a variable.↩︎\nWe’ll see this in action in a bit↩︎\nEven though for “literal string” like in the examples above, as opposed to regular expressions, this is not strictly necessary, it’s good habit to always quote.↩︎\nWe’ll learn more about regular expressions in Week 4.↩︎"
  },
  {
    "objectID": "week02/w2_exercises.html",
    "href": "week02/w2_exercises.html",
    "title": "Week 2 Exercises",
    "section": "",
    "text": "The following are some of the exercises from Chapter 1 of the CSB book."
  },
  {
    "objectID": "week02/w2_exercises.html#getting-set-up",
    "href": "week02/w2_exercises.html#getting-set-up",
    "title": "Week 2 Exercises",
    "section": "Getting set up",
    "text": "Getting set up\nYou should already have the book’s GitHub repository with exercise files in your personal dir within /fs/ess/PAS2880 from Thursday’s class. (If not, cd to /fs/ess/PAS2880/users/$USER, and run git clone https://github.com/CSB-book/CSB.git — this downloads the CSB directory referred to in the first step of the first exercise.)"
  },
  {
    "objectID": "week02/w2_exercises.html#intermezzo-1.1",
    "href": "week02/w2_exercises.html#intermezzo-1.1",
    "title": "Week 2 Exercises",
    "section": "Intermezzo 1.1",
    "text": "Intermezzo 1.1\n(a) Go to /fs/ess/PAS2880/users/$USER\n(b) Navigate to the sandbox directory within the CSB/unix directory.\n(c) Use a relative path to go to the data directory within the python directory.\n(d) Use an absolute path to go to the sandbox directory within CSB/python.\n(e) Return to the data directory within the python directory.\n\n\nShow hints\n\nWe didn’t see this in class, but the CSB book (page 21) mentions a shortcut you can use with cd to go back to the dir you were in previously (a little like a Browser’s “back” button)."
  },
  {
    "objectID": "week02/w2_exercises.html#intermezzo-1.2",
    "href": "week02/w2_exercises.html#intermezzo-1.2",
    "title": "Week 2 Exercises",
    "section": "Intermezzo 1.2",
    "text": "Intermezzo 1.2\nTo familiarize yourself with using basic Unix commands, try the following:\n(a) Go to the data directory within CSB/unix.\n(b) How many lines are in the file Marra2014_data.fasta?\n(c) Create the empty file toremove.txt in the CSB/unix/sandbox directory without leaving the current directory.\n\n\nShow hints\n\nWe didn’t see this in class, but the CSB book (page 23) demonstrates the use of the touch command to create a new, empty file.\n\n(d) List the contents of the directory unix/sandbox.\n(e) Remove the file toremove.txt."
  },
  {
    "objectID": "week02/w2_exercises.html#intermezzo-1.3",
    "href": "week02/w2_exercises.html#intermezzo-1.3",
    "title": "Week 2 Exercises",
    "section": "Intermezzo 1.3",
    "text": "Intermezzo 1.3\n(a) If we order all species names (fifth column) of Pacifici2013_data.csv (in CSB/unix/data/) in alphabetical order, which is the first species? And which is the last?\n\n\nShow hints\n\nYou can either first select the 5th column using cut and then use sort, or directly tell the sort command which column to sort by.\nIn either case, you’ll also need to specify the column delimiter (if you use the latter approach, check the book for how to do that with sort).\nTo view just the first or the last line so you don’t have to scroll to get your answer, pipe to head or tail.\n\n(b) How many families are represented in the database?\n\n\nShow hints\n\n\nCheck the first line of the file to see which column contains the family, and then select the relevant column with cut.\nUse the “tail trick” we saw in class to exclude the first line.\nRemember to sort before using uniq."
  },
  {
    "objectID": "week02/w2_exercises.html#exercise-1.10.1-next-generation-sequencing-data",
    "href": "week02/w2_exercises.html#exercise-1.10.1-next-generation-sequencing-data",
    "title": "Week 2 Exercises",
    "section": "Exercise 1.10.1: Next-Generation Sequencing Data",
    "text": "Exercise 1.10.1: Next-Generation Sequencing Data\nIn this exercise, we work with next generation sequencing (NGS) data. Unix is excellent at manipulating the huge FASTA files that are generated in NGS experiments.\nFASTA files contain sequence data in text format. Each sequence segment is preceded by a single-line description. The first character of the description line is a “greater than” sign (&gt;).\nThe NGS data set we will be working with was published by Marra and DeWoody (2014), who investigated the immunogenetic repertoire of rodents. You will find the sequence file Marra2014_data.fasta in the directory CSB/unix/data. The file contains sequence segments (contigs) of variable size. The description of each contig provides its length, the number of reads that contributed to the contig, its isogroup (representing the collection of alternative splice products of a possible gene), and the isotig status.\n1. Change directory to CSB/unix/sandbox.\n2. What is the size of the file Marra2014_data.fasta?\n\n\nShow hints\n\nRecall that you can see file sizes by using specific options to the ls command.\n\n3. Create a copy of Marra2014_data.fasta in the sandbox, and name it my_file.fasta.\n4. How many “contigs” (FASTA entries, in this case) are classified as isogroup00036?\n\n\nShow hints\n\nIs there a grep option that counts the number of occurrences? Alternatively, you can pass the output of grep to wc -l.\n\n5. Modify my_file.fasta to replace the original “two-spaces” delimiter with a comma (i.e. don’t just print the output to screen, but end up with a modified file). You’ll probably want to take a look at the “output file hints” below to see how you can end up with modified file contents.\n\n\nShow output file hints\n\n Due to the “streaming” nature of Unix commands, we can’t write output to a file that also serves as input (see here). So the following is not possible:\ncat myfile.txt | tr \"a\" \"b\" &gt; myfile.txt # Don't do this! \nIn this case, you’ll have to save the output in a different file. Then, if you do want to end up with a modified original file, you can overwrite the original file using mv.\n\n\n\nShow other hints\n\n\nIn the file, the information on each contig is separated by two spaces:\n&gt;contig00001  length=527  numreads=2  ...\nWe would like to obtain:\n&gt;contig00001,length=527,numreads=2,...\nUse cat to print the file, and substitute the spaces using the command tr. Note that you’ll first have to reduce the two spaces two one – can you remember an option to do that?\n\n\n6. How many unique isogroups are in the file?\n\n\nShow hints\n\nYou can use grep to match any line containing the word isogroup. Then, use cut to isolate the part detailing the isogroup. Finally, you want to remove the duplicates, and count.\n\n7. Which contig has the highest number of reads (numreads)? How many reads does it have?\n\n\nShow hints\n\nUse a combination of grep and cut to extract the contig names and read counts. The command sort allows you to choose the delimiter and to order numerically — we didn’t see those sort options in class, so check the book for details."
  },
  {
    "objectID": "week02/w2_exercises.html#exercise-1.10.2-hormone-levels-in-baboons",
    "href": "week02/w2_exercises.html#exercise-1.10.2-hormone-levels-in-baboons",
    "title": "Week 2 Exercises",
    "section": "Exercise 1.10.2: Hormone Levels in Baboons",
    "text": "Exercise 1.10.2: Hormone Levels in Baboons\nGesquiere et al. (2011) studied hormone levels in the blood of baboons. The data file is in CSB/unix/data/Gesquiere2011_data.csv.\nEvery individual was sampled several times. How many times were the levels of individuals 3 and 27 recorded?\n\n\nShow hints\n\n\nYou can first use cut to extract just the maleID column from the file.\nTo match an individual (3 or 27), you can use grep with the -w option to match whole “words” only: this will prevent and individual ID like “13” to match when you search for “3”."
  },
  {
    "objectID": "week02/w2_exercises.html#solutions",
    "href": "week02/w2_exercises.html#solutions",
    "title": "Week 2 Exercises",
    "section": "Solutions",
    "text": "Solutions\n\nIntermezzo 1.1\n\n\nSolution\n\n(a) Go to your home directory. Go to /fs/ess/PAS2700/users/$USER.\ncd /fs/ess/PAS2700/users/$USER # To home would have been: \"cd ∼\"\n(b) Navigate to the sandbox directory within the CSB/unix directory.\ncd CSB/unix/sandbox\n(c) Use a relative path to go to the data directory within the python directory.\ncd ../../python/data\n(d) Use an absolute path to go to the sandbox directory within python.\ncd /fs/ess/PAS2700/users/$USER/CSB/python/sandbox\n(e) Return to the data directory within the python directory.\n\nWith cd -:\n# The '-' shortcut for cd will move you back to the previously visited dir\n# (Note: you can't keep going back with this: using it a second time will toggle you \"forward\" again.)\ncd -\nUsing a relative path:\ncd ../data\n\n\n\n\n\nIntermezzo 1.2\n\n\nSolution\n\n(a) Go to the data directory within CSB/unix.\ncd /fs/ess/PAS2700/users/$USER/CSB/unix/data\n(b) How many lines are in file Marra2014_data.fasta?\nwc -l Marra2014_data.fasta\n9515 Marra2014_data.fasta\n(c) Create the empty file toremove.txt in the CSB/unix/sandbox directory without leaving the current directory.\ntouch ../sandbox/toremove.txt\n(d) List the contents of the directory unix/sandbox.\nls ../sandbox\nPapers and reviews  toremove.txt\n(e) Remove the file toremove.txt.\nrm ../sandbox/toremove.txt\n\n\n\n\nIntermezzo 1.3\n\n\nSolution\n\n(a) If we order all species names (fifth column) of Pacifici2013_data.csv in alphabetical order, which is the first species? And which is the last?\n# First species:\ncut -d \";\" -f 5 Pacifici2013_data.csv | sort | head -n 1\nAbditomys latidens\n# Last species:\ncut -d \";\" -f 5 Pacifici2013_data.csv | sort | tail -n 1\nZyzomys woodwardi\n# Or, using sort directly, but then you get all columns unless you pipe to cut:\nsort -t \";\" -k 5 Pacifici2013_data.csv | head -n 1\n42641;Rodentia;Muridae;Abditomys;Abditomys latidens;268.09;PanTHERIA;no information;no information;no information;no information;no information;no information;639.6318318208;Mean_family_same_body_mass\n(b) How many families are represented in the database?\ncut -d \";\" -f 3 Pacifici2013_data.csv | tail -n +2 | sort | uniq | wc -l\n152\n\n\n\n\nExercise 1.10.1: Next-Generation Sequencing Data\n\n\n1. Change directory to CSB/unix/sandbox.\n\ncd /fs/ess/PAS2700/users/$USER/CSB/unix/sandbox\n\n\n\n2. What is the size of the file Marra2014_data.fasta?\n\nls -lh ../data/Marra2014_data.fasta\n-rw-rw----+ 1 jelmer PAS0471 553K Feb 24 20:30 ../data/Marra2014_data.fasta\nAlternatively, the command du (disk usage) can be used for more compact output:\ndu -h ../data/Marra2014_data.fasta \n560K    ../data/Marra2014_data.fasta\n\n\n\n3. Create a copy of Marra2014_data.fasta in the sandbox, and name it my_file.fasta.\n\ncp ../data/Marra2014_data.fasta my_file.fasta\n\n\n\n4. How many contigs are classified as isogroup00036?\n\nTo count the occurrences of a given string, use grep with the option -c:\ngrep -c isogroup00036 my_file.fasta \n16\nSlightly less efficient is to use a “regular” grep and then pipe to wc -l:\ngrep isogroup00036 my_file.fasta | wc -l\n16\n\n\n\n5. Replace the original “two-spaces” delimiter with a comma.\n\n\nWe use the tr option -s (squeeze) to change two spaces two one, and then replace the space with a ,. Importantly, we also write the output to a new file (see the Hints for details):\ncat my_file.fasta | tr -s ' ' ',' &gt; my_file.tmp\nIf we want to change the original file, we can now overwrite it as follows:\nmv my_file.tmp my_file.fasta\nLet’s take a look to check whether out delimiter replacement worked:\ngrep \"&gt;\" my_file.fasta | head\n&gt;contig00001,length=527,numreads=2,gene=isogroup00001,status=it_thresh\n&gt;contig00002,length=551,numreads=8,gene=isogroup00001,status=it_thresh\n&gt;contig00003,length=541,numreads=2,gene=isogroup00001,status=it_thresh\n&gt;contig00004,length=291,numreads=3,gene=isogroup00001,status=it_thresh\n&gt;contig00005,length=580,numreads=12,gene=isogroup00001,status=it_thresh\n&gt;contig00006,length=3288,numreads=35,gene=isogroup00001,status=it_thresh\n&gt;contig00008,length=1119,numreads=10,gene=isogroup00001,status=it_thresh\n&gt;contig00010,length=202,numreads=4,gene=isogroup00001,status=it_thresh\n&gt;contig00011,length=5563,numreads=61,gene=isogroup00001,status=it_thresh\n&gt;contig00012,length=824,numreads=10,gene=isogroup00001,status=it_thresh\n\n\n\n\n6. How many unique isogroups are in the file?\n\n\nFirst, searching for &gt; with grep will extract all lines with contig information:\ngrep '&gt;' my_file.fasta | head -n 2\n&gt;contig00001,length=527,numreads=2,gene=isogroup00001,status=it_thresh\n&gt;contig00002,length=551,numreads=8,gene=isogroup00001,status=it_thresh\nNow, add cut to extract the 4th column:\ngrep '&gt;' my_file.fasta | cut -d ',' -f 4 | head -n 2\ngene=isogroup00001\ngene=isogroup00001\nFinally, add sort -&gt; uniq -&gt; wc -l to count the number of unique occurrences:\ngrep '&gt;' my_file.fasta | cut -d ',' -f 4 | sort | uniq | wc -l\n43\n\n\n\n\n7. Which contig has the highest number of reads (numreads)? How many reads does it have?\n\n\nFirst, we need to isolate the number of reads as well as the contig names. We can use a combination of grep and cut:\ngrep '&gt;' my_file.fasta | cut -d ',' -f 1,3 | head -n 3\n&gt;contig00001,numreads=2\n&gt;contig00002,numreads=8\n&gt;contig00003,numreads=2\nNow we want to sort according to the number of reads. However, the number of reads is part of a more complex string. We can use -t '=' to split according to the = sign, and then take the second column (-k 2) to sort numerically (-n):\ngrep '&gt;' my_file.fasta | cut -d ',' -f 1,3 | sort -t '=' -k 2 -n | head -n 5\n&gt;contig00089,numreads=1\n&gt;contig00176,numreads=1\n&gt;contig00210,numreads=1\n&gt;contig00001,numreads=2\n&gt;contig00003,numreads=2\nAdding the sort option -r, we can sort in reverse order, which tells us that contig00302 has the highest coverage, with 3330 reads:\ngrep '&gt;' my_file.fasta | cut -d ',' -f 1,3 | sort -t '=' -k 2 -n -r | head -n 1\n&gt;contig00302,numreads=3330\n\n\n\n\n\nExercise 1.10.2: Hormone Levels in Baboons\n\n\nHow many times were the levels of individuals 3 and 27 recorded?\n\n\nFirst, let’s move back into the data dir:\ncd ../data\nNext, let’s take a look at the structure of the file:\nhead -n 3 Gesquiere2011_data.csv\nmaleID        GC      T\n1     66.9    64.57\n1     51.09   35.57\nWe want to extract all the rows in which the first column is 3 (or 27), and count them. To extract only the first column, we can use cut:\ncut -f 1 Gesquiere2011_data.csv | head -n 3\nmaleID\n1\n1\n\n\nThen we can pipe the results to grep -c to count the number of occurrences (note the option -w to match whole “words” – this will make it match 3 but not 13 or 23):\n# For maleID 3\ncut -f 1 Gesquiere2011_data.csv | grep -c -w 3\n61\n# For maleID 27\ncut -f 1 Gesquiere2011_data.csv | grep -c -w 27\n5"
  }
]