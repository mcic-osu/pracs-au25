[
  {
    "objectID": "week02/removed.html",
    "href": "week02/removed.html",
    "title": "Removed",
    "section": "",
    "text": "Because spaces are special characters used to separate commands from options and arguments, etc., using them in file names is inconvenient at best:\n# You should be in /fs/ess/PAS2700/users/$USER/CSB/unix/sandbox\nls\n\ncd Papers and reviews     # NOPE!\n\ncd Papers\\ and\\ reviews   # \\ to escape each individual space\ncd \"Papers and reviews\"   # Quotes to escape special characters\nWe’ll talk more about spaces and file names in week 2.\n\n\n\nUse uniq -c to count occurrences of each unique value (more on this in week 4):\ncut -d \";\" -f 2 Pacifici2013_data.csv | tail -n +2 | sort | uniq -c\n   54 Afrosoricida\n  280 Carnivora\n  325 Cetartiodactyla\n 1144 Chiroptera\n   21 Cingulata\ncut -d \";\" -f 2 Pacifici2013_data.csv | tail -n +2 | sort | uniq -c | sort -nr\n 2220 Rodentia\n 1144 Chiroptera\n  442 Eulipotyphla\n  418 Primates\n\n\n\n\nLet’s say we want a list of animals sorted by body weight…\ncd ../sandbox/\ntail -n +2 ../data/Pacifici2013_data.csv\nIn the following commands, we will build up our “pipeline”.\nFirst, we print the file with the exception of the first line (tail -n +2) and then pipe that into cut to select the columns of interest — and to check our partial pipeline, end with a head command to only print the first lines:\n# Using head just to check the output\ntail -n +2 ../data/Pacifici2013_data.csv |\n    cut -d \";\" -f 5-6 | head\nSecond, we’ll add a tr command to change the column delimiter:\ntail -n +2 ../data/Pacifici2013_data.csv |\n    cut -d \";\" -f 5-6 | tr \";\" \" \" | head\nFinally, we’ll sort in reverse numerical order with sort, and redirect the output to a file:\ntail -n +2 ../data/Pacifici2013_data.csv |\n    cut -d \";\" -f 5-6 | tr \";\" \" \" | sort -r -n -k 3 &gt; BodyM.csv\nLet’s take a look at the output:\nhead BodyM.csv"
  },
  {
    "objectID": "week02/removed.html#bonus-material",
    "href": "week02/removed.html#bonus-material",
    "title": "Removed",
    "section": "",
    "text": "Because spaces are special characters used to separate commands from options and arguments, etc., using them in file names is inconvenient at best:\n# You should be in /fs/ess/PAS2700/users/$USER/CSB/unix/sandbox\nls\n\ncd Papers and reviews     # NOPE!\n\ncd Papers\\ and\\ reviews   # \\ to escape each individual space\ncd \"Papers and reviews\"   # Quotes to escape special characters\nWe’ll talk more about spaces and file names in week 2.\n\n\n\nUse uniq -c to count occurrences of each unique value (more on this in week 4):\ncut -d \";\" -f 2 Pacifici2013_data.csv | tail -n +2 | sort | uniq -c\n   54 Afrosoricida\n  280 Carnivora\n  325 Cetartiodactyla\n 1144 Chiroptera\n   21 Cingulata\ncut -d \";\" -f 2 Pacifici2013_data.csv | tail -n +2 | sort | uniq -c | sort -nr\n 2220 Rodentia\n 1144 Chiroptera\n  442 Eulipotyphla\n  418 Primates\n\n\n\n\nLet’s say we want a list of animals sorted by body weight…\ncd ../sandbox/\ntail -n +2 ../data/Pacifici2013_data.csv\nIn the following commands, we will build up our “pipeline”.\nFirst, we print the file with the exception of the first line (tail -n +2) and then pipe that into cut to select the columns of interest — and to check our partial pipeline, end with a head command to only print the first lines:\n# Using head just to check the output\ntail -n +2 ../data/Pacifici2013_data.csv |\n    cut -d \";\" -f 5-6 | head\nSecond, we’ll add a tr command to change the column delimiter:\ntail -n +2 ../data/Pacifici2013_data.csv |\n    cut -d \";\" -f 5-6 | tr \";\" \" \" | head\nFinally, we’ll sort in reverse numerical order with sort, and redirect the output to a file:\ntail -n +2 ../data/Pacifici2013_data.csv |\n    cut -d \";\" -f 5-6 | tr \";\" \" \" | sort -r -n -k 3 &gt; BodyM.csv\nLet’s take a look at the output:\nhead BodyM.csv"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About this site and course",
    "section": "",
    "text": "This is the GitHub website for the course Practical Computing Skills for Omics Data (PLNTPTH 5006), a 3-credit course taught at Ohio State University during the Fall semester of 2025.\nThe course is taught by by Jelmer Poelstra from OSU’s Molecular and Cellular Imaging Center (MCIC) for the Department of Plant Pathology.\n\n\nCourse description\nAs datasets have rapidly grown larger in biology, coding has been recognized as an increasingly important skill for biologists. This is especially true in “omics” research with data from e.g. genomics and transcriptomics, which typically cannot be analyzed on a desktop computer, where cutting-edge software has a command-line interface, and where workflows can include many steps that need to be coordinated.\nIn this course, students will gain hands-on experience with a set of general and versatile tools for data-intensive research in omics and beyond. The course will focus on foundational skills such as working in the Unix shell and writing shell scripts, managing software and submitting jobs at a compute cluster (the Ohio Supercomputer Center), and building flexible, automated workflows. Additionally, the course will cover reproducibly organizing, documenting, and version-controlling research projects. Taken together, this course will allow students to reproduce their own research, and enable others to reproduce their research, with as little as a single command.\n\n\n\nMore information\n\nTopics taught\n\nUnix shell: basics, scripting, running command-line programs\nR: Basics, data wrangling & visualization, Quarto, and specifics to omics data\nSupercomputer usage: basics, software, and running Slurm batch jobs\nVersion control with Git and GitHub\nReproducibility including project documentation with Markdown, project file organization, and data management\nRunning and building Nextflow pipelines\n\n\n\nCourse books\n\nAllesina S, Wilmes M (2019). Computing Skills for Biologists. Princeton UP.\nBuffalo V (2015). Bioinformatics Data Skills: Reproducible and Robust Research with Open Source Tools. O’Reilly Media, Inc.\n\n\n\nCarmenCanvas website\nIf you are a student in this course, you should also refer to the CarmenCanvas site for this course (link TBA).\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "week01/w1_exercises.html",
    "href": "week01/w1_exercises.html",
    "title": "Week 1 Exercises",
    "section": "",
    "text": "The following are some of the exercises from Chapter 1 of the CSB book."
  },
  {
    "objectID": "week01/w1_exercises.html#getting-set-up",
    "href": "week01/w1_exercises.html#getting-set-up",
    "title": "Week 1 Exercises",
    "section": "Getting set up",
    "text": "Getting set up\nYou should already have the book’s GitHub repository with exercise files in your personal dir within /fs/ess/PAS2700 from Thursday’s class. (If not, cd to /fs/ess/PAS2700/users/$USER, and run git clone https://github.com/CSB-book/CSB.git — this downloads the CSB directory referred to in the first step of the first exercise.)"
  },
  {
    "objectID": "week01/w1_exercises.html#intermezzo-1.1",
    "href": "week01/w1_exercises.html#intermezzo-1.1",
    "title": "Week 1 Exercises",
    "section": "Intermezzo 1.1",
    "text": "Intermezzo 1.1\n(a) Go to your home directory. Go to /fs/ess/PAS2700/users/$USER\n(b) Navigate to the sandbox directory within the CSB/unix directory.\n(c) Use a relative path to go to the data directory within the python directory.\n(d) Use an absolute path to go to the sandbox directory within CSB/python.\n(e) Return to the data directory within the python directory.\n\n\nShow hints\n\nWe didn’t see this in class, but the CSB book (page 21) mentions a shortcut you can use with cd to go back to the dir you were in previously (a little like a Browser’s “back” button)."
  },
  {
    "objectID": "week01/w1_exercises.html#intermezzo-1.2",
    "href": "week01/w1_exercises.html#intermezzo-1.2",
    "title": "Week 1 Exercises",
    "section": "Intermezzo 1.2",
    "text": "Intermezzo 1.2\nTo familiarize yourself with using basic Unix commands, try the following:\n(a) Go to the data directory within CSB/unix.\n(b) How many lines are in the file Marra2014_data.fasta?\n(c) Create the empty file toremove.txt in the CSB/unix/sandbox directory without leaving the current directory.\n\n\nShow hints\n\nWe didn’t see this in class, but the CSB book (page 23) demonstrates the use of the touch command to create a new, empty file.\n\n(d) List the contents of the directory unix/sandbox.\n(e) Remove the file toremove.txt."
  },
  {
    "objectID": "week01/w1_exercises.html#intermezzo-1.3",
    "href": "week01/w1_exercises.html#intermezzo-1.3",
    "title": "Week 1 Exercises",
    "section": "Intermezzo 1.3",
    "text": "Intermezzo 1.3\n(a) If we order all species names (fifth column) of Pacifici2013_data.csv (in CSB/unix/data/) in alphabetical order, which is the first species? And which is the last?\n\n\nShow hints\n\nYou can either first select the 5th column using cut and then use sort, or directly tell the sort command which column to sort by.\nIn either case, you’ll also need to specify the column delimiter (if you use the latter approach, check the book for how to do that with sort).\nTo view just the first or the last line so you don’t have to scroll to get your answer, pipe to head or tail.\n\n(b) How many families are represented in the database?\n\n\nShow hints\n\n\nCheck the first line of the file to see which column contains the family, and then select the relevant column with cut.\nUse the “tail trick” we saw in class to exclude the first line.\nRemember to sort before using uniq."
  },
  {
    "objectID": "week01/w1_exercises.html#exercise-1.10.1-next-generation-sequencing-data",
    "href": "week01/w1_exercises.html#exercise-1.10.1-next-generation-sequencing-data",
    "title": "Week 1 Exercises",
    "section": "Exercise 1.10.1: Next-Generation Sequencing Data",
    "text": "Exercise 1.10.1: Next-Generation Sequencing Data\nIn this exercise, we work with next generation sequencing (NGS) data. Unix is excellent at manipulating the huge FASTA files that are generated in NGS experiments.\nFASTA files contain sequence data in text format. Each sequence segment is preceded by a single-line description. The first character of the description line is a “greater than” sign (&gt;).\nThe NGS data set we will be working with was published by Marra and DeWoody (2014), who investigated the immunogenetic repertoire of rodents. You will find the sequence file Marra2014_data.fasta in the directory CSB/unix/data. The file contains sequence segments (contigs) of variable size. The description of each contig provides its length, the number of reads that contributed to the contig, its isogroup (representing the collection of alternative splice products of a possible gene), and the isotig status.\n1. Change directory to CSB/unix/sandbox.\n2. What is the size of the file Marra2014_data.fasta?\n\n\nShow hints\n\nRecall that you can see file sizes by using specific options to the ls command.\n\n3. Create a copy of Marra2014_data.fasta in the sandbox, and name it my_file.fasta.\n4. How many “contigs” (FASTA entries, in this case) are classified as isogroup00036?\n\n\nShow hints\n\nIs there a grep option that counts the number of occurrences? Alternatively, you can pass the output of grep to wc -l.\n\n5. Modify my_file.fasta to replace the original “two-spaces” delimiter with a comma (i.e. don’t just print the output to screen, but end up with a modified file). You’ll probably want to take a look at the “output file hints” below to see how you can end up with modified file contents.\n\n\nShow output file hints\n\n Due to the “streaming” nature of Unix commands, we can’t write output to a file that also serves as input (see here). So the following is not possible:\ncat myfile.txt | tr \"a\" \"b\" &gt; myfile.txt # Don't do this! \nIn this case, you’ll have to save the output in a different file. Then, if you do want to end up with a modified original file, you can overwrite the original file using mv.\n\n\n\nShow other hints\n\n\nIn the file, the information on each contig is separated by two spaces:\n&gt;contig00001  length=527  numreads=2  ...\nWe would like to obtain:\n&gt;contig00001,length=527,numreads=2,...\nUse cat to print the file, and substitute the spaces using the command tr. Note that you’ll first have to reduce the two spaces two one – can you remember an option to do that?\n\n\n6. How many unique isogroups are in the file?\n\n\nShow hints\n\nYou can use grep to match any line containing the word isogroup. Then, use cut to isolate the part detailing the isogroup. Finally, you want to remove the duplicates, and count.\n\n7. Which contig has the highest number of reads (numreads)? How many reads does it have?\n\n\nShow hints\n\nUse a combination of grep and cut to extract the contig names and read counts. The command sort allows you to choose the delimiter and to order numerically — we didn’t see those sort options in class, so check the book for details."
  },
  {
    "objectID": "week01/w1_exercises.html#exercise-1.10.2-hormone-levels-in-baboons",
    "href": "week01/w1_exercises.html#exercise-1.10.2-hormone-levels-in-baboons",
    "title": "Week 1 Exercises",
    "section": "Exercise 1.10.2: Hormone Levels in Baboons",
    "text": "Exercise 1.10.2: Hormone Levels in Baboons\nGesquiere et al. (2011) studied hormone levels in the blood of baboons. The data file is in CSB/unix/data/Gesquiere2011_data.csv.\nEvery individual was sampled several times. How many times were the levels of individuals 3 and 27 recorded?\n\n\nShow hints\n\n\nYou can first use cut to extract just the maleID column from the file.\nTo match an individual (3 or 27), you can use grep with the -w option to match whole “words” only: this will prevent and individual ID like “13” to match when you search for “3”."
  },
  {
    "objectID": "week01/w1_exercises.html#solutions",
    "href": "week01/w1_exercises.html#solutions",
    "title": "Week 1 Exercises",
    "section": "Solutions",
    "text": "Solutions\n\nIntermezzo 1.1\n\n\nSolution\n\n(a) Go to your home directory. Go to /fs/ess/PAS2700/users/$USER.\ncd /fs/ess/PAS2700/users/$USER # To home would have been: \"cd ∼\"\n(b) Navigate to the sandbox directory within the CSB/unix directory.\ncd CSB/unix/sandbox\n(c) Use a relative path to go to the data directory within the python directory.\ncd ../../python/data\n(d) Use an absolute path to go to the sandbox directory within python.\ncd /fs/ess/PAS2700/users/$USER/CSB/python/sandbox\n(e) Return to the data directory within the python directory.\n\nWith cd -:\n# The '-' shortcut for cd will move you back to the previously visited dir\n# (Note: you can't keep going back with this: using it a second time will toggle you \"forward\" again.)\ncd -\nUsing a relative path:\ncd ../data\n\n\n\n\n\nIntermezzo 1.2\n\n\nSolution\n\n(a) Go to the data directory within CSB/unix.\ncd /fs/ess/PAS2700/users/$USER/CSB/unix/data\n(b) How many lines are in file Marra2014_data.fasta?\nwc -l Marra2014_data.fasta\n9515 Marra2014_data.fasta\n(c) Create the empty file toremove.txt in the CSB/unix/sandbox directory without leaving the current directory.\ntouch ../sandbox/toremove.txt\n(d) List the contents of the directory unix/sandbox.\nls ../sandbox\nPapers and reviews  toremove.txt\n(e) Remove the file toremove.txt.\nrm ../sandbox/toremove.txt\n\n\n\n\nIntermezzo 1.3\n\n\nSolution\n\n(a) If we order all species names (fifth column) of Pacifici2013_data.csv in alphabetical order, which is the first species? And which is the last?\n# First species:\ncut -d \";\" -f 5 Pacifici2013_data.csv | sort | head -n 1\nAbditomys latidens\n# Last species:\ncut -d \";\" -f 5 Pacifici2013_data.csv | sort | tail -n 1\nZyzomys woodwardi\n# Or, using sort directly, but then you get all columns unless you pipe to cut:\nsort -t \";\" -k 5 Pacifici2013_data.csv | head -n 1\n42641;Rodentia;Muridae;Abditomys;Abditomys latidens;268.09;PanTHERIA;no information;no information;no information;no information;no information;no information;639.6318318208;Mean_family_same_body_mass\n(b) How many families are represented in the database?\ncut -d \";\" -f 3 Pacifici2013_data.csv | tail -n +2 | sort | uniq | wc -l\n152\n\n\n\n\nExercise 1.10.1: Next-Generation Sequencing Data\n\n\n1. Change directory to CSB/unix/sandbox.\n\ncd /fs/ess/PAS2700/users/$USER/CSB/unix/sandbox\n\n\n\n2. What is the size of the file Marra2014_data.fasta?\n\nls -lh ../data/Marra2014_data.fasta\n-rw-rw----+ 1 jelmer PAS0471 553K Feb 24 20:30 ../data/Marra2014_data.fasta\nAlternatively, the command du (disk usage) can be used for more compact output:\ndu -h ../data/Marra2014_data.fasta \n560K    ../data/Marra2014_data.fasta\n\n\n\n3. Create a copy of Marra2014_data.fasta in the sandbox, and name it my_file.fasta.\n\ncp ../data/Marra2014_data.fasta my_file.fasta\n\n\n\n4. How many contigs are classified as isogroup00036?\n\nTo count the occurrences of a given string, use grep with the option -c:\ngrep -c isogroup00036 my_file.fasta \n16\nSlightly less efficient is to use a “regular” grep and then pipe to wc -l:\ngrep isogroup00036 my_file.fasta | wc -l\n16\n\n\n\n5. Replace the original “two-spaces” delimiter with a comma.\n\n\nWe use the tr option -s (squeeze) to change two spaces two one, and then replace the space with a ,. Importantly, we also write the output to a new file (see the Hints for details):\ncat my_file.fasta | tr -s ' ' ',' &gt; my_file.tmp\nIf we want to change the original file, we can now overwrite it as follows:\nmv my_file.tmp my_file.fasta\nLet’s take a look to check whether out delimiter replacement worked:\ngrep \"&gt;\" my_file.fasta | head\n&gt;contig00001,length=527,numreads=2,gene=isogroup00001,status=it_thresh\n&gt;contig00002,length=551,numreads=8,gene=isogroup00001,status=it_thresh\n&gt;contig00003,length=541,numreads=2,gene=isogroup00001,status=it_thresh\n&gt;contig00004,length=291,numreads=3,gene=isogroup00001,status=it_thresh\n&gt;contig00005,length=580,numreads=12,gene=isogroup00001,status=it_thresh\n&gt;contig00006,length=3288,numreads=35,gene=isogroup00001,status=it_thresh\n&gt;contig00008,length=1119,numreads=10,gene=isogroup00001,status=it_thresh\n&gt;contig00010,length=202,numreads=4,gene=isogroup00001,status=it_thresh\n&gt;contig00011,length=5563,numreads=61,gene=isogroup00001,status=it_thresh\n&gt;contig00012,length=824,numreads=10,gene=isogroup00001,status=it_thresh\n\n\n\n\n6. How many unique isogroups are in the file?\n\n\nFirst, searching for &gt; with grep will extract all lines with contig information:\ngrep '&gt;' my_file.fasta | head -n 2\n&gt;contig00001,length=527,numreads=2,gene=isogroup00001,status=it_thresh\n&gt;contig00002,length=551,numreads=8,gene=isogroup00001,status=it_thresh\nNow, add cut to extract the 4th column:\ngrep '&gt;' my_file.fasta | cut -d ',' -f 4 | head -n 2\ngene=isogroup00001\ngene=isogroup00001\nFinally, add sort -&gt; uniq -&gt; wc -l to count the number of unique occurrences:\ngrep '&gt;' my_file.fasta | cut -d ',' -f 4 | sort | uniq | wc -l\n43\n\n\n\n\n7. Which contig has the highest number of reads (numreads)? How many reads does it have?\n\n\nFirst, we need to isolate the number of reads as well as the contig names. We can use a combination of grep and cut:\ngrep '&gt;' my_file.fasta | cut -d ',' -f 1,3 | head -n 3\n&gt;contig00001,numreads=2\n&gt;contig00002,numreads=8\n&gt;contig00003,numreads=2\nNow we want to sort according to the number of reads. However, the number of reads is part of a more complex string. We can use -t '=' to split according to the = sign, and then take the second column (-k 2) to sort numerically (-n):\ngrep '&gt;' my_file.fasta | cut -d ',' -f 1,3 | sort -t '=' -k 2 -n | head -n 5\n&gt;contig00089,numreads=1\n&gt;contig00176,numreads=1\n&gt;contig00210,numreads=1\n&gt;contig00001,numreads=2\n&gt;contig00003,numreads=2\nAdding the sort option -r, we can sort in reverse order, which tells us that contig00302 has the highest coverage, with 3330 reads:\ngrep '&gt;' my_file.fasta | cut -d ',' -f 1,3 | sort -t '=' -k 2 -n -r | head -n 1\n&gt;contig00302,numreads=3330\n\n\n\n\n\nExercise 1.10.2: Hormone Levels in Baboons\n\n\nHow many times were the levels of individuals 3 and 27 recorded?\n\n\nFirst, let’s move back into the data dir:\ncd ../data\nNext, let’s take a look at the structure of the file:\nhead -n 3 Gesquiere2011_data.csv\nmaleID        GC      T\n1     66.9    64.57\n1     51.09   35.57\nWe want to extract all the rows in which the first column is 3 (or 27), and count them. To extract only the first column, we can use cut:\ncut -f 1 Gesquiere2011_data.csv | head -n 3\nmaleID\n1\n1\n\n\nThen we can pipe the results to grep -c to count the number of occurrences (note the option -w to match whole “words” – this will make it match 3 but not 13 or 23):\n# For maleID 3\ncut -f 1 Gesquiere2011_data.csv | grep -c -w 3\n61\n# For maleID 27\ncut -f 1 Gesquiere2011_data.csv | grep -c -w 27\n5"
  },
  {
    "objectID": "week01/w1_osc-setup.html",
    "href": "week01/w1_osc-setup.html",
    "title": "Pre-course assignment: OSC access",
    "section": "",
    "text": "Overview\nBefore the course starts, you should make sure that you have access to the Ohio Supercomputer Center (OSC), and the OSC Project for this course (PAS2700).\n\n\nBackground\nMuch of the coding during this course will be done through your browser at the Ohio Supercomputer Center (OSC). To access OSC, you will need to have an account. This course has its own OSC Project, and membership of this specific project will allow you to access our shared files and reserve “compute nodes”.\n\n\nWhat you should do\nAfter completing the pre-course survey, you will receive an invitation email from OSC referencing the course project number PAS2700:\n\nIf you don’t have an OSC account yet\nFollow the instructions in the email to sign up for OSC and accept the invitation.\nIf you already have an OSC account\nThe email will likely tell you that you have been added to the project and don’t have to do anything.\nIn either case, check whether you can log in\nGo to https://ondemand.osc.edu and log in by typing your username and password on the left-hand side of the page. If you just created your account, it may take up to half an hour or so before you can log in.\n\nIf you have any questions about this or run into problems, don’t hesitate to contact Jelmer.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "week01/w1_course-intro.html#introductions-jelmer",
    "href": "week01/w1_course-intro.html#introductions-jelmer",
    "title": "Course Intro",
    "section": "Introductions: Jelmer",
    "text": "Introductions: Jelmer\n\nLead of Bioinformatics and Microscopy at MCIC in Wooster since 2020\n\nThe MCIC is a CFAES core facility that provides services in the areas of molecular biology, high-throughput sequencing, bioinformatics, and microscopy.\nThe majority of my time is spent providing research assistance,\nworking with grad students and postdocs on mostly genomic & transcriptomic data\nI also teach: some courses, workshops, Code Club (https://osu-codeclub.github.io)\n\n\n\n\n\nBackground in animal evolutionary genomics & speciation\n\n\n\nIn my free time, I enjoy bird watching – locally & all across the world"
  },
  {
    "objectID": "week01/w1_course-intro.html#introductions-you",
    "href": "week01/w1_course-intro.html#introductions-you",
    "title": "Course Intro",
    "section": "Introductions: You",
    "text": "Introductions: You\n\nName\nLab and Department\nResearch interests and/or current research topics\nSomething about you that is not work-related"
  },
  {
    "objectID": "week01/w1_course-intro.html#the-core-goals-of-this-course",
    "href": "week01/w1_course-intro.html#the-core-goals-of-this-course",
    "title": "Course Intro",
    "section": "The core goals of this course",
    "text": "The core goals of this course\nLearning skills to:\n\nDo your research more reproducibly and efficiently\nPrepare yourself for working with large “omics” datasets"
  },
  {
    "objectID": "week01/w1_course-intro.html#course-background-reproducibility",
    "href": "week01/w1_course-intro.html#course-background-reproducibility",
    "title": "Course Intro",
    "section": "Course background: Reproducibility",
    "text": "Course background: Reproducibility\n\nTwo related ideas:\n\nGetting same results with an independent experiment (replicable)\nGetting same results given the same data (reproducible)\n\nOur focus is on #2."
  },
  {
    "objectID": "week01/w1_course-intro.html#course-background-reproducibility-cont.",
    "href": "week01/w1_course-intro.html#course-background-reproducibility-cont.",
    "title": "Course Intro",
    "section": "Course background: Reproducibility (cont.)",
    "text": "Course background: Reproducibility (cont.)\n\n“The most basic principle for reproducible research is: Do everything via code.”\n—Karl Broman\n\n\n\nAlso important:\n\nProject organization and documentation (week 2)\nSharing data and code (for code: Git & GitHub, week 3)\nHow you code (e.g. week 4 - shell scripts, and 6 - Nextflow)\n\n\n\n\n\n\n\n\n\nAnother motivator: working reproducibly will benefit future you!"
  },
  {
    "objectID": "week01/w1_course-intro.html#course-background-efficiency-and-automation",
    "href": "week01/w1_course-intro.html#course-background-efficiency-and-automation",
    "title": "Course Intro",
    "section": "Course background: Efficiency and automation",
    "text": "Course background: Efficiency and automation\n\nUsing code enables you to work more efficiently and automatically —\nparticularly useful when having to:\n\nDo repetitive tasks\nRecreate a figure or redo an analysis after adding a sample\nRedo a project after uncovering a mistake in the first data processing step."
  },
  {
    "objectID": "week01/w1_course-intro.html#course-background-omics-data",
    "href": "week01/w1_course-intro.html#course-background-omics-data",
    "title": "Course Intro",
    "section": "Course background: Omics data",
    "text": "Course background: Omics data\n\nOmics data is increasingly important in biology, and most notably includes:\n\nGenomics, including Metagenomics\nTranscriptomics\nProteomics\nMetabolomics\n\n\n\n\n\nWhile we’ll be using some example omics datasets, this course will not teach you how to analyze omics data in full — our focus is on fundamental computational skills."
  },
  {
    "objectID": "week01/w1_course-intro.html#the-unix-shell-shell-scripts-wk-2-4-and-more",
    "href": "week01/w1_course-intro.html#the-unix-shell-shell-scripts-wk-2-4-and-more",
    "title": "Course Intro",
    "section": "The Unix shell & shell scripts (Wk 2-4 and more)",
    "text": "The Unix shell & shell scripts (Wk 2-4 and more)\nBeing able to work in the Unix shell is a fundamental skill in computational biology.\n\n\nYou’ll spend a lot of time with the Unix shell, starting this week, and including in weeks that aren’t solely focused on the shell.\nWe’ll also write shell scripts, and will use an editor called VS Code for this and other purposes.\n\n\n\n\n\n\n\n\n\n\nBash (shell language)\n\n\n\n\n\n\n\n\nVS Code"
  },
  {
    "objectID": "week01/w1_course-intro.html#project-organization-and-markdown-wk-3",
    "href": "week01/w1_course-intro.html#project-organization-and-markdown-wk-3",
    "title": "Course Intro",
    "section": "Project organization and Markdown (Wk 3)",
    "text": "Project organization and Markdown (Wk 3)\nGood project file organization & documentation is a necessary starting point for reproducible research.\n\n\nYou’ll learn best practices for project organization, file naming, etc.\nTo document and report what you are doing, you’ll use Markdown files.\n\n\n\n\n\n\n\n\nMarkdown"
  },
  {
    "objectID": "week01/w1_course-intro.html#version-control-with-git-and-github-wk-4",
    "href": "week01/w1_course-intro.html#version-control-with-git-and-github-wk-4",
    "title": "Course Intro",
    "section": "Version control with Git and GitHub (Wk 4)",
    "text": "Version control with Git and GitHub (Wk 4)\nUsing version control, you can more effectively keep track of project progress, collaborate, share code, revisit earlier versions, and undo.\n\n\nGit is the version control software we will use,\nand GitHub is the website that hosts Git projects (repositories).\nYou’ll also use Git+GitHub to hand in your graded assignments."
  },
  {
    "objectID": "week01/w1_course-intro.html#high-performance-computing-with-osc-wk-5",
    "href": "week01/w1_course-intro.html#high-performance-computing-with-osc-wk-5",
    "title": "Course Intro",
    "section": "High-performance computing with OSC (Wk 5)",
    "text": "High-performance computing with OSC (Wk 5)\nThanks to supercomputer resources, you can work with very large datasets at speed — running up to 100s of analyses in parallel, and using much larger amounts of memory and storage space than a personal computer has.\n\n\n\nWe will use OSC throughout the course, and you’ll get a brief intro to it today.\nIn week 5, we’ll learn to submit shell scripts as OSC “batch jobs” with Slurm, and use Conda to manage software."
  },
  {
    "objectID": "week01/w1_course-intro.html#automated-workflow-management-wk-6",
    "href": "week01/w1_course-intro.html#automated-workflow-management-wk-6",
    "title": "Course Intro",
    "section": "Automated workflow management (Wk 6)",
    "text": "Automated workflow management (Wk 6)\nUsing a workflow written with a workflow manager, you can run and rerun entire analysis pipeline with a single command, and easily change and rerun parts of it, too.\n\n\nWe’ll use the workflow language Nextflow."
  },
  {
    "objectID": "week01/w1_course-intro.html#zoom",
    "href": "week01/w1_course-intro.html#zoom",
    "title": "Course Intro",
    "section": "Zoom",
    "text": "Zoom\n\nBe muted by default, but feel free to unmute yourself to ask questions any time.\nQuestions can also be asked in the chat.\nHaving your camera turned on as much as possible is appreciated!\n\n\n\n“Screen real estate” — large/multiple monitors or multiple devices best.\nBe ready to share your screen."
  },
  {
    "objectID": "week01/w1_course-intro.html#websites-books",
    "href": "week01/w1_course-intro.html#websites-books",
    "title": "Course Intro",
    "section": "Websites & Books",
    "text": "Websites & Books\n\nInfo about CarmenCanvas website…\n\n\n\nThe GitHub website contains:\n\nOverviews of each week & readings\nSlide decks and lecture pages\nExercises\nFinal project assignment information\n\n\n\n\n\nBooks:\n\nComputing Skills for Biologists (“CSB”; Allesina & Wilmes 2019)\nBioinformatics Data Skills (“Buffalo”; Buffalo 2015)"
  },
  {
    "objectID": "week01/w1_course-intro.html#final-project-graded",
    "href": "week01/w1_course-intro.html#final-project-graded",
    "title": "Course Intro",
    "section": "Final project (graded)",
    "text": "Final project (graded)\nPlan and implement a small computational project, with the following checkpoints:\n\nI: Proposal (due week 4 – 10 points)\nII: Draft (due week 6 – 10 points)\nIII: Oral presentations on Zoom (week 7 – 10 points)\nIV: Final submission (due April 29 – 20 points)\n\n\n\n\n\n\n\n\n\nData sets for the final project\n\n\nIf you have your own data set & analysis ideas, that is ideal. If not, I can provide you with this.\nMore information about the final project will follow later in the course."
  },
  {
    "objectID": "week01/w1_course-intro.html#ungraded-homework",
    "href": "week01/w1_course-intro.html#ungraded-homework",
    "title": "Course Intro",
    "section": "Ungraded homework",
    "text": "Ungraded homework\n\nWeekly readings — somewhat up to you when to do these, ideally before and after the lectures!\nWeekly exercises — I recommend doing these on Fridays\nMiscellaneous small assignments such as surveys and account setup.\n\n\n\n\n\n\n\n\nWeekly materials & homework\n\n\nI will try add the materials for each week on the preceding Friday — at the least the week’s overview and readings.\nNone of this homework had to be handed in."
  },
  {
    "objectID": "week01/w1_course-intro.html#weekly-recitation-on-monday",
    "href": "week01/w1_course-intro.html#weekly-recitation-on-monday",
    "title": "Course Intro",
    "section": "Weekly recitation on Monday",
    "text": "Weekly recitation on Monday\nWe will have an optional weekly recitation meeting on Monday in which we go through the exercises for the preceding week.\n\nIf you’re interested, indicate your availability here: TBA"
  },
  {
    "objectID": "week01/w1_course-intro.html#rest-of-this-week",
    "href": "week01/w1_course-intro.html#rest-of-this-week",
    "title": "Course Intro",
    "section": "Rest of this week",
    "text": "Rest of this week\n\nBrief introduction to the Ohio Supercomputer Center (OSC)\n\n\n\nUnix shell basics\n\n\n\nHomework:\n\nReadings: mostly CSB Chapter 1\nExercises"
  },
  {
    "objectID": "week01/w1_osc.html#goals-for-this-session",
    "href": "week01/w1_osc.html#goals-for-this-session",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "1 Goals for this session",
    "text": "1 Goals for this session\nThis session will provide an introduction to high-performance computing in general and to the Ohio Supercomputer Center (OSC).\nThis is only meant as a brief introductory overview to give some context about the working environment that we will start using this week. During the course, you’ll learn a lot more about most topics touched on in this page — week 5 in particular focuses on OSC."
  },
  {
    "objectID": "week01/w1_osc.html#high-performance-computing",
    "href": "week01/w1_osc.html#high-performance-computing",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "2 High-performance computing",
    "text": "2 High-performance computing\nA supercomputer (also known as a “compute cluster” or simply a “cluster”) consists of many computers that are connected by a high-speed network, and that can be accessed remotely by its users. In more general terms, supercomputers provide high-performance computing (HPC) resources.\nThis is what Owens, one of the OSC supercomputers, physically looks like:\n\n\n\n\n\nHere are some possible reasons to use a supercomputer instead of your own laptop or desktop:\n\nYour analyses take a long time to run, need large numbers of CPUs, or a large amount of memory.\nYou need to run some analyses many times.\nYou need to store a lot of data.\nYour analyses require specialized hardware, such as GPUs (Graphical Processing Units).\nYour analyses require software available only for the Linux operating system, but you use Windows.\n\nWhen you’re working with omics data, many of these reasons typically apply. This can make it hard or sometimes simply impossible to do all your work on your personal workstation, and supercomputers provide a solution.\n\n\nThe Ohio Supercomputer Center (OSC)\nThe Ohio Supercomputer Center (OSC) is a facility provided by the state of Ohio in the US. It has two supercomputers, lots of storage space, and an excellent infrastructure for accessing these resources.\n\n\n\n\n\n\n\nOSC websites and “Projects”\n\n\n\nOSC has three main websites — we will mostly or only use the first:\n\nhttps://ondemand.osc.edu: A web portal to use OSC resources through your browser (login needed).\nhttps://my.osc.edu: Account and project management (login needed).\nhttps://osc.edu: General website with information about the supercomputers, installed software, and usage.\n\n\nAccess to OSC’s computing power and storage space goes through OSC “Projects”:\n\nA project can be tied to a research project or lab, or be educational like this course’s project, PAS2700.\nEach project has a budget in terms of “compute hours” and storage space1.\nAs a user, it’s possible to be a member of multiple different projects."
  },
  {
    "objectID": "week01/w1_osc.html#the-structure-of-a-supercomputer-center",
    "href": "week01/w1_osc.html#the-structure-of-a-supercomputer-center",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "3 The structure of a supercomputer center",
    "text": "3 The structure of a supercomputer center\n\n3.1 Terminology\nLet’s start with some (super)computing terminology, going from smaller things to bigger things:\n\nNode\nA single computer that is a part of a supercomputer.\nSupercomputer / Cluster\nA collection of computers connected by a high-speed network. OSC has two: “Pitzer” and “Owens”.\nSupercomputer Center\nA facility like OSC that has one or more supercomputers.\n\n\n\n3.2 Supercomputer components\nWe can think of a supercomputer as having three main parts:\n\nFile Systems: Where files are stored (these are shared between the two OSC supercomputers!)\nLogin Nodes: The handful of computers everyone shares after logging in\nCompute Nodes: The many computers you can reserve to run your analyses\n\n\n\n\n\n\n\n\nFile systems\nOSC has several distinct file systems:\n\n\n\n\n\n\n\n\n\n\n\nFile system\nLocated within\nQuota\nBacked up?\nAuto-purged?\nOne for each…\n\n\n\n\nHome\n/users/\n500 GB / 1 M files\nYes\nNo\nUser\n\n\nProject\n/fs/ess/\nFlexible\nYes\nNo\nOSC Project\n\n\nScratch\n/fs/scratch/\n100 TB\nNo\nAfter 90 days\nOSC Project\n\n\n\nDuring the course, we will be working in the project directory of the course’s OSC Project PAS2700: /fs/ess/PAS2700. (We’ll talk more about these different file systems in week 5.)\n\n\n\n\n\n\nDirectory is just another word for folder, often written as “dir” for short\n\n\n\n\n\n\n\n\n\nLogin Nodes\nLogin nodes are set aside as an initial landing spot for everyone who logs in to a supercomputer. There are only a handful of them on each supercomputer, they are shared among everyone, and cannot be “reserved”.\nAs such, login nodes are meant only to do things like organizing your files and creating scripts for compute jobs, and are not meant for any serious computing, which should be done on the compute nodes.\n\n\n\nCompute Nodes\nData processing and analysis is done on compute nodes. You can only use compute nodes after putting in a request for resources (a “job”). The Slurm job scheduler, which we will learn to use in week 5, will then assign resources to your request.\n\n\n\n\n\n\nCompute node types\n\n\n\nCompute nodes come in different shapes and sizes. Standard, default nodes work fine for the vast majority of analyses, even with large-scale omics data. But you will sometimes need non-standard nodes, such as when you need a lot of RAM memory or need GPUs2.\n\n\n\n\n\n\n\n\nAt-home reading: What works differently on a supercomputer like at OSC? (Click to expand)\n\n\n\n\n\nCompared to command-line computing on a laptop or desktop, a number of aspects are different when working on a supercomputer like at OSC. We’ll learn much more about these later on in the course, but here is an overview:\n\n“Non-interactive” computing is common\nIt is common to write and “submit” scripts to a queue instead of running programs interactively.\nSoftware\nYou generally can’t install “the regular way”, and a lot of installed software needs to be “loaded”.\nOperating system\nSupercomputers run on the Linux operating system.\nLogin versus compute nodes\nAs mentioned, the nodes you end up on after logging in are not meant for heavy computing and you have to request access to “compute nodes” to run most analyses."
  },
  {
    "objectID": "week01/w1_osc.html#osc-ondemand",
    "href": "week01/w1_osc.html#osc-ondemand",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "4 OSC OnDemand",
    "text": "4 OSC OnDemand\nThe OSC OnDemand web portal allows you to use a web browser to access OSC resources such as:\n\nA file browser where you can also create and rename folders and files, etc.\nA Unix shell\n“Interactive Apps”: programs such as RStudio, Jupyter, VS Code and QGIS.\n\n Go to https://ondemand.osc.edu and log in (use the boxes on the left-hand side)\nYou should see a landing page similar to the one below:\n\n\n\nWe will now go through some of the dropdown menus in the blue bar along the top.\n\n\n4.1 Files: File system access\nHovering over the Files dropdown menu gives a list of directories that you have access to. If your account is brand new, and you were added to PAS2700, you should only have three directories listed:\n\nA Home directory (starts with /users/)\nThe PAS2700 project’s “scratch” directory (/fs/scratch/PAS2700)\nThe PAS2700 project’s “project” directory (/fs/ess/PAS2700)\n\nYou will only ever have one Home directory at OSC, but for every additional project you are a member of, you should usually see additional /fs/ess and /fs/scratch directories appear.\n Click on our focal directory /fs/ess/PAS2700.\n\n\n\n\n\nOnce there, you should see whichever directories and files are present at the selected location, and you can click on the directories to explore the contents further:\n\n\n\n\n\nThis interface is much like the file browser on your own computer, so you can also create, delete, move and copy files and folders, and even upload (from your computer to OSC) and download (from OSC your computer) files3 — see the buttons across the top.\n\n\n\n4.2 Interactive Apps\nWe can access programs with Graphical User Interfaces (GUIs; point-and-click interfaces) via the Interactive Apps dropdown menu:\n\n\n\n\n\nNext week, we will start using the VS Code text editor, which is listed here as Code Server.\n\n\n\n4.3 Clusters: Unix shell access\n\n\n\n\n\n\nSystem Status within Clusters (Click to expand)\n\n\n\n\n\nIn the “Clusters” dropdown menu, click on the item at the bottom, “System Status”:\n\n\n\n\n\nThis page shows an overview of the live, current usage of the two clusters — that can be interesting to get a good idea of the scale of the supercomputer center, which cluster is being used more, what the size of the “queue” (which has jobs waiting to start) is, and so on.\n\n\n\n\n\n\n\n\nInteracting with a supercomputer is most commonly done using a Unix shell. Under the Clusters dropdown menu, you can access a Unix shell either on Owens or Pitzer:\n\n\n\n\n\nI’m selecting a shell on the Pitzer supercomputer (“Pitzer Shell Access”), which will open a new browser tab, where the bottom of the page looks like this:\n\n\n\n\n\nWe’ll return to this Unix shell in the next session."
  },
  {
    "objectID": "week01/w1_osc.html#footnotes",
    "href": "week01/w1_osc.html#footnotes",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n But we don’t have to pay anything for educational projects like this one. Otherwise, for OSC’s rates for academic research, see this page.↩︎\nGPUs are e.g. used for Nanopore basecalling↩︎\nThough this is not meant for large (&gt;1 GB) transfers. Different methods are available — we’ll talk about those later on.↩︎"
  },
  {
    "objectID": "week01/w1_overview.html#links",
    "href": "week01/w1_overview.html#links",
    "title": "Week 1: Course Intro & Shell Basics",
    "section": "1 Links",
    "text": "1 Links\n\nUngraded first-day assignments\n\nPre-course survey - link TBA\nGet access to the Ohio Supercomputer Center (OSC)\n\n\n\nLecture pages\n\nTue: Course intro (slides)\nTue: OSC intro\nThu: Shell basics\n\n\n\nExercises\n\nWeek 1 exercises"
  },
  {
    "objectID": "week01/w1_overview.html#content-overview",
    "href": "week01/w1_overview.html#content-overview",
    "title": "Week 1: Course Intro & Shell Basics",
    "section": "2 Content overview",
    "text": "2 Content overview\nThis week, you will get an overview of the course and a brief intro to the Ohio Supercomputer (OSC) during Tuesday’s session, and will be taught the basics of working in a Unix shell environment during Thursday’s meeting.\nMore specifically, this week you will learn:\n\nCourse intro & overview (Tuesday class)\n\nWhat to expect from this course.\nWhich tools and languages we will use.\nWhat is expected of you during this course.\nGet up to speed on the infrastructure of the course.\n\n\n\nOSC Intro (Tuesday class)\n\nWhat is a supercomputer and why is it useful?\nOverview of the resources the Ohio Supercomputer Center (OSC) provides.\nHow to use OSC OnDemand and access a Unix Shell in your browser.\n\n\n\nUnix shell basics (Thursday class, Readings, and Exercises)\n\nWhy using a command-line interface can be beneficial.\nWhat the Unix shell is and what you can do with it.\nUsing Unix commands, learn how to:\n\nNavigate around your computer.\nCreate and manage directories and files.\nView text files.\nSearch within, manipulate, and extract information from text files."
  },
  {
    "objectID": "week01/w1_overview.html#readings",
    "href": "week01/w1_overview.html#readings",
    "title": "Week 1: Course Intro & Shell Basics",
    "section": "3 Readings",
    "text": "3 Readings\nThe first few pages of the book Computing Skills for Biologists by Allesina & Wilmes (CSB for short), should give you an introduction to the rationale behind the book as well as this course.\nOur main text for this week will be Chapter 1 from CSB, which will introduce you to using the Unix shell.\nYou can access the books directly online through the links below, or download PDFs from the CarmenCanvas website. Consider buying a paper copy of one or both if you can afford it.\n\nRequired readings\n\nCSB Chapter 0: “Introduction Building a Computing Toolbox” – Introduction & Section 0.1 only\nCSB Chapter 1: “Unix”\n\n\n\nOptional readings\n\nBuffalo Preface and Chapter 1: “How to Learn Bioinformatics”"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "pracs-sp24",
    "section": "",
    "text": "Practical Computing Skills for Omics Data  Ohio State PLNTPTH 5004 Autumn 2025\n\n\n\n\n\n\n\n\n\n\nWeek\nTue\nThu\nHomework\n\n\n\n\n1\n08/26: Intro the course & omics data\n08/28: Intro to OSC and the Unix shell\n   \n\n\n2\n09/02: Unix shell basics I\n09/04: Unix shell basics II\n\n\n\n3\n09/09: Project file organization & Markdown\n09/11: Managing files in the shell\n   \n\n\n4\n09/16: Version control with Git\n09/18: Git remotes on GitHub\n   \n\n\n5\n09/23: Shell scripting\n09/25: Running CLI tools with shell scripts\n   \n\n\n6\n09/30: More on OSC & data management\n10/02: Software at OSC\n      \n\n\n7\n10/07: OSC Slurm batch jobs I\n10/09: OSC Slurm batch jobs II\n\n\n\n8\n10/14: Manual versus automated pipelines\n\n   \n\n\n9\n10/21: Nextflow nf-core pipelines\n10/23: Nextflow nf-core pipelines\n   \n\n\n10\n10/28: Building Nextflow pipelines I\n10/30: Building Nextflow pipelines II\n\n\n\n11\n11/04: Building Nextflow pipelines III\n11/06: Building Nextflow pipelines IV\n\n\n\n12\n\n11/13: R - Basics\n\n\n\n13\n11/18: R - data wrangling\n11/20: R - data visualization\n\n\n\n14\n11/25: R - Quarto\n\n\n\n\n15\n12/02: R - Omics data I\n12/04: R - Omics data II\n\n\n\n16\n12/09: Student presentations\n\n\n\n\n\n\nHomework column:  = Week overview;  = Exercises;  = Graded assignments; XXX = Ungraded assignments\n\n\n\n\n Back to top"
  },
  {
    "objectID": "week02/w1_shell.html#goals-for-this-session",
    "href": "week02/w1_shell.html#goals-for-this-session",
    "title": "Unix Shell Basics",
    "section": "1 Goals for this session",
    "text": "1 Goals for this session\nIn this session, we’ll cover much of the material from CSB Chapter 1, to learn:\n\nWhy using a command-line interface can be beneficial.\nWhat the Unix shell is and what you can do with it.\nUsing the shell, learn how to:\n\nNavigate around your computer.\nCreate and manage directories and files.\nView text files.\nSearch within, manipulate, and extract information from text files.\n\n\n\n\n\n\n\n\nSee the “Topic Overview” page on the Unix shell for an overview of shell commands we’ll cover during this course."
  },
  {
    "objectID": "week02/w1_shell.html#introduction-ch.-1.1-1.2",
    "href": "week02/w1_shell.html#introduction-ch.-1.1-1.2",
    "title": "Unix Shell Basics",
    "section": "2 Introduction (Ch. 1.1-1.2)",
    "text": "2 Introduction (Ch. 1.1-1.2)\n\n2.1 Some terminology\n\nUnix (vs. Linux)\nWe can conceptualize Unix (or “Unix-like” / “*nix”)1 as a family of operating systems, which includes Linux and Mac but not Windows.\n\n\nUnix shell-related terms\n\nCommand Line — the most general term, an interface2 where you type commands\nTerminal — the program/app/window that can run a Unix shell\nShell — a command line interface to your computer\nUnix Shell — the types of shells on Unix family (Linux + Mac) computers\nBash — the specific Unix shell language that is most common on Unix computers\n\nWhile these are not synonyms, in day-to-day computing/bioinformatics, they are often used interchangeably.\n\n\n\n2.2 Why use the Unix shell?\n\nVersus programs with graphical user interfaces:\n\nUsing software\nBest or only option to use many programs, especially in bioinformatics.\nAutomation and fewer errors\nThe shell allows you to repeat and automate tasks easily and without introducing errors.\nReproducibility\nThe shell keeps a record of what you have done.\nWorking with large files\nShell commands are good at processing large files, which are common when working with omics data\nRemote computing – especially HPCs\nIt is often only possible to work in a terminal when doing remote computing.\n\nVersus scripting languages like Python or R:\n\nFor many simpler tasks, built-in shell tools are faster — in terms of coding time, processing time, and the ease of processing very large files.\nThe Unix shell has a direct interface to other programs."
  },
  {
    "objectID": "week02/w1_shell.html#getting-started-with-unix-ch.-1.3",
    "href": "week02/w1_shell.html#getting-started-with-unix-ch.-1.3",
    "title": "Unix Shell Basics",
    "section": "3 Getting started with Unix (Ch. 1.3)",
    "text": "3 Getting started with Unix (Ch. 1.3)\n\nUnix directory structure\n\n“Directory” (or “dir” for short) is the term for folder that is commonly used in Unix contexts.\nThe Unix directory structure is hierarchical, with a single starting point: the root, depicted as /.\nA “path” gives the location of a file or directory, in which directories are separated by forward slashes /.\nFor example, the path to our OSC project’s dir is /fs/ess/PAS2700. This means: the dir PAS2700 is located inside the dir ess, which in turn is inside the dir fs, which in turn is in the computer’s root directory.\nThe OSC dir structure is somewhat different from that of a personal computer. Our Home dir is not /home/&lt;username&gt; like in the book and the schematic on the left, but /users/&lt;a-project&gt;/&lt;username&gt;.\n\n\n\n\n\n\nGeneric example, from O’Neil 2017\n\n\n\n\n\n\nKey dirs in OSC dir structure"
  },
  {
    "objectID": "week02/w1_shell.html#getting-started-with-the-shell-ch.-1.4",
    "href": "week02/w1_shell.html#getting-started-with-the-shell-ch.-1.4",
    "title": "Unix Shell Basics",
    "section": "4 Getting started with the shell (Ch. 1.4)",
    "text": "4 Getting started with the shell (Ch. 1.4)\n\n4.1 Accessing a shell at OSC\n\nGo to https://ondemand.osc.edu in your browser, and log in.\nClick on Clusters and then Pitzer Shell Access.\n\n\n\n\n\n\n\n\nCopying and pasting in this shell\n\n\n\nYou can’t right-click in this shell, so to copy-and-paste:\n\nCopy simply by selecting text (you should see a copy () icon appear).\nPaste using Ctrl+V.\n\n Try copying and pasting a random word into your shell. This may just work, you may get a permission pop-up, or it may silently fail — if the latter, click on the clipboard icon in your browser’s address bar (see red circle in screenshot below):\n\n\n\n\n\n4.2 The prompt\nInside your terminal, the “prompt” indicates that the shell is ready for a command. What is shown exactly varies across shells and can also be customized, but our prompts at OSC should show the following information:\n&lt;username&gt;@&lt;node-name&gt; &lt;working-dir&gt;]$\nFor example (and note that ~ means your Home dir):\n[jelmer@pitzer-login02 ~]$ \nWe type our commands after the dollar sign, and then press Enter to execute the command. When the command has finished executing, we’ll get our prompt back and can type a new command.\n\n\n\n\n\n\nClearing the screen\n\n\n\nOSC prints welcome messages and storage quota information when you open a shell. To reduce the amount of text on the screen, I will clear the screen now and regularly throughout. This can be done with the keyboard shortcut Ctrl+L.\n\n\n\n\n\n4.3 A few simple commands: date and pwd\nThe Unix shell comes with hundreds of “commands”: small programs that perform specific actions. If you’re familiar with R or Python, a Unix command is like an R/Python function.\nLet’s start with a few simple commands:\n\nThe date command prints the current date and time:\ndate\nThu Feb 29 14:58:19 EST 2024\nThe pwd (Print Working Directory) command prints the path to the directory you are currently located in:\npwd\n/users/PAS0471/jelmer\n\nBoth of those commands provided us with some output. That output was printed to screen, which is the default behavior for nearly every Unix command.\n\n\n\n4.4 The cal command — and options & arguments\nThe cal command is another example of a command that simply prints some information to the screen, in this case a calendar. We’ll use it to learn about command options and arguments.\nInvoking cal without options or arguments will show the current month:\ncal\n    February 2024   \nSu Mo Tu We Th Fr Sa\n             1  2  3\n 4  5  6  7  8  9 10\n11 12 13 14 15 16 17\n18 19 20 21 22 23 24\n25 26 27 28 29\n\nOption examples\nUse the option -j (dash and then j) for a Julian calendar, in which day numbering is continuous instead of restarting each month:\n# Make sure to leave a space between `cal` and `-j`!\ncal -j\n       February 2024       \nSun Mon Tue Wed Thu Fri Sat\n                 32  33  34\n 35  36  37  38  39  40  41\n 42  43  44  45  46  47  48\n 49  50  51  52  53  54  55\n 56  57  58  59  60\nUse the -3 option to show 3 months (adding the previous and next month):\ncal -3\n    January 2024          February 2024          March 2024     \nSu Mo Tu We Th Fr Sa  Su Mo Tu We Th Fr Sa  Su Mo Tu We Th Fr Sa\n    1  2  3  4  5  6               1  2  3                  1  2\n 7  8  9 10 11 12 13   4  5  6  7  8  9 10   3  4  5  6  7  8  9\n14 15 16 17 18 19 20  11 12 13 14 15 16 17  10 11 12 13 14 15 16\n21 22 23 24 25 26 27  18 19 20 21 22 23 24  17 18 19 20 21 22 23\n28 29 30 31           25 26 27 28 29        24 25 26 27 28 29 30\n                                            31   \nWe can always combine multiple options, for example:\ncal -j -3\n        January 2024                February 2024                  March 2024        \nSun Mon Tue Wed Thu Fri Sat  Sun Mon Tue Wed Thu Fri Sat  Sun Mon Tue Wed Thu Fri Sat\n      1   2   3   4   5   6                   32  33  34                       61  62\n  7   8   9  10  11  12  13   35  36  37  38  39  40  41   63  64  65  66  67  68  69\n 14  15  16  17  18  19  20   42  43  44  45  46  47  48   70  71  72  73  74  75  76\n 21  22  23  24  25  26  27   49  50  51  52  53  54  55   77  78  79  80  81  82  83\n 28  29  30  31               56  57  58  59  60           84  85  86  87  88  89  90\n                                                           91 \nHandily, options can be “pasted together” like so (output not shown - same as above):\ncal -j3\n\n\nOptions summary\nAs we’ve seen, options are specified with a dash - (or --, as you’ll see later). So far, we’ve only worked with the type of options that are also called “flags”, which change some functionality in an ON/OFF type way:\n\nTurning a Julian calender display ON with -j\nTurning a 3-month display ON with -3.\n\nIn general terms, options change the behavior of a command.\n\n\nArguments\nWhereas options change the behavior of a command, arguments typically tell the command what to operate on. Most commonly, these are file or directory paths.\nAdmittedly, the cal command is not the best illustration of this pattern — when you give it one argument, this is supposed to be the year to show a calendar for:\ncal 2020\n                              2020                               \n\n       January               February                 March       \nSu Mo Tu We Th Fr Sa   Su Mo Tu We Th Fr Sa   Su Mo Tu We Th Fr Sa\n          1  2  3  4                      1    1  2  3  4  5  6  7\n 5  6  7  8  9 10 11    2  3  4  5  6  7  8    8  9 10 11 12 13 14\n12 13 14 15 16 17 18    9 10 11 12 13 14 15   15 16 17 18 19 20 21\n19 20 21 22 23 24 25   16 17 18 19 20 21 22   22 23 24 25 26 27 28\n26 27 28 29 30 31      23 24 25 26 27 28 29   29 30 31\n\n# [...output truncated, entire year is shown...]\nWe can also combine options and arguments:\ncal -j 2020\n                            2020                          \n\n          January                       February         \nSun Mon Tue Wed Thu Fri Sat   Sun Mon Tue Wed Thu Fri Sat\n              1   2   3   4                            32\n  5   6   7   8   9  10  11    33  34  35  36  37  38  39\n 12  13  14  15  16  17  18    40  41  42  43  44  45  46\n 19  20  21  22  23  24  25    47  48  49  50  51  52  53\n 26  27  28  29  30  31        54  55  56  57  58  59  60\n \n# [...output truncated, entire year is shown...]\nSo, arguments to a command:\n\nAre not preceded by a - (or --)\nIf options and arguments are combined, arguments come after options3.\n\n\n\n\n\n4.5 Getting help\nMany commands – and other command-line programs! – have a -h option for help, which usually gives a concise summary of the command’s syntax, i.e. it’s available options and arguments:\ncal -h\n\nUsage:\n cal [options] [[[day] month] year]\n\nOptions:\n -1, --one        show only current month (default)\n -3, --three      show previous, current and next month\n -s, --sunday     Sunday as first day of week\n -m, --monday     Monday as first day of week\n -j, --julian     output Julian dates\n -y, --year       show whole current year\n -V, --version    display version information and exit\n -h, --help       display this help text and exit\n\n\n\n\n\n\nAnother way to see documentation: the man command (Click to expand)\n\n\n\n\n\nAn alternative way of getting help for Unix commands is with the man command:\nman cal\nThis manual page often includes a lot more details than the --help output, and it is opened inside a “pager” rather than printed to screen: type q to exit the pager that man launches.\n\n\n\n\n Exercise: Interpreting the help output\n\nLook through the options listed when you ran cal -h, and try an option we haven’t used yet4. (You can also combine this new option with other options, if you want.)\n\n\n\nSolution\n\n# Print a calendar with Monday as the first day of th week (instead of the default, Sunday) \ncal -m\n    February 2024   \nMo Tu We Th Fr Sa Su\n          1  2  3  4\n 5  6  7  8  9 10 11\n12 13 14 15 16 17 18\n19 20 21 22 23 24 25\n26 27 28 29\n\n\nTry using one or more options in their “long form” (with --). Why would those be useful?\n\n\n\nSolution\n\nFor example:\ncal --julian --monday\n       February 2024       \nMon Tue Wed Thu Fri Sat Sun\n             32  33  34  35\n 36  37  38  39  40  41  42\n 43  44  45  46  47  48  49\n 50  51  52  53  54  55  56\n 57  58  59  60\nThe advantage of using long options is that it is much more likely that any reader of the code (including yourself next week) will immediately understand what these options are doing.\nNote that long options cannot be “pasted together” like short options.\n\n\nBonus: Try to figure out / guess what the cal [options] [[[day] month] year] means. Can you print a calendar for April 2017?\n\n\n\nSolution\n\nFirst of all, the square brackets around options and all of the possible arguments means that none of these are required — as we’ve seen, just cal (with no options or arguments) is a valid command.\nThe structure of the multiple square brackets around the day-month-year arguments indicate that:\n\nIf you provide only one argument, it should be a year\nIf you provide a second argument, it should be a month\nIf you provide a third argument, it should be a day\n\nTherefore, to print a calendar for April 2017:\ncal 4 2017\n     April 2017     \nSu Mo Tu We Th Fr Sa\n                   1\n 2  3  4  5  6  7  8\n 9 10 11 12 13 14 15\n16 17 18 19 20 21 22\n23 24 25 26 27 28 29\n30\n\n\n\n\n\n4.6 cd and command “actions”\nAll the commands so far “merely” provided some information, which was printed to screen.\nBut many commands perform another kind of action. For example, the command cd will change your working directory. And like many commands that perform a potentially invisible action, cd normally has no output at all.\nFirst, let’s check again where we are — we should be in our Home directory:\n# (Note: you will have a different project listed in your Home dir. This is not important.)\npwd\n/users/PAS0471/jelmer\nNow, let’s use cd to move to another directory by specifying the path to that directory as an argument:\ncd /fs/ess/PAS2700\n# Double-check that we moved:\npwd\n/fs/ess/PAS2700\nIn summary:\n\nLike cal, cd accepts an argument. Unlike cal, this argument takes the form of a path that the command should operate on, which is much more typical.\ncd gives no output when it successfully changed the working directory. This is very common behavior for Unix commands that perform operations: when they succeed, they are silent.\n\nLet’s also see what happens when cd does not succeed — it gives an error:\ncd /fs/ess/PAs2700\n-bash: cd: /fs/ess/PAs2700: No such file or directory\n\n\n\n\n\n\n\nGeneral shell tips\n\n\n\n\nEverything in the shell is case-sensitive, including commands and file names (hence the error above).\nYour cursor can be anywhere on a line (not just at the end) when you press Enter to execute a command!\nAny text that comes after a # is considered a comment instead of code!\n\n# This entire line is a comment - you can run it and nothing will happen\npwd    # 'pwd' will be executed but everything after the '#' is ignored\n/users/PAS0471/jelmer\n\n\n\n\n\n\n4.7 Keyboard shortcuts\nUsing keyboard shortcuts help you work much more efficiently in the shell. And some are invaluable:\n\nCancel/stop/abort — If your prompt is “missing”, the shell is still busy executing your command, or you typed an incomplete command. To abort, press Ctrl+C and you will get your prompt back.\nCommand history — Up / Down arrow keys to cycle through your command history.\nTab completion — The shell will auto-complete partial commands or file paths when you press Tab.\n\n\n Practice with Tab completion & command history\n\nType /f and press Tab (will autocomplete to /fs/)\nAdd e (/fs/e) and press Tab (will autocomplete to /fs/ess/).\nAdd PAS (/fs/ess/PAS) and press Tab. Nothing should happen: there are multiple (many!) options.\nPress Tab Tab (i.e., twice in quick succession) and it should say:\nDisplay all 503 possibilities? (y or n)\nType n to answer no: we don’t need to see all the dirs starting with PAS.\nAdd 27 (/fs/ess/PAS27) and press Tab (should autocomplete to /fs/ess/PAS2700).\nPress Enter. What does the resulting error mean?\nbash: /fs/ess/PAS2700/: Is a directory\n\n\n\nClick to see the solution\n\nBasically, everything you type in the shell should start with a command. Just typing the name of a dir or file will not make the shell print some info about or the contents of said dir or file, as you perhaps expected.\n\n\nPress ⇧ to get the previous “command” back on the prompt.\nPress Ctrl+A to move to the beginning of the line at once.\nAdd cd and a space in front of the dir, and press Enter again.\ncd /fs/ess/PAS2700/\n\n\n\n Practice with canceling\nTo simulate a long-running command that we may want to abort, we can use the sleep command, which will make the computer wait for a specified amount of time until giving your prompt back. Run the below command and instead of waiting for the full 60 seconds, press Ctrl + C to get your prompt back sooner!\nsleep 60s\nOr, use Ctrl + C after running this example of an incomplete command (an opening parenthesis ():\n(\n\n\n\n\n\n\n\nTable with useful keyboard shortcuts (Click to expand)\n\n\n\n\n\nNote that even on Macs, you should use Ctrl instead of switching them out for Cmd as you may be used to doing (in some cases, like copy/paste, both work).\n\n\n\n\n\n\n\nShortcut\nFunction\n\n\n\n\nTab\nTab completion\n\n\n⇧ / ⇩\nCycle through previously issued commands\n\n\nCtrl(+Shift)+C\nCopy selected text\n\n\nCtrl(+Shift)+V\nPaste text from clipboard\n\n\nCtrl+A / Ctrl+E\nGo to beginning/end of line\n\n\nCtrl+U /Ctrl+K\nCut from cursor to beginning / end of line\n\n\nCtrl+W\nCut word before before cursor (Only works on Macs in our shell in the browser!)\n\n\nCtrl+Y\nPaste (“yank”) text that was cut with one of the shortcuts above\n\n\nAlt+. / Esc+.\nRetrieve last argument of previous command (very useful!) (Esc+. for Mac)\n\n\nCtrl+R\nSearch history: press Ctrl+R again to cycle through matches, Enter to put command in prompt.\n\n\nCtrl+C\nCancel (kill/stop/abort) currently active command\n\n\nCtrl+D\nExit (a program or the shell, depending on the context) (same as exit command)\n\n\nCtrl+L\nClear the screen (same as clear command)\n\n\n\n\n\n\n\n\n\n4.8 Environment variables\nYou may be familiar with the concept of variables from previous experience with perhaps R or another language. Variables can hold values and other pieces of data and are essential in programming.\n\nAssigning and printing the value of a variable in R:\n\n# (Don't run this)\nx &lt;- 5\nx\n\n[1] 5\n\n\nAssigning and printing the value of a variable in the Unix shell:\nx=5\necho $x\n5\n\n\n\n\n\n\n\nIn the Unix shell code above, note that:\n\n\n\n\nThere cannot be any spaces around the = in x=5.\nYou need a $ prefix to reference (but not to assign) variables in the shell5.\nYou need the echo command, a general command to print text, to print the value of $x (cf. in R).\n\nBy the way, echo can also print literal text (as shown below) or combinations of literals and variables (next exercise):\necho \"Welcome to PLNTPTH 6193\"\nWelcome to PLNTPTH 6193\n\n\n\nEnvironment variables are pre-existing variables that have been automatically assigned values. Two examples:\n# $HOME contains the path to your Home dir:\necho $HOME\n/users/PAS0471/jelmer\n# $USER contains your user name:\necho $USER\njelmer\n\n Exercise: environment variables\nUsing an environment variable, print “Hello, my name is &lt;your username&gt;” (e.g. “Hello, my name is natalie”).\n\n\nClick to see the solution\n\n# (This would also work without the \" \" quotes)\necho \"Hello, my name is $USER\"\nHello, my name is jelmer\n\n\n\n\n\n4.9 Create your own dir & get the CSB data\nOur base OSC directory for the course is the /fs/ess/PAS2700 dir we are currently in. Now, let’s all create our own subdir in here, and get the data from the CSB book.\n\nCreate a directory for yourself using the mkdir (make dir) command:\nmkdir users/$USER\nMove there using cd:\n# (Instead of $USER, you can also start typing your username and press Tab)\ncd users/$USER\nGet the files associated with the CSB book by “cloning” (downloading) its Git repository:\ngit clone https://github.com/CSB-book/CSB.git\nMove into the sandbox dir for the Unix chapter (remember to use tab completion):\ncd CSB/unix/sandbox\n\n\n\n\n4.10 Paths & navigation shortcuts\n\nAbsolute (full) paths versus relative paths\n\nAbsolute (full) paths (e.g. /fs/ess/PAS2700)\nPaths that begin with a / always start from the computer’s root directory, and are called “absolute paths”.\n(They are equivalent to GPS coordinates for a geographical location, as they work regardless of where you are).\nRelative paths (e.g. CSB/unix/sandbox)\nPaths that instead start from your current working directory are called “relative paths”.\n(These work like directions along the lines of “take the second left:” they depend on your current location.)\n\nNext week, we’ll talk more about the distinction between absolute and relative paths, and their respective merits.\n\n\nPath shortcuts\n\n~ (a tilde) — represents your Home directory. For example, cd ~ moves you to your Home dir.\n. (a single period) — represents the current working directory (we’ll soon see how why that can be useful).\n.. (two periods) — Represents the directory “one level up”, i.e. towards the computer’s root dir.\n\nSome examples of ..:\n\nUse .. to go up one level in the dir hierarchy:\npwd\n/fs/ess/PAS2700/users/jelmer/CSB/unix/sandbox\ncd ..\npwd\n/fs/ess/PAS2700/users/jelmer/CSB/unix\nThis pattern can be continued all the way to the root of the computer, so ../.. means two levels up:\ncd ../..\npwd\n/fs/ess/PAS2700/users/jelmer\n\n\n\n\n\n\n\nThese path shortcuts work with many commands\n\n\n\nThese are general shell shortcuts that work with any command that accepts a path/file name.\n\n\n\n\n\n Exercise\n\nA) Use a relative path to move back to the /fs/ess/PAS2700/users/$USER/CSB/unix/sandbox dir.\n\n\n\n(Click for the solution)\n\ncd CSB/unix/sandbox\n\n\nB) Use a relative path (with ..) to move into the /fs/ess/PAS2700/users/$USER/CSB/unix/data dir.\n\n\n\n(Click for the solution)\n\ncd ../data\nYou may have done this in two steps, because you may not have realized that you can “add to” a path after .. like we did above. So you may have done this:\ncd ..\ncd data\nThat’s OK, but is obviously more typing.\n\n\nC) The ls command lists files and dirs, and accepts one or more paths as arguments. Use ls to list the files in your Home dir with a shortcut and without moving there.\n\n\n\n(Click for the solution)\n\n\nWith the ~ shortcut:\n\nls ~\n# (Output not shown)\n\nWith the $HOME environment variable:\n\nls $HOME\n# (Output not shown)"
  },
  {
    "objectID": "week02/w1_shell.html#basic-unix-commands-ch.-1.5",
    "href": "week02/w1_shell.html#basic-unix-commands-ch.-1.5",
    "title": "Unix Shell Basics",
    "section": "5 Basic Unix commands (Ch. 1.5)",
    "text": "5 Basic Unix commands (Ch. 1.5)\n\n5.1 ls to list files\nThe ls command, short for “list”, will list files and directories — by default those in your current working dir:\n# (You should be in /fs/ess/PAS2700/users/$USER/CSB/unix/data)\nls\nBuzzard2015_about.txt  Gesquiere2011_about.txt  Marra2014_about.txt   miRNA                   Pacifici2013_data.csv  Saavedra2013_about.txt\nBuzzard2015_data.csv   Gesquiere2011_data.csv   Marra2014_data.fasta  Pacifici2013_about.txt  Saavedra2013\n\n\n\n\n\n\nls output colors (click to expand)\n\n\n\n\n\nThe ls output above does not show the different colors you should see in your shell — the most common ones are:\n\nEntries in blue are directories (like miRNA and Saavedra2013 above)\nEntries in black are regular files (like all other entries above)\nEntries in red are compressed files (we’ll see examples of this later).\n\n\n\n\nFor which dir ls lists its contents can be changed with arguments, and how it shows the output can be changed with options. For example, we can call ls with the option -l (lowercase L):\nls -l \ntotal 1793\n-rw-rw----+ 1 jelmer PAS0471     562 Feb 24 20:30 Buzzard2015_about.txt\n-rw-rw----+ 1 jelmer PAS0471   39058 Feb 24 20:30 Buzzard2015_data.csv\n-rw-rw----+ 1 jelmer PAS0471     447 Feb 24 20:30 Gesquiere2011_about.txt\n-rw-rw----+ 1 jelmer PAS0471   38025 Feb 24 20:30 Gesquiere2011_data.csv\n-rw-rw----+ 1 jelmer PAS0471     756 Feb 24 20:30 Marra2014_about.txt\n-rw-rw----+ 1 jelmer PAS0471  566026 Feb 24 20:30 Marra2014_data.fasta\ndrwxrwx---+ 2 jelmer PAS0471    4096 Feb 24 20:30 miRNA\n-rw-rw----+ 1 jelmer PAS0471     520 Feb 24 20:30 Pacifici2013_about.txt\n-rw-rw----+ 1 jelmer PAS0471 1076150 Feb 24 20:30 Pacifici2013_data.csv\ndrwxrwx---+ 2 jelmer PAS0471    4096 Feb 24 20:30 Saavedra2013\n-rw-rw----+ 1 jelmer PAS0471     322 Feb 24 20:30 Saavedra2013_about.txt\nIt lists the same items as earlier, but printed in a different format: one item per line, with additional information such as the date and time each file was last modified, and file sizes in bytes (to the left of the date).\nLet’s add another option, -h:\nls -lh\ntotal 1.8M\n-rw-rw----+ 1 jelmer PAS0471  562 Feb 24 20:30 Buzzard2015_about.txt\n-rw-rw----+ 1 jelmer PAS0471  39K Feb 24 20:30 Buzzard2015_data.csv\n-rw-rw----+ 1 jelmer PAS0471  447 Feb 24 20:30 Gesquiere2011_about.txt\n-rw-rw----+ 1 jelmer PAS0471  38K Feb 24 20:30 Gesquiere2011_data.csv\n-rw-rw----+ 1 jelmer PAS0471  756 Feb 24 20:30 Marra2014_about.txt\n-rw-rw----+ 1 jelmer PAS0471 553K Feb 24 20:30 Marra2014_data.fasta\ndrwxrwx---+ 2 jelmer PAS0471 4.0K Feb 24 20:30 miRNA\n-rw-rw----+ 1 jelmer PAS0471  520 Feb 24 20:30 Pacifici2013_about.txt\n-rw-rw----+ 1 jelmer PAS0471 1.1M Feb 24 20:30 Pacifici2013_data.csv\ndrwxrwx---+ 2 jelmer PAS0471 4.0K Feb 24 20:30 Saavedra2013\n-rw-rw----+ 1 jelmer PAS0471  322 Feb 24 20:30 Saavedra2013_about.txt\n\n\nWhat is different about the output, and what do you think that means? (Click to see the answer)\n\nThe only difference is in the format of the column reporting the sizes of the items listed.\nWe now have “Human-readable filesizes” (hence -h), where sizes on the scale of kilobytes will be shown with Ks, of megabytes with Ms, and of gigabytes with Gs. That can be really useful especially for very large files.\n\n\nWe can also list files in directories other than the one we are in, by specifying that dir as an argument:\nls miRNA\nggo_miR.fasta  hsa_miR.fasta  miR_about.txt  miRNA_about.txt  ppa_miR.fasta  ppy_miR.fasta  ptr_miR.fasta  ssy_miR.fasta\nAnd like we saw with cal, we can combine options and arguments:\nls -lh miRNA\ntotal 320K\n-rw-rw----+ 1 jelmer PAS0471  18K Feb 24 20:30 ggo_miR.fasta\n-rw-rw----+ 1 jelmer PAS0471 131K Feb 24 20:30 hsa_miR.fasta\n-rw-rw----+ 1 jelmer PAS0471  104 Feb 24 20:30 miR_about.txt\n-rw-rw----+ 1 jelmer PAS0471  104 Feb 24 20:30 miRNA_about.txt\n-rw-rw----+ 1 jelmer PAS0471 4.0K Feb 24 20:30 ppa_miR.fasta\n-rw-rw----+ 1 jelmer PAS0471  33K Feb 24 20:30 ppy_miR.fasta\n-rw-rw----+ 1 jelmer PAS0471  29K Feb 24 20:30 ptr_miR.fasta\n-rw-rw----+ 1 jelmer PAS0471  495 Feb 24 20:30 ssy_miR.fasta\nLet’s move into the sandbox dir in preparation for the next sections:\ncd ../sandbox\n\nls\nPapers and reviews\n\n\n\n5.2 cp to copy files\nThe cp command copies files and/or dirs from one location to another. It has two required arguments: what you want to copy (source), and where you want to copy it to (destination). Its basic syntax is cp &lt;source&gt; &lt;destination&gt;.\nFor example, to copy a file to our current working directory using the . shortcut, keeping the original file name:\ncp ../data/Buzzard2015_about.txt .\nWe can also copy using a new name for the copy:\ncp ../data/Buzzard2015_about.txt buzz2.txt\ncp will by default refuse to copy directories and their contents — that is, it is not “recursive” by default. The -r option is needed for recursive copying:\ncp -r ../data/ . \nCheck the contents of the sandbox dir again now that we’ve copied several items there:\nls\nbuzz2.txt  Buzzard2015_about.txt  data  Papers and reviews\n\n\n\n5.3 mv to move and rename files\nUse mv both to move and rename files (this is fundamentally the same operation):\n# Same directory, different file name (\"renaming\")\nmv buzz2.txt buzz_copy.txt\n# Different directory, same file name (\"moving\")\nmv buzz_copy.txt data/\nUnlike cp, mv is recursive by default, so you won’t need the -r option.\n\n\n\n\n\n\nBoth the mv and cp commands will by default:\n\n\n\n\nNot report what they do: no output = success (use the -v option for verbose to make them report what they do).\nOverwrite existing files without reporting this (use the -i option for interactive to make them ask before overwriting).\n\n\n\n\n\n\n5.4 rm to remove files and dirs\nThe rm command removes files and optionally dirs — here, we’ll remove the file copy we made above:\nrm Buzzard2015_about.txt\nLike with cp, the -r option is needed to make the command work recursively:\n# First we create 3 levels of dirs - we need `-p` to make mkdir work recursively:\nmkdir -p d1/d2/d3\n\n# Then we try to remove the d1 dir - which fails:\nrm d1\nrm: cannot remove ‘d1’: Is a directory\n# But it does work (silently!) with the '-r' option:\nrm -r d1\n\n\n\n\n\n\nThere is no thrash bin when deleting files in the shell, so use rm with caution! (Click to expand)\n\n\n\n\n\nrm -r can be very dangerous — for example rm -r / would at least attempt to remove the entire contents of the computer, including the operating system.\nA couple ways to take precautions:\n\nYou can add the -i option, which will have you confirm each individual removal (can be tedious)\nWhen you intend to remove an empty dir, you can use the rmdir command which will do just (and only) that — that way, if the dir isn’t empty after all, you’ll get an error.\n\n\n\n\n\n Bonus exercise: cp and mv behavior\nFor both cp and mv, when operating on files (and this works equivalently for dirs):\n\nIf the destination is an existing dir, the file will go into that dir and keep its original name.\nIf the destination is not an existing dir, the (last bit of the) destination specifies the new file name.\nA trailing slash in the destination makes explicit that you are referring to a dir and not a file.\n\n\nWith that in mind, try to answer the following questions about this command:\ncp Buzzard2015_about.txt more_data/\n\nWhat do you think the command would do or attempt to do?\nDo you think the command will succeed?\nWhat would the command have done if we had omitted the trailing forward slash?\n\n\n\nClick to see the solution\n\n\n\nBecause we put a trailing forward slash in more_data/, we are making clear that we are referring to a directory. So the file should be copied into a dir more_data, and keep the same file name.\nHowever, the more_data/ dir does not exist, and cp will not create a dir on the fly, so this will fail:\ncp Buzzard2015_about.txt more_data/\ncp: cannot create regular file ‘more_data/’: Not a directory\nIf we had omitted the trailing forward slash, we would have created a copy of the file with file name more_data (note that no file extension is needed, per se).\nP.S: To make the original intention work, first create the destination dir:\nmkdir more_data\ncp Buzzard2015_about.txt more_data/\nNote also that once the more_data dir exists, it does not make a difference whether or not you using a trailing slash (!).\n\n\n\n\n\n\n5.5 Viewing and processing text files\ncd ../data   # Move to the data dir for the next commands\n\ncat\nThe cat command will print the entire contents of (a) file(s) to screen:\ncat Marra2014_about.txt\nData published by:\nMarra NJ, DeWoody JA (2014) Transcriptomic characterization of the immunogenetic repertoires of heteromyid rodents. BMC Genomics 15: 929. http://dx.doi.org/10.1186/1471-2164-15-929\n\nData description:\nFile D_spec_spleen_filtered.fasta (57.01Mb) contains Dipodomys spectabilis spleen transcriptome data. Combined assembly of 454 reads and fragmented Illumina assembly (see methods of associated paper) in gsAssembler version 2.6.\nNote that we truncated the original file to 1% of its original size and named it Marra2014_data.txt\n\nData taken from:\nMarra NJ, DeWoody JA (2014) Data from: Transcriptomic characterization of the immunogenetic repertoires of heteromyid rodents. Dryad Digital Repository. http://dx.doi.org/10.5061/dryad.qn474\n\n\n\nhead and tail\nThe head and tail commands will print the first or last lines of a file:\n\nhead & tail’s defaults are to print 10 lines:\nhead Gesquiere2011_data.csv\nmaleID  GC      T\n1       66.9    64.57\n1       51.09   35.57\n1       65.89   114.28\n1       80.88   137.81\n1       32.65   59.94\n1       60.52   101.83\n1       65.89   65.84\n1       52.72   43.98\n1       84.85   102.31\nUse the -n option to specify the number of lines to print:\nhead -n 3 Gesquiere2011_data.csv\nmaleID  GC      T\n1       66.9    64.57\n1       51.09   35.57\nA neat trick with tail is to start at a specific line, often used to skip the header line, like in this example6:\ntail -n +2 Gesquiere2011_data.csv\n1       66.9    64.57\n1       51.09   35.57\n1       65.89   114.28\n1       80.88   137.81\n1       32.65   59.94\n1       60.52   101.83\n1       65.89   65.84\n1       52.72   43.98\n1       84.85   102.31\n1       98.25   149.61\n[...output truncated...]\n\n\n\n\nwc -l\nThe wc command is different from the previous commands, which all printed file contents. By default, wc will count lines, words, and characters — but it is most commonly used to only count lines, with the -l option:\nwc -l Marra2014_about.txt\n9 Marra2014_about.txt"
  },
  {
    "objectID": "week02/w1_shell.html#advanced-unix-commands-ch.-1.6",
    "href": "week02/w1_shell.html#advanced-unix-commands-ch.-1.6",
    "title": "Unix Shell Basics",
    "section": "6 Advanced Unix commands (Ch. 1.6)",
    "text": "6 Advanced Unix commands (Ch. 1.6)\nStart by moving back to the sandbox dir:\ncd ../sandbox\n\n6.1 Standard output and redirection\nThe regular output of a command is also called “standard out” (“stdout”). As we’ve seen many times now, such output is by default printed to screen, but it can alternatively be “redirected”, such as into a file.\nWith “&gt;”, we redirect output to a file:\n\nIf the file doesn’t exist, it will be created.\nIf the file does exist, any contents will be overwritten.\n\nFirst, let’s remind ourselves what echo does without redirection:\necho \"My first line\"\nMy first line\nNow, let’s redirect to a new file test.txt — no output is printed, as it went into the file:\necho \"My first line\" &gt; test.txt\ncat test.txt\nMy first line\nLet’s redirect another line into that same file:\necho \"My second line\" &gt; test.txt\ncat test.txt\nMy second line\nThat may not have been what we intended! As explained above, the file was overwritten.\nWith “&gt;&gt;”, however, we append the output to a file:\necho \"My third line\" &gt;&gt; test.txt\ncat test.txt\nMy second line\nMy third line\n\n\n\n6.2 Standard input and pipes\nRecall from today’s previous examples that a file name can be given as an argument to many commands — for example, see the following sequence of commands:\n# First we redirect the ls output to a file\nls ../data/Saavedra2013 &gt; filelist.txt\n\n# Let's check what that looks like:\nhead -n 5 filelist.txt\nn10.txt\nn11.txt\nn12.txt\nn13.txt\nn14.txt\n# Then we count the nr. of lines, which is the number of files+dirs in Saavedra2013:\nwc -l filelist.txt\n59 filelist.txt\nHowever, most commands also accept input from so-called “standard input” (stdin) using the pipe, “|”:\nls ../data/Saavedra2013 | wc -l\n59\nWhen we use the pipe, the output of the command on the left-hand side (a file listing, in this case) is no longer printed to screen but is redirected into the wc command, which will gladly accept input that way instead of via an argument with a file name like in the earlier example.\nPipes are useful because they avoid having to write/read intermediate files — this saves typing, makes the operation quicker, and reduces file clobber.\n\n\n\n6.3 Selecting columns using cut\nWe’ll now turn to some commands that may be described as Unix “data tools”: cut, sort, uniq, tr, and grep.\nMost of the examples will use the file Pacifici2013_data.csv, so let’s have a look at the contents of that file first:\ncd ../data\nhead -n 3 Pacifici2013_data.csv\nTaxID;Order;Family;Genus;Scientific_name;AdultBodyMass_g;Sources_AdultBodyMass;Max_longevity_d;Sources_Max_longevity;Rspan_d;AFR_d;Data_AFR;Calculated_GL_d;GenerationLength_d;Sources_GL\n7580;Rodentia;Cricetidae;Eligmodontia;Eligmodontia typus;17.37;PanTHERIA;292;PanTHERIA;254.64;73.74;calculated;147.5856;147.5856;Rspan-AFR(SM+Gest)\n42632;Rodentia;Cricetidae;Microtus;Microtus oregoni;20.35;PanTHERIA;456.25;PanTHERIA;445.85;58.06;calculated;187.3565;187.3565;Rspan-AFR(SM+Gest)\n\n\n\n\n\n\nTabular plain-text files (Click to expand)\n\n\n\n\n\nIn this course, we’ll be working almost exclusively with so-called “plain-text” files. These are simple files that can be opened by any text editor and Unix shell tool, as opposed to more complex “binary” formats like Excel or Word files. Almost all common genomics file formats are plain-text.\n“Tabular” files contain data that is arranged in a rows-and-columns format, like a table or an Excel worksheet.\nPlain-text tabular files, like Pacifici2013_data.csv, contain a specific “delimiter” to delimit columns — most commonly:\n\nA Tab, and such files are often stored with a .tsv extension for Tab-Separated Values (“TSV file”).\nA comma, and such files are often stored with a .csv extension for Comma-Separated Values (“CSV file”).\n\n\n\n\n\n\nWhat delimiter does the Pacifici2013_data.csv file appear to contain? (Click for the answer)\n\nFrom looking at the first couple of lines that we printed above, the delimiter is a semicolon ; — “even though” the file has a .csv extension.\n\n\nThe cut command will select/“cut out” one or more columns from a tabular file:\n\nWe’ll always have to use the -f option to specify the desired column(s) / “field”(s).\nBecause its default column delimiter is a Tab, for this file, we’ll have to specify the delimiter with -d.\n\n# Select the first column of the file:\ncut -d \";\" -f 1 Pacifici2013_data.csv\nTaxID\n7580\n42632\n42653\n42662\n16652\n[...output truncated...]\nThat worked, but a ton of output was printed, and we may find ourselves scrolling to the top to see the first few lines – in many cases, it can be useful to pipe the output to head to see if our command works:\ncut -d \";\" -f 1 Pacifici2013_data.csv | head -n 3\nTaxID\n7580\n42632\n\n\n\n\n\n\nSelecting multiple columns with cut (Click to expand)\n\n\n\n\n\nTo select multiple columns, use a range or comma-delimited list:\ncut -d \";\" -f 1-4 Pacifici2013_data.csv | head -n 3\nTaxID;Order;Family;Genus\n7580;Rodentia;Cricetidae;Eligmodontia\n42632;Rodentia;Cricetidae;Microtus\ncut -d \";\" -f 2,8 Pacifici2013_data.csv | head -n 3\nOrder;Max_longevity_d\nRodentia;292\nRodentia;456.25\n\n\n\n\n\n\n6.4 Combining cut, sort, and uniq to create a list\nLet’s say we want an alphabetically sorted list of animal orders from the Pacifici2013_data.csv file. To do this, we’ll need two new commands:\n\nsort to sort/order/arrange rows, by default in alphanumeric order.\nuniq to remove duplicates (i.e., keep all distinct) entries from a sorted file/list\n\nWe’ll build up a small “pipeline” to do this, step-by-step and piping the output into head every time. First, we get rid of the header line with our tail trick:\ntail -n +2 Pacifici2013_data.csv | head -n 5\n7580;Rodentia;Cricetidae;Eligmodontia;Eligmodontia typus;17.37;PanTHERIA;292;PanTHERIA;254.64;73.74;calculated;147.5856;147.5856;Rspan-AFR(SM+Gest)\n42632;Rodentia;Cricetidae;Microtus;Microtus oregoni;20.35;PanTHERIA;456.25;PanTHERIA;445.85;58.06;calculated;187.3565;187.3565;Rspan-AFR(SM+Gest)\n42653;Rodentia;Cricetidae;Peromyscus;Peromyscus gossypinus;27.68;PanTHERIA;471.45833335;PanTHERIA;444.87833335;72.58;calculated;201.59471667;201.5947166715;Rspan-AFR(SM+Gest)\n42662;Macroscelidea;Macroscelididae;Elephantulus;Elephantulus myurus;59.51;PanTHERIA;401.5;PanTHERIA;412.34;90.48;calculated;210.0586;210.0586;Rspan-AFR(SM+Gest)\n16652;Rodentia;Cricetidae;Peromyscus;Peromyscus boylii;23.9;PanTHERIA;547.5;PanTHERIA;514.13;79.97;calculated;229.0677;229.0677;Rspan-AFR(SM+Gest)\nSecond, we select our column of interest with cut:\ntail -n +2 Pacifici2013_data.csv | cut -d \";\" -f 2 | head -n 5\nRodentia\nRodentia\nRodentia\nMacroscelidea\nRodentia\nThird, we pipe to sort to sort the result:\ntail -n +2 Pacifici2013_data.csv | cut -d \";\" -f 2 | sort | head -n 5\nAfrosoricida\nAfrosoricida\nAfrosoricida\nAfrosoricida\nAfrosoricida\nFourth and finally, we use uniq to only keep unique rows (values):\ntail -n +2 Pacifici2013_data.csv | cut -d \";\" -f 2 | sort | uniq\nAfrosoricida\nCarnivora\nCetartiodactyla\nChiroptera\nCingulata\nDasyuromorphia\nDermoptera\nDidelphimorphia\n[...output truncated...]\n\n\n\n\n\n\nGet a count table with uniq -c (Click to expand)\n\n\n\n\n\nWith a very small modification to our pipeline, using uniq’s -c option (for count), we can generate a “count table” instead of a simple list:\ntail -n +2 Pacifici2013_data.csv | cut -d \";\" -f 2 | sort | uniq -c\n     54 Afrosoricida\n    280 Carnivora\n    325 Cetartiodactyla\n   1144 Chiroptera\n     21 Cingulata\n[...output truncated...]\n\n\n\n\n\n\n6.5 grep to print lines matching a pattern\nThe grep command is useful to find specific text or patterns in a file. By default, it will print each line that contains a “match” in full. It’s basic syntax is grep \"&lt;pattern&gt;\" file.\nFor example, this will print all lines from Pacifici2013_data.csv that contain “Vombatidae”:\ngrep \"Vombatidae\" Pacifici2013_data.csv\n40555;Diprotodontia;Vombatidae;Lasiorhinus;Lasiorhinus latifrons;26163.8;PanTHERIA;9928;\"PanTHERIA;AnAge\";9317.5;652.24;calculated;3354.315;3354.315;Rspan-AFR(SM+Gest)\n40556;Diprotodontia;Vombatidae;Vombatus;Vombatus ursinus;26000;PanTHERIA;10238.25;\"PanTHERIA;AnAge\";9511.6;783.65;calculated;3542.014;3542.014;Rspan-AFR(SM+Gest)\n11343;Diprotodontia;Vombatidae;Lasiorhinus;Lasiorhinus krefftii;31849.99;PanTHERIA;10950;\"PanTHERIA;AnAge\";no information;no information;no information;no information;3354.315;Mean_congenerics_same_body_mass\nInstead of printing matching lines, we can also count them with the -c option:\ngrep -c \"Chiroptera\" Pacifici2013_data.csv\n1144\nThe option -v inverts grep’s behavior and will print all lines not matching the pattern — here, we’ll combine -v and -c to count the number of lines that do not contain “Vombatidae”:\ngrep -vc \"Vombatidae\" Pacifici2013_data.csv\n5424\n\n\n\n\n\n\ngrep tips\n\n\n\n\nIt is best to always use quotes around the pattern.7\nIncomplete matches, including in individual words, work: “Vombat” matches Vombatidae.\ngrep has many other useful options — more in Week 4:\n\n-i to ignore case\n-r to search files recursively\n-w to match “words”\n\n\n\n\n\n\n\n6.6 Substituting characters using tr\ntr for translate will substitute characters – here, any a for a b:\necho \"aaaabbb\" | tr a b\nbbbbbbb\nOddly enough tr does not take a file name as an argument, so how can we provide it with input from a file? The easiest way is by using a pipe (note that the book also shows another method):\ncat Pacifici2013_data.csv | tr \";\" \"\\t\" | head -n 3\nTaxID   Order   Family  Genus   Scientific_name AdultBodyMass_g Sources_AdultBodyMass   Max_longevity_d Sources_Max_longevity   Rspan_d AFR_d   Data_AFR        Calculated_GL_d   GenerationLength_d      Sources_GL\n7580    Rodentia        Cricetidae      Eligmodontia    Eligmodontia typus      17.37   PanTHERIA       292     PanTHERIA       254.64  73.74   calculated      147.5856  147.5856        Rspan-AFR(SM+Gest)\n42632   Rodentia        Cricetidae      Microtus        Microtus oregoni        20.35   PanTHERIA       456.25  PanTHERIA       445.85  58.06   calculated      187.3565  187.3565        Rspan-AFR(SM+Gest)\nThe example above converted the ; delimited to a Tab (i.e., a CSV file to a TSV file), where \\t is a regular expression meaning Tab8. (Though note that we didn’t modify the original file nor saved the output in a new file.)\n\n\n\n\n\n\nDeletion and “squeezing” with tr (Click to expand)\n\n\n\n\n\n\nDelete all a’s:\necho \"aabbccddee\" | tr -d a\nbbccddee\nRemove consecutive duplicates a’s:\necho \"aabbccddee\" | tr -s a\nabbccddee\n\n\n\n\n\n\n\n\n\n\ntr also works well with multiple values and replacements (Click to expand)\n\n\n\n\n\n\nReplace multiple types of characters with one:\n\necho \"123456789\" | tr 1-5 0    # Replace any of 1-5 with 0\n000006789\n\nOne-to-one mapping of input and replacement!\n\necho \"ACtGGcAaTT\" | tr actg ACTG\nACTGGCAATT\necho \"aabbccddee\" | tr a-c 1-3\n112233ddee\n\n\n\n\n Exercise: Redirecting tr output\nModify our command in which we changed the delimiter to a Tab to redirect the output to a new file, Pacifici2013_data.tsv (note the extension) in the sandbox dir (that’s not where you are located yourself).\n\n\nSolution\n\ncat Pacifici2013_data.csv | tr \";\" \"\\t\" &gt; ../Pacifici2013_data.tsv"
  },
  {
    "objectID": "week02/w1_shell.html#wrap-up-the-unix-philosophy",
    "href": "week02/w1_shell.html#wrap-up-the-unix-philosophy",
    "title": "Unix Shell Basics",
    "section": "7 Wrap-up & the Unix philosophy",
    "text": "7 Wrap-up & the Unix philosophy\n\n7.1 Covered in the chapter but not in today’s lecture\n\nThe less pager to view files\nThe find command to find files\nShowing and changing file permissions\nBasic shell scripting\nfor loops\n$PATH and bash profile settings\n\nWe’ll cover all of these in class over the next few weeks.\n\n\n\n7.2 The Unix philosophy\n\nThis is the Unix philosophy: Write programs that do one thing and do it well. Write programs to work together. Write programs to handle text streams, because that is a universal interface. — Doug McIlory\n\nAdvantages of a modular approach:\n\nEasier to spot errors\nEasy to swap out components, including in other languages\nEasier to learn (?)\n\nText “streams”?\nRather than loading entire files into memory, process them one line at a time. Very useful with large files!\n# This command would combine all files in the working dir ending in `.fa`\n# (i.e. FASTA files) into a single file -- even if that's multiple GBs,\n# this will not be a heavy lift at all!\ncat *.fa &gt; combined.fa\n\n\n\n7.3 The Unix shell in the weeks ahead\n\nIn all course weeks, we will be working in the Unix shell, though our focus in several cases will be on a specific tool, such as Git in week 3.\nIn week 4, we’ll fully focus on the shell and shell scripting."
  },
  {
    "objectID": "week02/w1_shell.html#footnotes",
    "href": "week02/w1_shell.html#footnotes",
    "title": "Unix Shell Basics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n Technically, the latter terms are more correct, as Unix formally does refer to a specific operating system.↩︎\nCommand-line Interface (CLI), as opposed to Graphical User Interface (GUI)↩︎\n Though some commands are flexible and accept either order.↩︎\nThere really is only one proper additional options: two others reflect the defaults, and then there’s the version option.↩︎\nBecause of this, anytime you see a word/string that starts with a $ in the shell, you can safely assume that it is a variable.↩︎\nWe’ll see this in action in a bit↩︎\nEven though for “literal string” like in the examples above, as opposed to regular expressions, this is not strictly necessary, it’s good habit to always quote.↩︎\nWe’ll learn more about regular expressions in Week 4.↩︎"
  }
]